topic_track update racing date race no track

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Sep 22 19:04:53 2019

@author: root
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Sep 22 00:27:07 2019

@author: root
"""

#%reset -f
import os
import numpy as np
#os.chdir(r'C:\Users\simon\Desktop\python\WinPython-64bit-3.6.2.0Qt5\notebooks\index_analysis')
target_dir='/home/simon/Dropbox/notebooks/horse'

os.chdir(target_dir)

import re
from pandas import read_excel
#import pandas_datareader as pdr
from datetime import timedelta
from datetime import datetime as dt
from datetime import timedelta
import datetime
import pandas as pd
from time import sleep

from sqlalchemy import create_engine
import configparser
import re
import numpy as np
import pymysql
import pandas.io.sql as sql
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
from bs4 import BeautifulSoup


#output stan out
import sys
import time
from pandas import read_excel



def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

    
#output stan out
import sys
import time
time_now_save=time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")
stan_out_log1=os.path.join('log','stan_out_update_racing_date_raceno_track_'+time_now_save+'_production'+'.log')
sys.stderr = open(stan_out_log1, 'w')
    
    
    
    
    
    
    
    
    
#below is special mwthod to extract html with javascript enabled
from math import cos, pi, floor
import requests

def parse_challenge(page):
    """
    Parse a challenge given by mmi and mavat's web servers, forcing us to solve
    some math stuff and send the result as a header to actually get the page.
    This logic is pretty much copied from https://github.com/R3dy/jigsaw-rails/blob/master/lib/breakbot.rb
    """
    #page=r.text
    top = page.split('<script>')[1].split('\n')
    challenge = top[1].split(';')[0].split('=')[1]
    challenge_id = top[2].split(';')[0].split('=')[1]
    return {'challenge': challenge, 'challenge_id': challenge_id, 'challenge_result': get_challenge_answer(challenge)}


def get_challenge_answer(challenge):
    """
    Solve the math part of the challenge and get the result
    """
    arr = list(challenge)
    last_digit = int(arr[-1])
    arr.sort()
    min_digit = int(arr[0])
    subvar1 = (2 * int(arr[2])) + int(arr[1])
    subvar2 = str(2 * int(arr[2])) + arr[1]
    power = ((int(arr[0]) * 1) + 2) ** int(arr[1])
    x = (int(challenge) * 3 + subvar1)
    y = cos(pi * subvar1)
    answer = x * y
    answer -= power
    answer += (min_digit - last_digit)
    answer = str(int(floor(answer))) + subvar2
    return answer




#output stan out
import sys
import time

#today_date='2021-12-28'
today_date=time.strftime("%Y-%m-%d")
#extraction_date=dt(2021,12,29)
extraction_date=dt.strptime(today_date,"%Y-%m-%d")+ timedelta(days=1)
#extraction_date=dt.strptime(today_date,"%Y-%m-%d")#+ timedelta(days=1)
extraction_date_hkjc_format=extraction_date.strftime('%Y%m%d')
extraction_date_format2=extraction_date.strftime('%Y-%m-%d')
extraction_date_format3=extraction_date.strftime('%Y/%m/%d')

day=extraction_date.day
month=extraction_date.month


def get_html_using_selenium(link):
    op = webdriver.ChromeOptions()
    op.add_argument('headless')
    op.add_argument('--no-sandbox')
    op.add_argument('--disable-dev-shm-usage')
    #op.add_argument("--enable-javascript")

    
    browser = webdriver.Chrome(options=op,executable_path='/home/simon/Dropbox/notebooks/horse/chrome_driver/chromedriver_linux64/chromedriver84')
    #browser.execute_script("return document.cookie")  
    browser.execute_script("return navigator.userAgent")  
    
    #browser = webdriver.Chrome(executable_path=r'C:\Users\simon.chan\.Spyder\chromedriver.exe')
    browser.get(link)
    
    #browser.set_window_position(-10000,0)
    #find the html source code
    #html_string=browser.page_source 
    
    #https://stackoverflow.com/questions/22739514/how-to-get-html-with-javascript-rendered-sourcecode-by-using-selenium
    #html_string=browser.execute_script("return document.getElementsByTagName('html')[0].innerHTML")
    sleep(1)  #this is very important, other fail to extract
    html_string=browser.execute_script("return document.body.innerHTML")

    browser.quit()
    return html_string





#date_key='20200913'
def extract_racecard(date_key):
    #date_key='2006/10/04';raceno_key=str(3);track_key='HV'
    date_key_old=date_key
    url='https://racing.hkjc.com/racing/info/meeting/RaceCard/English/Local/'+date_key
    html_found=False
    problem_races=pd.DataFrame()

    for ii in range(0,100):
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        html_found=False
        source_html=''
        try:
            s = requests.Session()
            r = s.get(url)
            source_html=r.text            
            if not ('f_fs13' in source_html) & ('f_fs12 f_fr js_racecard' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('f_fs13' in source_html) & ('f_fs12 f_fr js_racecard' in source_html):
                html_found=True
                break
        except:
            print("Oops!",sys.exc_info()[0],"occured when extracting ",'racecard ',date_key_old,'\n')
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'racecard ',date_key_old,'\n')
        sleep(0.2)
        
    if html_found:
        no_of_trial=ii+1
        print('racecard html code',' tried ',no_of_trial,' times extraction with success',date_key_old,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"find_NoOfRace_track_racecard_source_code_"+date_key_old+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
        
    if html_found:
        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
        try:
            #find track
            table = soup.find_all('div', {'class':'f_fs13'})[0] # Grab the first table     
#            columns = table.find_all('tr')[0].find_all('td')
#            all_text=columns[0].get_text()
#            all_text2=all_text.split("\r\n")
#            racecourse=all_text2[1].split(",")[-2].strip()
            racecourse=table.get_text().split(",")[3].strip()
        
            #find total number of races
            table = soup.find_all('table', {'class':'f_fs12 f_fr js_racecard'})[0] # Grab the first table 
            
            link_list=[]
            for td in table.find_all('tr')[0].find_all('td'):  # if oversea races, there will be two tr, so use first tr is ok 
                if td.find('a') is not None:    
                    link_list.append(td.find('a').get('href'))
    
            no_of_race=link_list[-1].split("=")[-1]
            
        except:
            print("racecard aboveinfo string format may not be standard",date_key_old,' ',sys.exc_info()[0],'\n') 
            eprint("racecard aboveinfo string format may not be standard",date_key_old,' ',sys.exc_info()[0],'\n') 
        
    else:
        print('required table in html code not found ',date_key_old)
    return [racecourse,no_of_race]



#below is to find the racecourse HV/ST and total number of races on the coming racing date
#then update the 'racing_date_raceno_track.csv'
output=extract_racecard(extraction_date_hkjc_format)

if output[0]=='Happy Valley':
    output[0]='HV'
else:
    output[0]='ST'

#update track and raceno
race_key_source=pd.read_csv("./racing_date_raceno_track.csv")
race_key_source['pre_post']=''

for i in range(1,int(output[1])+1):
    date_raceno_track=extraction_date_format2+'@'+str(i)+'@'+output[0]
    temp=pd.DataFrame({'date_raceno_track':[date_raceno_track],'pre_post':['pre']})
    race_key_source=race_key_source.append(temp)

race_key_source=race_key_source.reset_index(drop=True)

race_key_source=race_key_source.drop_duplicates(subset='date_raceno_track', keep="last")

race_key_source.to_csv('racing_date_raceno_track.csv',index=False)

sleep(5)






import subprocess
#pre race update
subprocess.call(['/root/anaconda3/bin/python','extract_pre_post_race_linux_v2.py','pre'],shell=False)



sleep(30)

import subprocess
#to create main_data
subprocess.call(['/root/anaconda3/bin/python','edit_data_v2.py'],shell=False)








#clear cron job of comment odds pooling
from crontab import CronTab
my_cron = CronTab(user='root')
for job in my_cron:
    if job.comment == 'odds pooling':
        my_cron.remove(job)
        my_cron.write()



#set up odds pooling cron job
connection=create_engine("mysql+pymysql://root:pp#@localhost:3306/horseracing")
                         
query='SELECT * from racecard where date='+'"'+extraction_date_format3+'"'
racecard= sql.read_sql(query,connection)
race_start_race1=racecard.loc[racecard['RaceNo']=='1','race_start_time'].values[0]

race_start_race1_dt=dt.strptime(race_start_race1,'%H:%M')

race_start_race1_dt_lag_1hour=race_start_race1_dt-timedelta(minutes=30)

minute=race_start_race1_dt_lag_1hour.minute
hour=race_start_race1_dt_lag_1hour.hour


race_end=racecard.loc[racecard['RaceNo']==output[1],'race_start_time'].values[0]

race_end_dt=dt.strptime(race_end,'%H:%M')

race_end_dt_fast=race_end_dt+timedelta(minutes=30)

minute=race_start_race1_dt_lag_1hour.minute
hour=race_start_race1_dt_lag_1hour.hour

minute_end=race_end_dt_fast.minute
hour_end=race_end_dt_fast.hour


#hour=23
#minute=20
#month=10
#day=6
#hour='17-18'
#minute='37-59/2'


#3 cron string
#cron_string=str(minute)+'-'+'59'+'/10'+' '+str(hour)+' '+str(day)+' '+str(month)+' * '+ "sh /home/simon/Dropbox/notebooks/horse/run_extract_odds.sh > /home/simon/Dropbox/notebooks/horse/log/odds/extract_odd_\$(date +'\%Y\%m\%d'_'\%H\%M\%S').log 2>&1 &"+" #odds pooling"
#cron_shell_cmd='crontab -l | { cat; echo '+'"'+cron_string+'"'+'; } | crontab -'
#os.system(cron_shell_cmd)
#
#a=hour+1
#b=hour_end-1
#cron_string='0'+'-'+'59'+'/10'+' '+str(a)+'-'+str(b)+' '+str(day)+' '+str(month)+' * '+ "sh /home/simon/Dropbox/notebooks/horse/run_extract_odds.sh > /home/simon/Dropbox/notebooks/horse/log/odds/extract_odd_\$(date +'\%Y\%m\%d'_'\%H\%M\%S').log 2>&1 &"+" #odds pooling"
#cron_shell_cmd='crontab -l | { cat; echo '+'"'+cron_string+'"'+'; } | crontab -'
#os.system(cron_shell_cmd)
#
#cron_string='0'+'-'+str(minute_end)+'/10'+' '+str(hour_end)+' '+str(day)+' '+str(month)+' * '+ "sh /home/simon/Dropbox/notebooks/horse/run_extract_odds.sh > /home/simon/Dropbox/notebooks/horse/log/odds/extract_odd_\$(date +'\%Y\%m\%d'_'\%H\%M\%S').log 2>&1 &"+" #odds pooling"
#cron_shell_cmd='crontab -l | { cat; echo '+'"'+cron_string+'"'+'; } | crontab -'
#os.system(cron_shell_cmd)


cron_string=str(minute)+' '+str(hour)+' '+str(day)+' '+str(month)+' * '+ "sh /home/simon/Dropbox/notebooks/horse/run_extract_odds.sh > /home/simon/Dropbox/notebooks/horse/log/odds/extract_odd_\$(date +'\%Y\%m\%d'_'\%H\%M\%S').log 2>&1 &"+" #odds pooling"
cron_shell_cmd='crontab -l | { cat; echo '+'"'+cron_string+'"'+'; } | crontab -'
os.system(cron_shell_cmd)





#clear cron job of track race start time
from crontab import CronTab
my_cron = CronTab(user='root')
for job in my_cron:
    if job.comment == 'track race start time':
        my_cron.remove(job)
        my_cron.write()

cron_string=str(minute)+' '+str(hour)+' '+str(day)+' '+str(month)+' * '+ "sh /home/simon/Dropbox/notebooks/horse/track_race_start.sh > /home/simon/Dropbox/notebooks/horse/log/gen_factor_place_bet/trigger_track_racestart_\$(date +'\%Y\%m\%d'_'\%H\%M\%S').log 2>&1 &"+" #track race start time"
cron_shell_cmd='crontab -l | { cat; echo '+'"'+cron_string+'"'+'; } | crontab -'
os.system(cron_shell_cmd)

















#clear cron job of post race update
from crontab import CronTab
my_cron = CronTab(user='root')
for job in my_cron:
    if job.comment == 'post update':
        my_cron.remove(job)
        my_cron.write()
        
        
        
#schedule post race update

#find final race time and schedule post race update 30 mins after final race 
#extraction_date_format3='2019/09/25'
#last_race_no='9'
last_race_no=output[1]
                         
query='SELECT * from racecard where date='+'"'+extraction_date_format3+'"'
racecard= sql.read_sql(query,connection)
race_start_race_last=racecard.loc[racecard['RaceNo']==last_race_no,'race_start_time'].values[0]

race_start_race_last=extraction_date_format2+' '+race_start_race_last

race_start_race_last_dt=dt.strptime(race_start_race_last,'%Y-%m-%d %H:%M')

race_start_race_last_dt_more=race_start_race_last_dt+timedelta(minutes=30) #post race update at 30 mins after final race 

month_post=race_start_race_last_dt_more.month
day_post=race_start_race_last_dt_more.day
hour_post=race_start_race_last_dt_more.hour
minute_post=race_start_race_last_dt_more.minute

cron_string=str(minute_post)+' '+str(hour_post)+' '+str(day_post)+' '+str(month_post)+' * '+ "/root/anaconda3/bin/python /home/simon/Dropbox/notebooks/horse/extract_pre_post_race_linux_v2.py 'post' 2>&1 &"+" #post update"
cron_shell_cmd='crontab -l | { cat; echo '+'"'+cron_string+'"'+'; } | crontab -'
os.system(cron_shell_cmd)











#test odd links
import requests
from concurrent.futures import ThreadPoolExecutor
track=output[0]
raceno_end=output[1]

all_links=[]
#i=1
for i in range(1,int(raceno_end)+1):
    all_links.append(today_date+'@'+str(i)+'@'+'win'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=win&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    all_links.append(today_date+'@'+str(i)+'@'+'pla'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=pla&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    all_links.append(today_date+'@'+str(i)+'@'+'qin'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=qin&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    all_links.append(today_date+'@'+str(i)+'@'+'qpl'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=qpl&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    all_links.append(today_date+'@'+str(i)+'@'+'tri'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=tri&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    all_links.append(today_date+'@'+str(i)+'@'+'ff'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=ff&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    all_links.append(today_date+'@'+str(i)+'@'+'fct'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=fct&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    
    all_links.append(today_date+'@'+str(i)+'@'+'pooltot'+'@'+'https://bet.hkjc.com/racing/getJSON.aspx?type=pooltot&date='+extraction_date_format2+'&venue='+track+'&raceno='+str(i))
    

def get_url(url):
    cond=True
    count=0

    #output=requests.get(url).json()
    date=url.split("@")[0]
    raceno=url.split("@")[1]
    bettype=url.split("@")[2]
    url_use=url.split("@")[3]
    
    while (cond==True)&(count<=10): #max try 10 times, if still error, then leave and go on
        output={'OUT':''}
        try:
            r=requests.post(url_use, allow_redirects=False,timeout=2)                  
            output=r.json()                 
            cond=False

        except:
#            log_output="Oops! "+str(sys.exc_info()[0])+' '+str(sys.exc_info()[1])+" occured when extracting odds at time "+str(dt.now())+' with '+url_use+'\n'
#            eprint(log_output)
#            file1 = open(os.path.join('log/odds/odds_errors','adhot_error '+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+'_production'+'.txt'),"a") 
#            file1.write(log_output)
#            file1.close() 
            pass
        count=count+1
 
    extraction_time=time.strftime("%Y-%m-%d")+' '+time.strftime("%H:%M:%S")
    output.update({'date':date})
    output.update({'extraction_time':extraction_time})
    output.update({'raceno':raceno})
    output.update({'bettype':bettype})
    output.update({'url_use':url_use})

    #print('finished ',url)
    return output

# parse odds jsons file with multiprocessing
all_jsons=[]
with ThreadPoolExecutor(max_workers=20) as pool:
    all_jsons=list(pool.map(get_url,all_links))

#remove elements with output={}
all_jsons2=all_jsons.copy()
count=0
for k in all_jsons2: 
    for key in k:
        if (key=='OUT' and k[key]==''):
           count=count+1

odd_problem='OK'
if count>=3:  #for some race if less hr, no qpl
    odd_problem='Not OK'
else:
    eprint('odd ok')
    print('odd ok')






















sys.stderr.close()
sys.stderr = sys.__stderr__


sleep(1)








#stan_out_log=os.path.join('log','stan_out_post_race_data_part_20191029_213408_production.log')



with open(stan_out_log1) as f:
    data = f.readlines()

key_words=['error','Error','exception','Exception']

#check key work in log file line by line
output=[x for x in data if any(s in x for s in key_words)]
output="\n".join(output)

error_exist= 'No errors' if len(output)==0 else 'has errors'

race_information=track+' '+'No.of.races is '+raceno_end

#python send email to check is there any exception/error in log file
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
mail_content = output
#The mail addresses and password
sender_address = 'random9522@gmail.com'
sender_pass = '95229522'
receiver_address = 'simon.ertwewe@gmail.com'
#Setup the MIME
message = MIMEMultipart()
message['From'] = sender_address
message['To'] = receiver_address
message['Subject'] = 'Error/Exception in raceno_track: '+error_exist  #The subject line
#The body and the attachments for the mail
message.attach(MIMEText(mail_content))

#attachement
fp = open(os.path.join(target_dir,stan_out_log1), 'rb')
part = MIMEBase('application',"octet-stream")           #send txt file as attachement
part.set_payload(fp.read())
fp.close()
encoders.encode_base64(part)
part.add_header('Content-Disposition', 'attachment', filename=stan_out_log1)
message.attach(part)


#Create SMTP session for sending the mail
session = smtplib.SMTP('smtp.gmail.com', 587) #use gmail with port
session.starttls() #enable security
session.login(sender_address, sender_pass) #login with mail_id and password
text = message.as_string()
session.sendmail(sender_address, receiver_address, text)
session.quit()
print('Mail Sent')



















#send email for odd link checking
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
mail_content = 'Nothing'
#The mail addresses and password
sender_address = 'random9522@gmail.com'
sender_pass = '95229522'
receiver_address = 'simon.ertwewe@gmail.com'
#Setup the MIME
message = MIMEMultipart()
message['From'] = sender_address
message['To'] = receiver_address
message['Subject'] = 'Odds links: '+odd_problem  #The subject line
#The body and the attachments for the mail
message.attach(MIMEText(mail_content))

#attachement
fp = open(os.path.join(target_dir,stan_out_log1), 'rb')
part = MIMEBase('application',"octet-stream")           #send txt file as attachement
part.set_payload(fp.read())
fp.close()
encoders.encode_base64(part)
part.add_header('Content-Disposition', 'attachment', filename=stan_out_log1)
message.attach(part)


#Create SMTP session for sending the mail
session = smtplib.SMTP('smtp.gmail.com', 587) #use gmail with port
session.starttls() #enable security
session.login(sender_address, sender_pass) #login with mail_id and password
text = message.as_string()
session.sendmail(sender_address, receiver_address, text)
session.quit()
print('Mail Sent')










topic_track edit date_v2




#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Sep 22 18:14:12 2019

@author: root
"""

######################################################################################
######################################################################################
##################################Edit Data###########################################
######################################################################################
######################################################################################

from sqlalchemy import create_engine
import configparser
import re
#import MySQLdb as mdb
import pandas as pd
import numpy as np
import pymysql
import pandas.io.sql as sql
from datetime import datetime as dt
import datetime
from datetime import timedelta

from pandas import HDFStore,DataFrame

from numba import jit




import os
os.getcwd()
os.chdir('/home/simon/Dropbox/notebooks/horse')



connection=create_engine("mysql+pymysql://root:pp222#@localhost:3306/horseracing")
                         
                         
#query='SELECT * from dividend'
#racecard= sql.read_sql(query,connection)
#racecard_check=racecard.loc[racecard['Pool']=='TRIPLE TRIO(Consolation)',:].copy()                                                
#racecard_check['yes']=racecard_check['Comb'].apply(lambda x: ('Any Combination/Any Combination' in x)|('Any combination/Any combination' in x))                         



#look at dividend
dividend = pd.read_sql('SELECT * from dividend' , con=connection)
dividend['Date']=dividend['Date'].apply(lambda x:dt.strptime(x,"%Y/%m/%d").strftime("%Y-%m-%d")) #string to date then to string again
dividend=dividend.loc[~(dividend['dividend']=='REFUND'),:]
dividend['RaceNo']=dividend['RaceNo'].astype(str)
dividend['Date_RaceNo']=dividend['Date']+'_'+dividend['RaceNo']

dividend=dividend.loc[dividend['Pool']=='TIERCE',:].copy()
dividend['dividend']=dividend['dividend'].apply(lambda x: str(x).replace(",",""))
dividend['dividend']=dividend['dividend'].astype(np.float)












                         
                         


query='SELECT * from racecard'
racecard= sql.read_sql(query,connection)
list(racecard.columns.values)
racecard.surface_condition.unique()

racecard_check=racecard.head(10000)

check=racecard.describe

racecard_check=racecard.loc[racecard['Date']=='2020/06/21',:]




main_data=racecard.copy()

def move_column_to_index(data,field_name,ind):
    cols=data.columns.tolist()
    cols.insert(ind,cols.pop(cols.index(field_name)))
    data=data.reindex(columns=cols)
    return data

main_data=move_column_to_index(main_data,'Date',0)

check2=len(main_data.Date.unique().tolist())



#convert date to %Y-%m-%d format
main_data['Date']=main_data['Date'].apply(lambda x:dt.strptime(x,"%Y/%m/%d").strftime("%Y-%m-%d")) #string to date then to string again

#remove 2019-09-18 races, on this day, hkjc stop all races because of ho kwan yiu horse
main_data=main_data.loc[~(main_data['Date']=='2019-09-18'),:]
main_data=main_data.reset_index(drop=True)

#remove 2019-11-13 races, on this day, hkjc stop all races because of protest
main_data=main_data.loc[~(main_data['Date']=='2019-11-13'),:]
main_data=main_data.reset_index(drop=True)

#edit raceno
main_data=move_column_to_index(main_data,'RaceNo',1)
main_data['RaceNo'].unique()  #from 1 to 11
main_data['RaceNo']=main_data['RaceNo'].astype(int)

#edit horseno
main_data=move_column_to_index(main_data,'Horse No.',2)
main_data=main_data.rename(columns={'Horse No.':'HorseNo'})
main_data.HorseNo.unique()

main_data_check=main_data.loc[(main_data['Date']=='2019-09-15')&(main_data['RaceNo']==6),:]
main_data_check=main_data.loc[(main_data['Date']=='2016-07-10')&(main_data['RaceNo']==2),:]


main_data_check=main_data.loc[(main_data['HorseNo']=='0'),:]
main_data['HorseNo']=main_data['HorseNo'].astype(int)














main_data_check=main_data.loc[(main_data['Date']=='2021-01-24')&(main_data['RaceNo']==8),:]




















#before 20110710, if hourse withdraw, it will show 0, so remove them
#i define late withdraw only when capture odds start and it start at 2015, so removing these 0 is ok
main_data=main_data.loc[main_data['HorseNo']!='0',:]
main_data_check=main_data.loc[(main_data['HorseNo']=='0'),:]
main_data_check=main_data.head(100)

#sort by date, raceno, horse no
main_data=main_data.sort_values(by=['Date','RaceNo','HorseNo'],ascending=[False,True,True])
main_data=main_data.reset_index(drop=True)

#there is a withdraw near horse name
#main_data_check=main_data.loc[(main_data['Date']=='2019-09-21')&(main_data['RaceNo']=='6'),:]
main_data[['HorseName','withdrawn']]=main_data.Horse.str.split('(',expand=True)
main_data.loc[pd.isnull(main_data['withdrawn']),"withdrawn"]=''
main_data['withdrawn']=main_data['withdrawn'].apply(lambda x:x.replace(')',''))
main_data['withdrawn']=main_data['withdrawn'].apply(lambda x: x.strip())
main_data['withdrawn']=main_data['withdrawn'].apply(lambda x: x.lower())

#starting from 202009, withdrawn become 'scratch'
main_data.loc[main_data['withdrawn']=='scratched','withdrawn']='withdrawn'

del main_data['Horse']





#trim horse name right and left space coz some horse name have right space
main_data['HorseName']=main_data['HorseName'].apply(lambda x: x.strip())
#lower case horse name
main_data['HorseName']=main_data['HorseName'].apply(lambda x: x.lower())
check_horsename=list(main_data['HorseName'].unique())

main_data=move_column_to_index(main_data,'HorseName',3)
main_data=move_column_to_index(main_data,'withdrawn',4)

#check brand name
main_data=main_data.rename(columns={'Brand No.':'BrandName'})
main_data['BrandName']=main_data['BrandName'].apply(lambda x: x.strip()) #no brand name with right space, but for safety reason, just also trim them 
check_brand=list(main_data['BrandName'].unique())
main_data=move_column_to_index(main_data,'BrandName',5)
main_data['Horse_Brand']=main_data['HorseName']+'_'+main_data['BrandName']
main_data=move_column_to_index(main_data,'Horse_Brand',6)
check_hr_brand=list(main_data['Horse_Brand'].unique())

#HorseName is less than brand name, 
check1=list(main_data['BrandName'].unique())
check2=list(main_data['HorseName'].unique())
check_hr_brand=list(main_data['Horse_Brand'].unique())


#edit jockey and create jockey allowance
main_data[['Jockey','Jkc_allowance']]=main_data.Jockey.str.split('(',expand=True)
main_data.loc[pd.isnull(main_data['Jkc_allowance']),"Jkc_allowance"]=''
main_data['Jkc_allowance']=main_data['Jkc_allowance'].apply(lambda x:x.replace(')',''))
main_data['Jkc_allowance']=main_data['Jkc_allowance'].apply(lambda x: x.strip())
main_data.loc[main_data['Jkc_allowance']=='',"Jkc_allowance"]=0
main_data['Jkc_allowance']=main_data['Jkc_allowance'].astype(int)
check_Jkc_allowance=list(main_data['Jkc_allowance'].unique())


#edit jockey
check_jockey=list(main_data['Jockey'].unique())
main_data['Jockey']=main_data['Jockey'].apply(lambda x: x.strip())#some jockey name with right space
#lower case jocky name
main_data['Jockey']=main_data['Jockey'].apply(lambda x: x.lower())
main_data=move_column_to_index(main_data,'Jockey',7)
main_data=move_column_to_index(main_data,'Jkc_allowance',8)


#check age  #2 to 10 only
check_age=list(main_data['Age'].unique())
main_data=move_column_to_index(main_data,'Age',9)

main_data.loc[main_data['Age']=='-','Age']=np.nan
main_data['Age']=main_data['Age'].astype(float)









#check draw
#if withdraw, before 2011, it use 0, after 2011 it use '\xa0', which is space
check_draw=list(main_data['Draw'].unique())
main_data['Draw']=main_data['Draw'].apply(lambda x: x.replace('\xa0',''))

draw_check=main_data.loc[(main_data['Draw']=='0')|(main_data['Draw']==''),:]
main_data=move_column_to_index(main_data,'Draw',10)

#check trainer
check_trainer=list(main_data['Trainer'].unique())
main_data['Trainer']=main_data['Trainer'].apply(lambda x: x.strip())
#lower case jocky name
main_data['Trainer']=main_data['Trainer'].apply(lambda x: x.lower())
main_data=move_column_to_index(main_data,'Trainer',11)

#check sex
check_sex=list(main_data['Sex'].unique())
main_data=move_column_to_index(main_data,'Sex',12)

#check hkjc rating #for graffin and G1 G2 G3, and some 3 YEAR OLDS AND UPWARDS no data,
check_rtg=list(main_data['Rtg.'].unique())
main_data=main_data.rename(columns={'Rtg.':'Rtg'})
main_data['Rtg']=main_data['Rtg'].apply(lambda x: x.strip())
#lower case jocky name
main_data['Rtg']=main_data['Rtg'].apply(lambda x: x.lower())
check_rtg=list(main_data['Rtg'].unique())

rtg_check=main_data.loc[(main_data['Rtg']=='-')|(main_data['Rtg']=='0'),:]

main_data.loc[(main_data['Rtg']=='-')|(main_data['Rtg']=='0'),'Rtg']=0
main_data['Rtg']=main_data['Rtg'].astype(int)

#del main_data['Rtg']
#del main_data['Rtg.+/-']





#check dam
check_dam=list(main_data['Dam'].unique())
main_data['Dam']=main_data['Dam'].apply(lambda x: x.strip())
#lower case jocky name
main_data['Dam']=main_data['Dam'].apply(lambda x: x.lower())
main_data=move_column_to_index(main_data,'Dam',13)

#check sire
check_sire=list(main_data['Sire'].unique())
main_data['Sire']=main_data['Sire'].apply(lambda x: x.strip())
#lower case jocky name
main_data['Sire']=main_data['Sire'].apply(lambda x: x.lower())
main_data=move_column_to_index(main_data,'Sire',14)

#check Wt
check_wt=list(main_data['Wt.'].unique())
main_data=main_data.rename(columns={'Wt.':'Wt'})
main_data['Wt']=main_data['Wt'].apply(lambda x: x.strip())
main_data['Wt']=main_data['Wt'].apply(lambda x: x.lower())


main_data['Wt']=pd.to_numeric(main_data['Wt'], errors='coerce')  

main_data=move_column_to_index(main_data,'Wt',15)


#colour column has no value, so remove it
check_COLOUR=list(main_data['Colour'].unique())
del main_data['Colour']


#WFA ONLY HAS VALUE ['-', '5'] so remove it
check_wfa=list(main_data['WFA'].unique())
del main_data['WFA']

#horse weight, sometimes missing but not many
main_data=main_data.rename(columns={'Horse Wt. (Declaration)':'HorseWeight','Wt.+/- (vs Declaration)':'HorseWeightChange'})
check_hrweight=list(main_data['HorseWeight'].unique())
main_data['HorseWeight']=main_data['HorseWeight'].apply(lambda x: x.strip())
main_data['HorseWeight']=main_data['HorseWeight'].apply(lambda x: x.lower())
main_data['HorseWeight']=pd.to_numeric(main_data['HorseWeight'], errors='coerce')  #conver '' and '-' to nan

hr_weight_check=main_data.loc[(main_data['HorseWeight']==''),:]
main_data=move_column_to_index(main_data,'HorseWeight',16)

check_hrweight_change=list(main_data['HorseWeightChange'].unique())
main_data['HorseWeightChange']=main_data['HorseWeightChange'].apply(lambda x: x.strip())
main_data['HorseWeightChange']=main_data['HorseWeightChange'].apply(lambda x: x.lower())
main_data=move_column_to_index(main_data,'HorseWeightChange',17)

#gear
check_gear=list(main_data['Gear'].unique())
main_data['Gear']=main_data['Gear'].apply(lambda x: x.strip())
main_data['Gear']=main_data['Gear'].apply(lambda x: x.lower())
main_data=move_column_to_index(main_data,'Gear',18)

hr_gear_check=main_data.loc[(main_data['Gear']==''),:]


#best time
check_bt=list(main_data['Best Time'].unique())
main_data=main_data.rename(columns={'Best Time':'BestTime'})
main_data['BestTime']=main_data['BestTime'].apply(lambda x: x.strip())

#import cat
check_import=list(main_data['Import Cat.'].unique())
main_data=main_data.rename(columns={'Import Cat.':'ImportCat'})
main_data['ImportCat']=main_data['ImportCat'].apply(lambda x: x.strip())

#last 6 runs
check_last6=list(main_data['Last 6 Runs'].unique())
main_data=main_data.rename(columns={'Last 6 Runs':'Last6Runs'})
main_data['Last6Runs']=main_data['Last6Runs'].apply(lambda x: x.strip())

#over weight
check_overweight=list(main_data['Over Wt.'].unique())
main_data=main_data.rename(columns={'Over Wt.':'over_weight'})
main_data['over_weight']=main_data['over_weight'].apply(lambda x: x.strip())

#over weight
check_owner=list(main_data['Owner'].unique())
main_data['Owner']=main_data['Owner'].apply(lambda x: x.strip())
main_data['Owner']=main_data['Owner'].apply(lambda x: x.lower())

#priority
main_data['Priority'].str.encode('utf-8')
main_data['Priority']=main_data['Priority'].apply(lambda x:x.replace('\xa0',''))
check_priority=list(main_data['Priority'].unique())
main_data['Priority']=main_data['Priority'].apply(lambda x: x.strip())
main_data['Priority']=main_data['Priority'].apply(lambda x: x.lower())

main_data_check=main_data[['Date','RaceNo','HorseNo','Priority']].copy()

main_data['Priority_trumpcard']=main_data['Priority'].apply(lambda x: ('+' in x)*1)

main_data['Priority_priority']=main_data['Priority'].apply(lambda x: ('*' in x)*1)

main_data['Priority_one']=main_data['Priority'].apply(lambda x: ('1' in x)*1)

main_data['Priority_two']=main_data['Priority'].apply(lambda x: ('2' in x)*1)

main_data_check=main_data[['Date','RaceNo','HorseNo','Priority','Priority_trumpcard','Priority_priority','Priority_one','Priority_two']].copy()


#season stake
check_stake=list(main_data['Season Stakes'].unique())
main_data=main_data.rename(columns={'Season Stakes':'SeasonStakes'})
main_data['SeasonStakes']=main_data['SeasonStakes'].apply(lambda x: x.strip())
check_stake=list(main_data['SeasonStakes'].unique())



#convert distance type
main_data['distance']=main_data['distance'].astype(float)






main_data_check=main_data.loc[(main_data['Date']=='2021-01-24')&(main_data['RaceNo']==8),['Date','RaceNo','Horse_Brand']].copy()










#######################################
##########add custom column############
#######################################

#add a track3 column to indicate ST,AWT,HV
main_data.loc[main_data['truf_dirt']=='All Weather Track','track3']='AWT'
main_data.loc[main_data['truf_dirt']!='All Weather Track','track3']=main_data['track']
list(main_data['track3'].unique())


#add year column
main_data[['Date_year','Date_month','Date_day']]=main_data['Date'].str.split("-",expand=True)
main_data['Date_month']=main_data['Date_month'].apply(lambda x:x[1:] if x[0:1]=='0' else x)
main_data['month_day']=main_data['Date_month']+main_data['Date_day']
main_data['month_day']=main_data['month_day'].astype(int)
main_data_check=main_data.head(10000)
main_data['year_cohord']=main_data['Date_year'].copy()
main_data['year_cohord']=main_data['year_cohord'].astype(int)
main_data['year_cohord_lag1']=main_data['year_cohord']-1
main_data.loc[main_data['month_day']<=831,'year_cohord']=main_data['year_cohord_lag1']

del main_data['year_cohord_lag1']
del main_data['Date_month']
del main_data['Date_day']
main_data_check=main_data.head(40000)
main_data_check.dtypes


list(main_data['rating'].unique())
list(main_data['course'].unique())
list(main_data['race_class'].unique())
list(main_data['surface_condition'].unique())
list(main_data['track'].unique())
list(main_data['racecourse'].unique())
list(main_data['truf_dirt'].unique())
list(main_data['distance'].unique())



#create date_raceno
main_data['Date_RaceNo']=main_data['Date']+'_'+main_data['RaceNo'].astype(str)



main_data2=main_data.loc[main_data['Date']>='2000-09-01',:]
check3=main_data2.groupby(['race_class','Date_year']).size()
check3.to_csv('race_class_year_distribution.csv')




#race class
list(main_data['race_class'].unique())
main_data_check=main_data.loc[main_data['race_class']=='Hong Kong Group One']
main_data_check=main_data.loc[main_data['race_class']=='Group One']

check2=main_data.groupby(['Date','RaceNo']).head(1)
check2=check2['race_class'].value_counts()



main_data_check=main_data.loc[main_data['race_class']=='4 Year Olds',:]
main_data_check1=main_data.loc[main_data['race_class']=='Group One',:]
main_data_check1=main_data.loc[main_data['race_class']=='Group Two',:]
main_data_check2=main_data.loc[main_data['race_class']=='Hong Kong Group One',:]
main_data_check2=main_data.loc[main_data['race_class']=='Hong Kong Group Two',:]
main_data_check2=main_data.loc[main_data['race_class']=='',:]
main_data_check_gr=main_data.loc[main_data['race_class']=='Griffin Race',:]
main_data_check4=main_data.loc[main_data['race_class']=='Class 4 (Special Condition)',:]
main_data_check4=main_data.loc[main_data['race_class']=='3 YEAR OLDS AND UPWARDS',:]
main_data_check4=main_data.loc[main_data['race_class']=='Class 1',:]





main_data_check2=main_data.loc[main_data['race_class']=='Restricted Race',:]
main_data_check2=main_data.loc[main_data['race_class']=='Restricted Race',:]
list(main_data.columns.values)

#create new class defination
main_data['race_class2']=main_data['race_class'].copy()

main_data['race_class2']=main_data['race_class2'].apply(lambda x:x.replace("(Bonus Prize Money)",''))
main_data['race_class2']=main_data['race_class2'].apply(lambda x:x.replace("(Special Condition)",''))
main_data['race_class2']=main_data['race_class2'].apply(lambda x:x.replace("(Restricted)",''))

main_data.loc[(main_data['race_class2']=='2 & 3 YEAR OLDS')|(main_data['race_class2']=='4 Year Olds'),'race_class2']='Restricted Race'


#for race_class with null value, checked that rating is high, so assign it to group G
main_data.loc[(main_data['race_class2']==''),'race_class2']='G'

#also assign 3 YEAR OLDS AND UPWARDS as G
main_data.loc[(main_data['race_class2']=='3 YEAR OLDS AND UPWARDS'),'race_class2']='G'


#group all group 1,2,3 and hong konw group 1,2,3 as G
main_data.loc[(main_data['race_class2']=='Group One')|
              (main_data['race_class2']=='Group Two')|
              (main_data['race_class2']=='Group Three')|
              (main_data['race_class2']=='Hong Kong Group One')|
              (main_data['race_class2']=='Hong Kong Group Two')|
              (main_data['race_class2']=='Hong Kong Group Three'),'race_class2']='G'


main_data['race_class2']=main_data['race_class2'].apply(lambda x: x.strip())
main_data['race_class2']=main_data['race_class2'].apply(lambda x:x.replace(' ',''))

main_data2=main_data.loc[main_data['Date']>='2000-09-01',:]
check3=main_data2.groupby(['race_class2','Date_year']).size()
check3.to_csv('race_class_year_distribution_revised.csv')






#indicate handicap race
main_data['handicap']=main_data['race_name'].apply(lambda x:"HANDICAP" in x)
main_data['handicap']=main_data['handicap']*1





#create distance group

main_data.loc[(main_data['distance']>=1000)&(main_data['distance']<=1000)&(main_data['track3']=='ST'),'distance_group']='ST_10'
main_data.loc[(main_data['distance']>=1200)&(main_data['distance']<=1800)&(main_data['track3']=='ST'),'distance_group']='ST_12141618'
main_data.loc[(main_data['distance']>=2000)&(main_data['distance']<=2400)&(main_data['track3']=='ST'),'distance_group']='ST_202224'

main_data.loc[(main_data['distance']>=1200)&(main_data['distance']<=1200)&(main_data['track3']=='AWT'),'distance_group']='AWT_12'
main_data.loc[(main_data['distance']>=1650)&(main_data['distance']<=2000)&(main_data['track3']=='AWT'),'distance_group']='AWT_161820' #no 2000
main_data.loc[(main_data['distance']>=2400)&(main_data['distance']<=2400)&(main_data['track3']=='AWT'),'distance_group']='AWT_24' #no 2400

main_data.loc[(main_data['distance']>=1000)&(main_data['distance']<=1000)&(main_data['track3']=='HV'),'distance_group']='HV_10'
main_data.loc[(main_data['distance']>=1200)&(main_data['distance']<=1200)&(main_data['track3']=='HV'),'distance_group']='HV_12'
main_data.loc[(main_data['distance']>=1650)&(main_data['distance']<=1800)&(main_data['track3']=='HV'),'distance_group']='HV_1618'
main_data.loc[(main_data['distance']>=2200)&(main_data['distance']<=2200)&(main_data['track3']=='HV'),'distance_group']='HV_22'

main_data.groupby(['track3','distance']).size()
main_data.groupby(['distance_group']).size()





main_data_check=main_data.loc[(main_data['Date']=='2019-09-21')&(main_data['RaceNo']==6),:]#before 1pm withdraw
main_data_check=main_data.loc[(main_data['Date']=='2019-07-10')&(main_data['RaceNo']==9),:]#late withdrawn
main_data_check=main_data.loc[(main_data['Date']=='2019-09-11')&(main_data['RaceNo']==2),:]#withdraw before some mins of start
main_data_check=main_data.head(2000)






##############################################
############read result page##################
##############################################
query='SELECT * from race_result'
raceresult= sql.read_sql(query,connection)
raceresult_check=raceresult.head(30000)


#convert date to %Y-%m-%d format
raceresult['Date']=raceresult['Date'].apply(lambda x:dt.strptime(x,"%Y/%m/%d").strftime("%Y-%m-%d"))

#edit horseno
raceresult=raceresult.rename(columns={'Horse No.':'HorseNo'})
raceresult.HorseNo.unique()
raceresult['HorseNo']=raceresult['HorseNo'].apply(lambda x: x.strip())


#Finish Time
raceresult['Finish Time']=raceresult['Finish Time'].apply(lambda x: x.strip())
raceresult=raceresult.rename(columns={'Finish Time':'FinishTime'})
raceresult.HorseNo.unique()





#raceresult_check=raceresult.loc[raceresult['Date']=='2016-04-16',:].copy()

#LBW
#--- means no value
#- means FP is 1
#TO mean very very far away
#ML also mean very very far away

raceresult['LBW']=raceresult['LBW'].apply(lambda x: x.strip())
lbw_unique=list(raceresult.LBW.unique())
raceresult_check=raceresult.loc[raceresult['LBW']=='-',:]  #if FP is 1, LBW is -

#find out what is below special meaning
lb_list=['+1-1/2','+1/2','+N','+NOSE','+SH','---','HD','ML','N','NOSE','SH','TO']
raceresult['LBW_selected']=raceresult['LBW'].apply(lambda x:any(s in x for s in lb_list))
raceresult_check=raceresult.loc[raceresult['LBW_selected']==True,:]

#if there is a +, which mean that this horse is faster than the first horse, but after appeal, this horse run second
#so i will treat it as win, so replace LB with -
raceresult['LBW_edit']=raceresult['LBW'].copy()
key_temp=raceresult['LBW'].apply(lambda x:'+' in x)
raceresult.loc[key_temp,'LBW_edit']='-'


#raceresult_check=raceresult.loc[raceresult['LBW_edit']=='HD',:]


raceresult.loc[raceresult['LBW_edit']=='-','LBW_edit']='0'
raceresult.loc[raceresult['LBW_edit']=='NOSE','LBW_edit']='0.01'
raceresult.loc[raceresult['LBW_edit']=='SH','LBW_edit']='0.05'
raceresult.loc[raceresult['LBW_edit']=='HD','LBW_edit']='0.1'
raceresult.loc[raceresult['LBW_edit']=='N','LBW_edit']='0.25'
raceresult.loc[raceresult['LBW_edit']=='TO','LBW_edit']='300'
raceresult.loc[raceresult['LBW_edit']=='ML','LBW_edit']='300'
raceresult.loc[raceresult['LBW_edit']=='---','LBW_edit']='99999'

lbw_unique=list(raceresult.LBW_edit.unique())

#make LBW_edit as float

new=raceresult['LBW_edit'].str.split('-',expand=True)
new.loc[pd.isnull(new[1]),1]='99999'
new[0]=new[0].apply(lambda x:float(x.split('/')[0])/float(x.split('/')[1]) if '/' in x else float(x))
new[1]=new[1].apply(lambda x:float(x.split('/')[0])/float(x.split('/')[1]) if '/' in x else float(x))
new.loc[new[0]==99999,2]=99999
new.loc[(new[1]==99999)&(new[0]!=99999),2]=new[0]
new.loc[(new[1]!=99999)&(new[0]!=99999),2]=new[0]+new[1]
raceresult['LBW_edit2']=new[2]

#one horse length is around 3m, so 12 horse length is around 36m, round to 30
raceresult.loc[(raceresult['LBW_edit2']!=99999)&(raceresult['LBW_edit2']>=30),'LBW_edit2']=30
raceresult.loc[(raceresult['LBW_edit2']==99999),'LBW_edit2']=np.nan
lbw_unique=list(raceresult.LBW_edit2.unique())

list(raceresult.columns.values)
check4=raceresult['LBW_edit2'].value_counts()


raceresult_check=raceresult.head(10000)
raceresult_check=raceresult.loc[raceresult['LBW']=='+1-1/2',:]
raceresult_check=raceresult.loc[raceresult['LBW']=='57-1/4',:]


#final win odds
check_final_odd=list(raceresult['Win Odds'].unique())
raceresult=raceresult.rename(columns={'Win Odds':'win_odds_final'})
raceresult.loc[raceresult['win_odds_final']=='---','win_odds_final']=np.nan
raceresult['win_odds_final']=raceresult['win_odds_final'].astype(float)
check_final_odd=list(raceresult['win_odds_final'].unique())




raceresult_check=raceresult.head(1000)

#sum(raceresult['win_odds_final']==0)


#fp
#before 2020, fp is Plc.
#after 2002, fp is Pla. but i rescrapped all data, so use Pla.
raceresult['FP']=raceresult['Pla.'].copy()
raceresult['FP']=raceresult['FP'].apply(lambda x: x.strip())
check_FP=list(raceresult['FP'].unique())






#convert raceno and horseno as int
raceresult['RaceNo']=raceresult['RaceNo'].astype(int)
raceresult=raceresult.loc[~(raceresult['HorseNo']==''),:]  #remove horseno =''
raceresult=raceresult.reset_index(drop=True)
raceresult['HorseNo']=raceresult['HorseNo'].astype(int)



#raceresult=raceresult.drop_duplicates(subset=['Date','RaceNo','HorseNo'], keep="first")



##need to use the weight carry in race result page, because hkjc only make weight carry change in change log and race result not in race card
#raceresult.columns.values
#raceresult=raceresult.rename(columns={'Act. Wt.':'Wt'})
#list(raceresult['Wt'].unique())
#raceresult['Wt']=raceresult['Wt'].apply(lambda x: x.strip())
#raceresult['Wt']=raceresult['Wt'].apply(lambda x: x.lower())
#main_data=pd.merge(main_data,raceresult[['Date','RaceNo','HorseNo','Wt']].copy(),how='left',on=['Date','RaceNo','HorseNo'])
#
#list(main_data['Wt'].unique())
#
#main_data_check=main_data.loc[pd.isnull(main_data['Wt']),:].copy()
#
#main_data['Wt.']=main_data['Wt.'].apply(lambda x: x.strip())
#main_data['Wt.']=main_data['Wt.'].apply(lambda x: x.lower())
#
##in result page, some missing Wt, fill with race card Wt
#main_data.loc[pd.isnull(main_data['Wt']),'Wt']=main_data['Wt.']
#
#main_data['Wt']=pd.to_numeric(main_data['Wt'], errors='coerce')  


#merge finish time, lb, final odd and fp with main_data
main_data=pd.merge(main_data,raceresult[['Date','RaceNo','HorseNo','FinishTime','LBW_edit2','win_odds_final','FP']].copy(),how='left',on=['Date','RaceNo','HorseNo'])
main_data_check=main_data.head(1500)








a=main_data.loc[pd.isnull(main_data['LBW_edit2']),:]



#make LBW_edit2 as four group
target_variable='LBW_edit2'

distinct_year=main_data['year_cohord'].unique()
distinct_year=[i for i in distinct_year if i >=2007]
yy=2007
percentile_cum=pd.DataFrame([])
for yy in distinct_year:
    data_use=main_data.loc[main_data['year_cohord']<yy,target_variable].values
    first_percentile_capture=np.nanpercentile(data_use,25)
    second_percentile_capture=np.nanpercentile(data_use,50)
    third_percentile_capture=np.nanpercentile(data_use,75)
    t1='first_percentile_'+target_variable
    t2='second_percentile_'+target_variable
    t3='third_percentile_'+target_variable
    percentile_cum=percentile_cum.append(pd.DataFrame({'year_cohord':[yy],
                                                       t1:[first_percentile_capture],
                                                       t2:[second_percentile_capture],
                                                       t3:[third_percentile_capture]}))


main_data=pd.merge(main_data,percentile_cum,how='left',on=['year_cohord'])
a_check=main_data.head(1000)


##divied capture odd by 4 groups
#target_group_variable=target_variable+'_group'
#group1=target_variable+'_group1'
#group2=target_variable+'_group2'
#group3=target_variable+'_group3'
#group4=target_variable+'_group4'
#
#main_data.loc[main_data[target_variable]<=main_data[t1],target_group_variable]=group1
#main_data.loc[(main_data[target_variable]>main_data[t1])&
#              (main_data[target_variable]<=main_data[t2]),target_group_variable]=group2
#main_data.loc[(main_data[target_variable]>main_data[t2])&
#              (main_data[target_variable]<=main_data[t3]),target_group_variable]=group3
#main_data.loc[(main_data[target_variable]>main_data[t3]),target_group_variable]=group4
#
#
#a=main_data.loc[:,['Date_RaceNo',target_variable,target_group_variable]].copy()




































#merge 25s win odds
connection_odd=create_engine("mysql+pymysql://root:pp222#@localhost:3306/Odds_store")  
win_odds_25s_after_start_nice_format= sql.read_sql('SELECT * from win_odds_25s_nice_format',connection_odd)    
win_odds_25s_after_start_nice_format.dtypes
main_data=pd.merge(main_data,win_odds_25s_after_start_nice_format[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','HorseNo'],right_on=['Date','RaceNo','HorseNo'])





check=list(main_data['win_odd_25s'].unique())

x=main_data.loc[(main_data['Date']=='2019-09-21')&(main_data['RaceNo']==1)]


#create late withdrawn horse column (only indicate that horse)
main_data.loc[(main_data['win_odd_25s']>0)&pd.isnull(main_data['win_odds_final']),'late_withdrawn_horse']=1
#main_data['late_withdrawn_horse']=main_data['late_withdrawn_horse'].fillna(0)
main_data_late_withdrawn_horse=main_data.loc[main_data['late_withdrawn_horse']==1,:]

#create late withdrawn race column (indicate whole race)
late_races=main_data_late_withdrawn_horse[['Date','RaceNo']].copy()
late_races=late_races.drop_duplicates()
late_races['late_withdrawn_race']=1
main_data=pd.merge(main_data,late_races,how='left',on=['Date','RaceNo'])
#main_data['late_withdrawn_race']=main_data['late_withdrawn_race'].fillna(0)
main_data_check=main_data.head(3000)


main_data_check=main_data.loc[(main_data['Date']=='2016-07-10')&(main_data['RaceNo']==2),:]



#remove withdrawn
#but keep the one in late withdraw races, because when pressing the button
#the horse still there
main_data=main_data.loc[~((main_data['withdrawn']=='withdrawn')&(main_data['late_withdrawn_horse']!=1)),:]
list(main_data.columns.values)
main_data.dtypes
main_data_check=main_data.head(30000)



#create deadheat
#don't know why error TypeError: argument of type 'float' is not iterable
try:
    main_data['deadheat_horse']=main_data['FP'].apply(lambda x:1 if 'DH' in x else 0) 
except:
    main_data['deadheat_horse']=main_data['FP'].apply(lambda x:1 if 'DH' in str(x) else 0)
main_data_deadheat_race=main_data.loc[main_data['deadheat_horse']==1,['Date','RaceNo']]
main_data_deadheat_race=main_data_deadheat_race.drop_duplicates()
main_data_deadheat_race['deadheat_race']=1
main_data=pd.merge(main_data,main_data_deadheat_race,how='left',on=['Date','RaceNo'])
main_data_check=main_data.head(30000)







#remove races having DISQ horses (18 races only)
#note that hkjc only update DISQ hrs after several weeks coz need to wewet for medical report
#so live update post race results will not have DISQ horses 
main_data_check=main_data.loc[(main_data['FP']=='DISQ')|(main_data['FP']=='TNP'),:]
main_data_disq_race=main_data.loc[(main_data['FP']=='DISQ'),['Date','RaceNo']]
main_data_disq_race=main_data_disq_race.drop_duplicates()
main_data_disq_race['disq_race']=1
main_data=pd.merge(main_data,main_data_disq_race,how='left',on=['Date','RaceNo'])
main_data=main_data.loc[~(main_data['disq_race']==1),:]



#edit FP
main_data['FP_original']=main_data['FP'].copy()
main_data['FP']=main_data['FP'].apply(lambda x:str(x).replace(' DH',''))

main_data.dtypes

check_fp=main_data.FP.unique()




#create DNF_UR_FE_TNP_PU races indicator
DNF_UR_FE_TNP_PU=['DNF','UR','FE','TNP','PU']
main_data['DNF_UR_FE_TNP_PU_horse']=main_data['FP'].apply(lambda x: 1*any(k in x for k in DNF_UR_FE_TNP_PU))





main_data_duftp_race=main_data.loc[main_data['DNF_UR_FE_TNP_PU_horse']==1,['Date','RaceNo']]
main_data_duftp_race=main_data_duftp_race.drop_duplicates()
main_data_duftp_race['DNF_UR_FE_TNP_PU_race']=1
main_data=pd.merge(main_data,main_data_duftp_race,how='left',on=['Date','RaceNo'])


check_fp=main_data.loc[main_data['DNF_UR_FE_TNP_PU_race']==1,:]


data_temp=main_data.loc[(main_data['Date']=='2018-04-08')&(main_data['RaceNo']==2),:].reset_index(drop=True)
data_temp=main_data.loc[(main_data['Date']=='2019-07-10')&(main_data['RaceNo']==7),:].reset_index(drop=True)
list(data_temp.columns.values)



##treat 'DNF','UR','FE','TNP','PU' as FP last
#def DNF_UR_FE_TNP_PU_treat_last(data_temp):
#    if data_temp['DNF_UR_FE_TNP_PU_race'].values[0]==1:
#        all_fp=list(data_temp.FP.values)
#        all_fp_old=all_fp.copy()
#        #remove text in all_fp
#        for k in all_fp_old:
#            if any(x==k for x in ['DNF','UR','FE','TNP','PU','WXNR', 'WV-A', 'WX-A','WV','WX','WR']):#note that till to now no data is WR
#                    all_fp.remove(k)
#        #convert to integer
#        all_fp=[int(x) for x in all_fp]
#        fp_max=max(all_fp)
#        fp_start=fp_max+1
#        #i=0
#        for i in range(0,data_temp.shape[0]):
#            if data_temp[i:i+1]['FP'].values[0] in DNF_UR_FE_TNP_PU:
#                data_temp.loc[i,'FP']=str(fp_start)
#                fp_start=fp_start+1
#    return data_temp
##a=dt.now()
#main_data=main_data.groupby(['Date','RaceNo']).apply(lambda x:DNF_UR_FE_TNP_PU_treat_last(x.reset_index(drop=True)))
##b=dt.now()
##b-a
#main_data=main_data.reset_index(drop=True)
#main_data=main_data.sort_values(by=['Date','RaceNo','HorseNo'],ascending=[False,True,True])
#main_data=main_data.reset_index(drop=True)
#main_data_check=main_data.head(5000)
#
#check_fp=main_data.loc[main_data['DNF_UR_FE_TNP_PU_race']==1,['Date','RaceNo','HorseNo','FP','FP_original']]




#data_temp=main_data.loc[(main_data['Date']=='2021-02-10')&(main_data['RaceNo']==8),:]

#treat 'DNF','UR','FE','TNP','PU' as FP last
def DNF_UR_FE_TNP_PU_treat_last(data_temp):
    index_out=data_temp.index
    #print(index_out)
    data_temp=data_temp.reset_index(drop=True)
    if data_temp['DNF_UR_FE_TNP_PU_race'].values[0]==1:
        all_fp=list(data_temp.FP.values)
        all_fp_old=all_fp.copy()
        #remove text in all_fp
        for k in all_fp_old:
            if any(x==k for x in ['DNF','UR','FE','TNP','PU','WXNR', 'WV-A', 'WX-A','WV','WX','WR']):#note that till to now no data is WR
                    all_fp.remove(k)
        #convert to integer
        all_fp=[int(x) for x in all_fp]
        fp_max=max(all_fp)
        fp_start=fp_max+1
        #i=0
        for i in range(0,data_temp.shape[0]):
            if data_temp[i:i+1]['FP'].values[0] in DNF_UR_FE_TNP_PU:
                data_temp.loc[i,'FP']=str(fp_start)
                fp_start=fp_start+1
    output=pd.Series(data_temp['FP'].values,index=index_out)
    return output
#a=dt.now()
#a=main_data[0:100].copy()
#using groupby, group_by_columns will be used as past of the index, disable this index using group_keys=False,#
#so only use the index defined in the function
#but if only select one column as input after groupby, like below newH, group_by_columns will not used as index
#out=main_data.groupby(['Date','RaceNo'],group_keys=False).apply(lambda x:DNF_UR_FE_TNP_PU_treat_last(x)) 
out=main_data[['Date','RaceNo','FP','DNF_UR_FE_TNP_PU_race']].groupby(['Date','RaceNo'],group_keys=False).apply(lambda x:DNF_UR_FE_TNP_PU_treat_last(x)) 


main_data['FP']=out
#b=dt.now()
#b-a


main_data_check=main_data.loc[main_data['FP']=='',:]


#convert FP as float
main_data['FP']=main_data['FP'].apply(lambda x:x if x.isnumeric() else np.nan)
main_data['FP']=main_data['FP'].astype(float)


main_data=main_data.reset_index(drop=True)
main_data=main_data.sort_values(by=['Date','RaceNo','HorseNo'],ascending=[False,True,True])
main_data=main_data.reset_index(drop=True)
main_data_check=main_data.head(5000)

check_fp=main_data.loc[main_data['DNF_UR_FE_TNP_PU_race']==1,['Date','RaceNo','HorseNo','FP','FP_original']]









#
##below is an example to use dark
#
##data_temp=main_data.loc[(main_data['Date']=='2019-09-01')&(main_data['RaceNo']==1),:]
#def DNF_UR_FE_TNP_PU_treat_last(data_temp):
#    index_out=data_temp.index
#    data_temp=data_temp.reset_index(drop=True)
#    if data_temp['DNF_UR_FE_TNP_PU_race'].values[0]==1:
#        all_fp=list(data_temp.FP.values)
#        all_fp_old=all_fp.copy()
#        #remove text in all_fp
#        for k in all_fp_old:
#            if any(x==k for x in ['DNF','UR','FE','TNP','PU','WXNR', 'WV-A', 'WX-A','WV','WX','WR']):#note that till to now no data is WR
#                    all_fp.remove(k)
#        #convert to integer
#        all_fp=[int(x) for x in all_fp]
#        fp_max=max(all_fp)
#        fp_start=fp_max+1
#        #i=0
#        for i in range(0,data_temp.shape[0]):
#            if data_temp[i:i+1]['FP'].values[0] in DNF_UR_FE_TNP_PU:
#                data_temp.loc[i,'FP']=str(fp_start)
#                fp_start=fp_start+1
#    output=pd.DataFrame({'FP':data_temp['FP'].values},index=index_out)   #output must be a dataframe with name specified in meta
#    return output
#
#from dask import dataframe as dd
#from dask.multiprocessing import get 
#from dask.distributed import Client
#
#
#
##client = Client(n_workers=8, threads_per_worker=8)
#client = Client()
#
#ddf = dd.from_pandas(main_data[['Date','RaceNo','FP','DNF_UR_FE_TNP_PU_race']].copy(), npartitions=5)
#
#meta = [("FP", str)]
#g=ddf.groupby(['Date','RaceNo'],group_keys=False).apply(lambda x:DNF_UR_FE_TNP_PU_treat_last(x),meta=meta)
#
#out=g.compute()
#
#main_data['FP']=out












#add new horse columns
#when first time appear, it is newH regardless of late wv or not,
#to be specific, when join the race defined by the capture odd (when bet, it is there), will be treated as newH
#data=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143',:]
#
#
#def new_horse_column(data):
#    temp=np.zeros((data.shape[0],))
#    temp[-1]=1
#    data['newH']=temp
#    return data
#
#
#main_data=main_data.groupby(['Horse_Brand']).apply(new_horse_column)
#
#main_data['newH']=main_data['newH'].astype(int)
#main_data['newH']=main_data['newH'].astype(str)
#main_data=main_data.reset_index(drop=True)
#main_data_check=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143',:]












#data=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143','one']
def new_horse_column(data):
    temp=np.zeros((len(data),))
    temp[-1]=1
    return pd.Series(temp,index=data.index)

main_data['one']=1
#note that because only one column selected ('one'), index will not use horse_brand, so group_keys=False is useless
main_data['newH']=main_data.groupby(['Horse_Brand'],group_keys=False)['one'].apply(new_horse_column)




#main_data_check=main_data.head(1000)

#main_data.dtypes

main_data['newH']=main_data['newH'].astype(int)
#main_data['newH']=main_data['newH'].astype(str)
main_data=main_data.reset_index(drop=True)
main_data_check=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143',:]






def find_newS(DF):
    temp=np.full((DF.shape[0],),0.0)
    temp[temp.shape[0]-1]=1.0
    output=pd.Series(temp,index=DF.index)
    return output


#add newS column
main_data['newS'] = main_data.groupby(['Horse_Brand','track3'],group_keys=False).apply(lambda x:find_newS(x))   #make newH column only

main_data['newS']=main_data['newS'].astype(int)
#main_data['newS']=main_data['newS'].astype(str)
main_data=main_data.reset_index(drop=True)
main_data_check=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143',:]





#make sts (total number ]of horse when the race start)
def sts_column(data):
    temp=data.shape[0]
    return pd.Series([temp]*temp,index=data.index)

main_data['one']=1
main_data['sts']=main_data.groupby(['Date','RaceNo'])['one'].apply(sts_column)
#main_data=main_data.sort_values(by=['Date','RaceNo','HorseNo'],ascending=[False,True,True])

main_data_check=main_data.head(3000)



data=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143',['Date']]
#add last race date column
def last_race_date(data):
    data_index=data.index
    temp=data['Date'].shift(-1).values
    output=pd.Series(temp,index=data_index)
    return output    
main_data['last_race_date']=main_data[['Date','Horse_Brand']].groupby(['Horse_Brand'],group_keys=False).apply(lambda x:last_race_date(x))
main_data.loc[pd.isnull(main_data['last_race_date']),'last_race_date']=main_data['Date']

main_data_check=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143',['Date','last_race_date']]



#add datesince_last_race
x='2019-10-10'
y='2019-10-01'
main_data['day_since_last_race']=main_data.apply(lambda row:(dt.strptime(str(row['Date']),"%Y-%m-%d")-dt.strptime(str(row['last_race_date']),"%Y-%m-%d")).days ,axis=1)
main_data_check=main_data.loc[main_data['Horse_Brand']=='nashashuk_V143',['Date','last_race_date','day_since_last_race']]
#a=main_data.loc[main_data['day_since_last_race']==930,:]



#create win place indicate column
main_data_check=main_data.head(10000)


main_data.loc[main_data['FP']==1,'win_indicator']=1
main_data['win_indicator']=main_data['win_indicator'].fillna(0)

main_data.loc[(main_data['FP']>=1)&(main_data['FP']<=3),'pla_indicator']=1
main_data['pla_indicator']=main_data['pla_indicator'].fillna(0)


main_data_remove_late_wv=main_data.loc[(main_data['late_withdrawn_horse']==1),:]




#create nfp column
main_data['nfp']=main_data['FP']/(main_data['sts']+1)
main_data['nfp'].unique()


#normalize fp
main_data['FP_normalized']=main_data.groupby(['Date','RaceNo'])['FP'].apply(lambda x:(x-x.min())/(x.max()-x.min()))



#2015/05/24&Racecourse=ST&RaceNo=9 horse 3, hkjc is wrong, draw is 11 not 15
main_data_check=main_data.loc[main_data['Draw']=='15',:]
main_data.loc[(main_data['Date']=='2015-05-24')&(main_data['RaceNo']==9)&(main_data['HorseNo']==3),'Draw']='11'


# convert draw as int
main_data.loc[main_data['Draw']=='-','Draw']=np.nan
main_data['Draw']=main_data['Draw'].astype(float)

#create nbp
main_data['nbp']=(main_data['Draw']+1)/(main_data['sts']+1)



#create win odd 25s (after 20151004) mix with final (before 20151004)
main_data.loc[main_data['Date']<'2015-10-04','win_odd_25s_mix_final']=main_data['win_odds_final']
main_data.loc[main_data['Date']>='2015-10-04','win_odd_25s_mix_final']=main_data['win_odd_25s']
main_data.dtypes


#create prob
main_data['win_prob_25s_mix_final']=main_data.groupby(['Date','RaceNo'])['win_odd_25s_mix_final'].apply(lambda x:1/x/(np.nansum(1/x)))

main_data['win_probs_final']=main_data.groupby(['Date','RaceNo'])['win_odds_final'].apply(lambda x:1/x/(np.nansum(1/x)))

main_data_check=main_data.head(10000)






#create odd rank
data=main_data.loc[main_data['Date_RaceNo']=='2016-07-10_2',:]

def odd_indicate1(data):
    temp=data['win_odd_25s_mix_final'].values==data['win_odd_25s_mix_final'].values.min()
    output=pd.Series(temp*1,index=data.index)
    return output
    

main_data['capture_odd_lowest_indicate']=main_data.groupby(['Date_RaceNo'],group_keys=False).apply(lambda x:odd_indicate1(x))


def rank_odd(data):
    ll=data['win_odd_25s_mix_final'].values
    ll[np.isnan(ll)]=0 #becaue there may be all single race nan, coz not yet updated odds
    temp=[sorted(ll).index(x)+1 for x in ll]
    output=pd.Series(temp,index=data.index)
    return output


l = [-1, 3, 2, 0,0]
[sorted(l).index(x) for x in l]

main_data['odd_rank_all']=main_data.groupby(['Date_RaceNo'],group_keys=False).apply(lambda x:rank_odd(x))


a=main_data[['Date_RaceNo','win_odd_25s_mix_final','capture_odd_lowest_indicate','odd_rank_all']].copy()








#x=DJI_use.loc[DJI_use['year']==1990,:]
#asset_name='DJI'
#find percentile of capture

distinct_year=main_data['year_cohord'].unique()
distinct_year=[i for i in distinct_year if i >=2007]
yy=2007
percentile_cum=pd.DataFrame([])
for yy in distinct_year:
    data_use=main_data.loc[main_data['year_cohord']<yy,'win_odd_25s_mix_final'].values
    first_percentile_capture=np.nanpercentile(data_use,25)
    second_percentile_capture=np.nanpercentile(data_use,50)
    third_percentile_capture=np.nanpercentile(data_use,75)
    percentile_cum=percentile_cum.append(pd.DataFrame({'year_cohord':[yy],
                                                       'first_percentile_capture':[first_percentile_capture],
                                                       'second_percentile_capture':[second_percentile_capture],
                                                       'third_percentile_capture':[third_percentile_capture]}))


main_data=pd.merge(main_data,percentile_cum,how='left',on=['year_cohord'])
a_check=main_data.head(1000)


#divied capture odd by 4 groups
main_data.loc[main_data['win_odd_25s_mix_final']<=main_data['first_percentile_capture'],'capture_group']='Capture1'
main_data.loc[(main_data['win_odd_25s_mix_final']>main_data['first_percentile_capture'])&
              (main_data['win_odd_25s_mix_final']<=main_data['second_percentile_capture']),'capture_group']='Capture2'
main_data.loc[(main_data['win_odd_25s_mix_final']>main_data['second_percentile_capture'])&
              (main_data['win_odd_25s_mix_final']<=main_data['third_percentile_capture']),'capture_group']='Capture3'
main_data.loc[(main_data['win_odd_25s_mix_final']>main_data['third_percentile_capture']),'capture_group']='Capture4'

a=main_data[['Date_RaceNo','win_odd_25s_mix_final','capture_odd_lowest_indicate','odd_rank_all','capture_group']].copy()







##########################################################
#####################read mset############################
##########################################################
query='SELECT * from mset'
mset= sql.read_sql(query,connection)

mset_check=mset.loc[mset['Date']=='2019/09/21',:]  


mset[['Sec1_rp','Sec1_lb','Sec1_time']]=mset.sec1.str.split("@",expand=True)
mset[['Sec2_rp','Sec2_lb','Sec2_time']]=mset.sec2.str.split("@",expand=True)
mset[['Sec3_rp','Sec3_lb','Sec3_time']]=mset.sec3.str.split("@",expand=True)
mset[['Sec4_rp','Sec4_lb','Sec4_time']]=mset.sec4.str.split("@",expand=True)
mset[['Sec5_rp','Sec5_lb','Sec5_time']]=mset.sec5.str.split("@",expand=True)
mset[['Sec6_rp','Sec6_lb','Sec6_time']]=mset.sec6.str.split("@",expand=True)


#sectional position
list(mset['Sec6_rp'].unique())
list(mset['Sec5_rp'].unique())
list(mset['Sec4_rp'].unique())
list(mset['Sec3_rp'].unique())
list(mset['Sec2_rp'].unique())
list(mset['Sec1_rp'].unique())
mset_check=mset.loc[mset['Sec1_rp']=='',:]  

#use wither below method 1 or 2 to convert string to integer 
# note that in int, cannot use np.nan, so use 99999 or 999999
#method 1
mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']]=mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']].replace('',99999)
mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']]=mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']].astype(int)

##method 2 #this will convert all non numeric to np.nan
#mset['Sec1_rp'] = pd.to_numeric(mset['Sec1_rp'], errors='coerce')
#mset['Sec2_rp'] = pd.to_numeric(mset['Sec2_rp'], errors='coerce')
#mset['Sec3_rp'] = pd.to_numeric(mset['Sec3_rp'], errors='coerce')
#mset['Sec4_rp'] = pd.to_numeric(mset['Sec4_rp'], errors='coerce')
#mset['Sec5_rp'] = pd.to_numeric(mset['Sec5_rp'], errors='coerce')
#mset['Sec6_rp'] = pd.to_numeric(mset['Sec6_rp'], errors='coerce') 
#                                                                  
## note that in int, no nan allowed, so use 99999 or 999999
#mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']]=mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']].replace(np.nan,99999)
#mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']]=mset[['Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp']].astype(int)



#sectional time
check=list(mset['Sec6_time'].unique())
list(mset['Sec5_time'].unique())
list(mset['Sec4_time'].unique())
list(mset['Sec3_time'].unique())
list(mset['Sec2_time'].unique())
list(mset['Sec1_time'].unique())
mset_check=mset.loc[mset['Sec6_time']=='',]
mset_check=mset.head(50000)


#replace blank '' with np.nan
mset[['Sec1_time','Sec2_time','Sec3_time','Sec4_time','Sec5_time','Sec6_time']]=mset[['Sec1_time','Sec2_time','Sec3_time','Sec4_time','Sec5_time','Sec6_time']].replace('',np.nan)
#convert to float
mset[['Sec1_time','Sec2_time','Sec3_time','Sec4_time','Sec5_time','Sec6_time']]=mset[['Sec1_time','Sec2_time','Sec3_time','Sec4_time','Sec5_time','Sec6_time']].astype(float)





#sectional margin
colname='Sec3_lb'
colname_rp='Sec3_rp'
round_limit=30
def modify_lb(colname,colname_rp,round_limit):
    mset[colname]=mset[colname]
    #LBW
    #'' means no value
    #note that for sectional lb, sectional fp=1 and sectional fp=2 are the same 
    #TO mean very very far away
    #ML also mean very very far away
    mset.loc[pd.isnull(mset[colname]),colname]='99999'    
    mset[colname]=mset[colname].apply(lambda x: x.strip())
    lbw_unique=list(mset[colname].unique())
    mset_check=mset.loc[mset[colname]=='',:]
    
    #find out what is below special meaning
    lb_list=['+1-1/2','+1/2','+N','+NOSE','+SH','---','HD','ML','N','NOSE','SH','TO','H','-NOSE','-SH','-N','NK','DH']
    mset['LBW_selected']=mset[colname].apply(lambda x:any(s in x for s in lb_list))
    mset_check=mset.loc[mset['LBW_selected']==True,:]
    
    
    new_col_name1=colname+'_edit'
    new_col_name2=colname+'_edit2'
    mset[new_col_name1]=mset[colname].copy()
    
    
    if colname=='Sec4_lb':
        #one race there is + and - in two horses 20181007 race 2. but they are rp1 and rp2 so remove + and - (this is hkjc mistake)
        mset.loc[(mset['Date']=='2018/10/07')&(mset['RaceNo']=='2')&(mset['HorseNo']=='5'),new_col_name1]='NOSE'
        mset.loc[(mset['Date']=='2018/10/07')&(mset['RaceNo']=='2')&(mset['HorseNo']=='10'),new_col_name1]='NOSE'
        #hkjc string wrong
        mset.loc[(mset['Date']=='2008/12/20')&(mset['RaceNo']=='6')&(mset['HorseNo']=='6'),new_col_name1]='13-3/4'   
        mset.loc[(mset['Date']=='2012/10/10')&(mset['RaceNo']=='4')&(mset['HorseNo']=='5'),new_col_name1]='7-1/2'   

    if colname=='Sec3_lb':
        #hkjc is wrong, it use NK, but should be N
        mset.loc[(mset['Date']=='2008/04/06')&(mset['RaceNo']=='2')&(mset['HorseNo']=='4'),new_col_name1]='N'  
        mset.loc[(mset['Date']=='2008/04/06')&(mset['RaceNo']=='2')&(mset['HorseNo']=='1'),new_col_name1]='N'  
        #hkjc string wrong
        mset.loc[(mset['Date']=='2009/05/27')&(mset['RaceNo']=='3')&(mset['HorseNo']=='4'),new_col_name1]='7-1/4' 
    #because gradually find that some cases hkjc added + and - in first and second horse, so remove them automatically
    mset[new_col_name1]=mset[new_col_name1].apply(lambda x:x[1:] if any(k==x[0:1] for k in ['+','-']) else x)
    

    
    mset.loc[(mset[colname_rp]=='1')|(mset[new_col_name1]=='DH'),new_col_name1]='0'
    mset.loc[mset[new_col_name1]=='NOSE',new_col_name1]='0.01'
    mset.loc[mset[new_col_name1]=='SH',new_col_name1]='0.05'
    mset.loc[mset[new_col_name1]=='HD',new_col_name1]='0.1'
    mset.loc[mset[new_col_name1]=='H',new_col_name1]='0.1'   #assume H is head first
    mset.loc[mset[new_col_name1]=='N',new_col_name1]='0.25'
    mset.loc[mset[new_col_name1]=='TO',new_col_name1]='300'
    mset.loc[mset[new_col_name1]=='ML',new_col_name1]='300'
    mset.loc[mset[new_col_name1]=='',new_col_name1]='99999'
    
    lbw_unique=list(mset[new_col_name1].unique())
    
    #make LBW_edit as float
    check=list(mset[new_col_name1].unique())
    new=mset[new_col_name1].str.split('-',expand=True)
    new.loc[pd.isnull(new[1]),1]='99999'
    new[0]=new[0].apply(lambda x:float(x.split('/')[0])/float(x.split('/')[1]) if '/' in x else float(x))
    new[1]=new[1].apply(lambda x:float(x.split('/')[0])/float(x.split('/')[1]) if '/' in x else float(x))
    
    #mset_check=mset[100211:100212]
    
    new.loc[new[0]==99999,2]=99999
    new.loc[(new[1]==99999)&(new[0]!=99999),2]=new[0]
    new.loc[(new[1]!=99999)&(new[0]!=99999),2]=new[0]+new[1]
    mset[new_col_name2]=new[2]
    
    #one horse length is around 3m, so 12 horse length is around 36m, round to 30
    mset.loc[(mset[new_col_name2]!=99999)&(mset[new_col_name2]>=round_limit),new_col_name2]=round_limit
    mset.loc[(mset[new_col_name2]==99999),new_col_name2]=np.nan
    lbw_unique=list(mset[new_col_name2].unique())
    del mset[new_col_name1] 



modify_lb('Sec1_lb','Sec1_rp',30)
modify_lb('Sec2_lb','Sec2_rp',30)
modify_lb('Sec3_lb','Sec3_rp',30)
modify_lb('Sec4_lb','Sec4_rp',30)
modify_lb('Sec5_lb','Sec5_rp',30)
modify_lb('Sec6_lb','Sec6_rp',30)

mset_check=mset.head(10000)

mset_check=mset.loc[(mset['Date']=='2019/02/07')&(mset['RaceNo']=='9'),:]



#13824



#convert date to %Y-%m-%d format
mset['Date']=mset['Date'].apply(lambda x:dt.strptime(x,"%Y/%m/%d").strftime("%Y-%m-%d")) #string to date then to string again

#edit raceno
mset['RaceNo']=mset['RaceNo'].astype(int)

#edit horseno
mset['HorseNo']=mset['HorseNo'].astype(int)


#merge to main data
main_data=pd.merge(main_data,mset[['Date','RaceNo','HorseNo','Sec1_rp','Sec2_rp','Sec3_rp','Sec4_rp','Sec5_rp','Sec6_rp',
                                    'Sec1_lb_edit2','Sec2_lb_edit2','Sec3_lb_edit2','Sec4_lb_edit2','Sec5_lb_edit2','Sec6_lb_edit2',
                                    'Sec1_time','Sec2_time','Sec3_time','Sec4_time','Sec5_time','Sec6_time']].copy(),how='left',
                                     on=['Date','RaceNo','HorseNo'])

main_data_check=main_data.head(10000)








main_data.loc[main_data['Sec1_rp']==99999,'Sec1_rp']=np.nan
main_data.loc[main_data['Sec2_rp']==99999,'Sec2_rp']=np.nan
main_data.loc[main_data['Sec3_rp']==99999,'Sec3_rp']=np.nan
main_data.loc[main_data['Sec4_rp']==99999,'Sec4_rp']=np.nan
main_data.loc[main_data['Sec5_rp']==99999,'Sec5_rp']=np.nan
main_data.loc[main_data['Sec6_rp']==99999,'Sec6_rp']=np.nan
main_data['Sec6_rp'].unique()

main_data.loc[main_data['Sec1_time']==0,'Sec1_time']=np.nan
main_data.loc[main_data['Sec2_time']==0,'Sec2_time']=np.nan
main_data.loc[main_data['Sec3_time']==0,'Sec3_time']=np.nan
main_data.loc[main_data['Sec4_time']==0,'Sec4_time']=np.nan
main_data.loc[main_data['Sec5_time']==0,'Sec5_time']=np.nan
main_data.loc[main_data['Sec6_time']==0,'Sec6_time']=np.nan
























































##########################################################
#####################read track work######################
##########################################################
fn ='./h5_data/trackwork.hdf5'
store = pd.HDFStore(fn)
trackwork= store.select('trackwork_dataframe')
store.close()

trackwork_check=trackwork.head(100000)
trackwork_check=trackwork.loc[trackwork['Date_racecard']=='2019/10/30',:]

trackwork_check=trackwork.loc[trackwork['HorseName_Brand']=='regency darling_T138',:]

a=pd.crosstab(trackwork_check['RaceNo_racecard'],trackwork_check['HorseNo_racecard'])


#convert date to %Y-%m-%d format
trackwork['Date']=trackwork['Date'].apply(lambda x:dt.strptime(x,"%d/%m/%Y").strftime("%Y-%m-%d")) #string to date then to string again

trackwork_check=trackwork.head(1000)

trackwork_check=trackwork.loc[trackwork['HorseName_Brand']=="i'm the conquist_B155",:]

list(trackwork.columns.values)
trackwork_swim=trackwork.loc[trackwork['Type']=='Swimming',['Date','HorseName_Brand']]
trackwork_swim_check=trackwork_swim.head(100)

trackwork_trotting=trackwork.loc[trackwork['Type']=='Trotting',['Date','HorseName_Brand']]
trackwork_gallop=trackwork.loc[trackwork['Type']=='Gallop',['Date','HorseName_Brand']]

trackwork_barrier=trackwork.loc[trackwork['Type']=='Barrier Trial',['Date','HorseName_Brand']]

trackwork_barrier=trackwork.loc[trackwork['Type']=='Barrier Trial',['Date','HorseName_Brand','Date_racecard']]


main_data_temp=main_data[['Date','RaceNo','Horse_Brand']].copy()
main_data_temp['target_date_lag_1']=main_data_temp['Date'].apply(lambda x:(dt.strptime(x,'%Y-%m-%d')-timedelta(days=1)).strftime("%Y-%m-%d"))
main_data_temp['target_date_lag_2']=main_data_temp['Date'].apply(lambda x:(dt.strptime(x,'%Y-%m-%d')-timedelta(days=2)).strftime("%Y-%m-%d"))
main_data_temp['target_date_lag_14']=main_data_temp['Date'].apply(lambda x:(dt.strptime(x,'%Y-%m-%d')-timedelta(days=14)).strftime("%Y-%m-%d"))
main_data_temp['target_date_lag_22']=main_data_temp['Date'].apply(lambda x:(dt.strptime(x,'%Y-%m-%d')-timedelta(days=22)).strftime("%Y-%m-%d"))
main_data_temp['target_date_lag_30']=main_data_temp['Date'].apply(lambda x:(dt.strptime(x,'%Y-%m-%d')-timedelta(days=30)).strftime("%Y-%m-%d"))





main_data_input=main_data_temp.copy()
track_work_type_df=trackwork_swim.copy()
first_lag='target_date_lag_2'
period=14
output_var_name='swim_14'

def track_function_count(main_data_input,track_work_type_df,first_lag,period,output_var_name):
    main_temp=pd.merge(main_data_input,track_work_type_df,how='left',left_on=['Horse_Brand'],right_on=['HorseName_Brand'],suffixes=('','_tr'))
    #only select out trackwork day within lag 2 and lag 14,22,30
    period_variable='target_date_lag_'+str(period)
    main_temp_2=main_temp[(main_temp['Date_tr']<=main_temp[first_lag])&(main_temp['Date_tr']>=main_temp[period_variable])]
    main_temp_2_count=pd.DataFrame(main_temp_2.groupby(['Date','RaceNo','HorseName_Brand']).size())
    
    main_temp_2_count=main_temp_2_count.rename(columns={0:output_var_name})
    
    #split group_key to df
    df=pd.DataFrame(main_temp_2_count.index.values.tolist())
    main_temp_2_count=pd.concat([main_temp_2_count.reset_index(drop=True),df],axis=1)
    main_temp_2_count=main_temp_2_count.rename(columns={0:'Date',1:'RaceNo',2:'Horse_Brand'})
    main_data_input=pd.merge(main_data_input,main_temp_2_count,on=['Date','RaceNo','Horse_Brand'],how='left')
    main_data_input[output_var_name]=main_data_input[output_var_name].fillna(0)
    return main_data_input


#for sunday races, sat swim record will be updated on Sunday 3pm
#so sat record cannot be got, so use lag 2 
main_data_temp=track_function_count(main_data_temp,trackwork_swim,'target_date_lag_2',14,'swim_14')
#main_data_temp=track_function_count(main_data_temp,trackwork_swim,'target_date_lag_2',22,'swim_22')
#main_data_temp=track_function_count(main_data_temp,trackwork_swim,'target_date_lag_2',30,'swim_30')

#trotting cause memory error
#main_data_temp=track_function_count(main_data_temp,trackwork_trotting,'target_date_lag_1',14,'trotting_14')
#main_data_temp=track_function_count(main_data_temp,trackwork_trotting,'target_date_lag_1',22,'trotting_22')
#main_data_temp=track_function_count(main_data_temp,trackwork_trotting,'target_date_lag_1',30,'trotting_30')


main_data_temp=track_function_count(main_data_temp,trackwork_gallop,'target_date_lag_1',14,'gallop_14')
#main_data_temp=track_function_count(main_data_temp,trackwork_gallop,'target_date_lag_1',22,'gallop_22')
#main_data_temp=track_function_count(main_data_temp,trackwork_gallop,'target_date_lag_1',30,'gallop_30')

#main_data_temp=track_function_count(main_data_temp,trackwork_barrier,'target_date_lag_1',14,'barrier_14')
#main_data_temp=track_function_count(main_data_temp,trackwork_barrier,'target_date_lag_1',30,'barrier_30')





#main_data_temp=main_data_temp[['Date','RaceNo','Horse_Brand','swim_14','swim_22','swim_30','trotting_14','trotting_22','trotting_30','gallop_14','gallop_22','gallop_30','barrier_14','barrier_30']].copy()
main_data_temp=main_data_temp[['Date','RaceNo','Horse_Brand','swim_14','gallop_14']].copy()

main_data=pd.merge(main_data,main_data_temp,how='left',on=['Date','RaceNo','Horse_Brand'])


#main_data_check=main_data[['Date','RaceNo','Horse_Brand','swim_14','swim_22','swim_30','trotting_14','trotting_22','trotting_30','gallop_14','gallop_22','gallop_30','barrier_14','barrier_30']].copy()

del trackwork
del trackwork_swim
del trackwork_trotting
del trackwork_gallop
del trackwork_barrier




#period=14
#data=main_data.loc[(main_data['Date']=='2019-10-20'),['Date','Horse_Brand']]
#def trackwork_swim_extract(data,period):
#    data_index=data.index
#    #data=data.reset_index(drop=True)
#    target_horse=data['Horse_Brand'].values[0]
#    
#    target_date=data['Date'].values[0]
#    target_date_lag_two=dt.strptime(target_date,'%Y-%m-%d')-timedelta(days=2)
#    target_date_lag_two=target_date_lag_two.strftime("%Y-%m-%d")
#    
#    target_date_lag_period=dt.strptime(target_date,'%Y-%m-%d')-timedelta(days=period)
#    target_date_lag_period=target_date_lag_period.strftime("%Y-%m-%d")
#
#    data_all=pd.merge(data,trackwork_swim,how='left',left_on=['Horse_Brand'],right_on=['HorseName_Brand'])
#    var_name='swim_'+str(period)
#    
#    data_all_2=data_all.loc[(data_all['Date_y']<=target_date_lag_two)&(data_all['Date_y']>=target_date_lag_period)]
#    data_all_2=pd.DataFrame(data_all_2.groupby('Horse_Brand').size())
#    data_all_2['Horse_Brand']=data_all_2.index
#    data_all_2=data_all_2.reset_index(drop=True)
#    data_all_2=data_all_2.rename(columns={0:var_name})
#    data=pd.merge(data,data_all_2,how='left',on=['Horse_Brand'])
#    data=data.fillna(0)  #nan mean no swimming 
#
#    output=pd.DataFrame({var_name:data[var_name].values.tolist()},index=data_index)
#    #output=pd.DataFrame({'swim_14':[count_14],'swim_22':[count_22],'swim_30':[count_30]},index=data_index)
#    return output

#para=14
#def swim_parallel(para):
#    out=main_data[['Date','Horse_Brand']].groupby(['Date'],group_keys=False).apply(lambda x:trackwork_swim_extract(x,period=para))
#    var_name='swim_'+str(para)
#    main_data[var_name]=out


#main_data_check=main_data[['Date','RaceNo','Horse_Brand','swim_14','swim_22','swim_30']].copy()
#
#from concurrent.futures import ThreadPoolExecutor
#import time
#a=dt.now()
#with ThreadPoolExecutor(max_workers=None) as pool:
#    pool.map(swim_parallel, [14,22,30])
#
#b=dt.now()
#b-a




#from dask import dataframe as dd
#from dask.multiprocessing import get 
#from dask.distributed import Client
#
##client = Client(n_workers=8, threads_per_worker=8)
#client = Client()
#
#ddf = dd.from_pandas(main_data[['Date','Horse_Brand']].copy(), npartitions=4)
#
#meta = [("swim_14", int)]
#g=ddf[['Date','Horse_Brand']].groupby(['Date'],group_keys=False).apply(lambda x:trackwork_swim_extract(x,period=14),meta=meta)
#
#
#a=dt.now()
#out=g.compute()
#b=dt.now()
#b-a

#create swim count
#main_data['swim_14']=main_data[['Date','Horse_Brand']].groupby(['Date'],group_keys=False).apply(lambda x:trackwork_swim_extract(x,period=14))
#main_data['swim_22']=main_data[['Date','Horse_Brand']].groupby(['Date'],group_keys=False).apply(lambda x:trackwork_swim_extract(x,period=22))
#main_data['swim_30']=main_data[['Date','Horse_Brand']].groupby(['Date'],group_keys=False).apply(lambda x:trackwork_swim_extract(x,period=30))








#read sick history
connection=create_engine("mysql+pymysql://root:pp222#@localhost:3306/horseracing")


query='SELECT * from sickhistory'
sickhistory= sql.read_sql(query,connection)

sickhistory=sickhistory.loc[~pd.isnull(sickhistory['Date']),:]
sickhistory=sickhistory.reset_index(drop=True)

#convert date to %Y-%m-%d format

sickhistory['Date']=sickhistory['Date'].apply(lambda x:dt.strptime(x,"%d/%m/%Y").strftime("%Y-%m-%d"))
sickhistory['all_info']=sickhistory['Date']+"|"+sickhistory['Details']+"|"+sickhistory['Passed Date']


#find sick record between today and last race date
main_data_sick=pd.merge(main_data[['Date','RaceNo','HorseNo','Horse_Brand','last_race_date']].copy(),
                        sickhistory[['Date','HorseName_Brand','all_info']].copy(),
                        how='left',left_on=['Horse_Brand'],right_on=['HorseName_Brand'],suffixes=('','_sick'))

#some horse may not have sick records
main_data_sick=main_data_sick.loc[~pd.isnull(main_data_sick['Date_sick']),:]



#use less than, because at race day, doctor may check issue
key_old_horse=(main_data_sick['Date_sick']<main_data_sick['Date'])&(main_data_sick['Date_sick']>=main_data_sick['last_race_date'])
key_new_horse=(main_data_sick['Date']==main_data_sick['last_race_date'])&(main_data_sick['Date_sick']<main_data_sick['Date'])


main_data_sick2=main_data_sick.loc[key_old_horse|key_new_horse]
#main_data_sick2=main_data_sick2.rename(columns={'all_info':'sickhistory_within_lastraceday'})



#find all sick records before today

data=main_data_sick2[3:6]

def aggregate_sick_1(data):
    temp=data['all_info'].values.tolist()
    temp='@@'.join(temp)

    return temp


main_data_sick3=main_data_sick2.groupby(['Date','RaceNo','HorseNo']).apply(lambda x:aggregate_sick_1(x)).reset_index()
main_data_sick3=main_data_sick3.rename(columns={0:'sickhistory_within_lastraceday'})

main_data=pd.merge(main_data,main_data_sick3[['Date','RaceNo','HorseNo','sickhistory_within_lastraceday']].copy(),how='left',on=['Date','RaceNo','HorseNo'])




#find all sick records before today
key=main_data_sick['Date_sick']<main_data_sick['Date']


main_data_sick3=main_data_sick.loc[key]
main_data_sick3=main_data_sick3.reset_index(drop=True)

data=main_data_sick3[3:6]

def aggregate_sick(data):
    temp=data['all_info'].values.tolist()
    temp='@@'.join(temp)

    return temp


main_data_sick4=main_data_sick3.groupby(['Date','RaceNo','HorseNo']).apply(lambda x:aggregate_sick(x)).reset_index()
main_data_sick4=main_data_sick4.rename(columns={0:'sickhistory_before_thisraceday'})

main_data=pd.merge(main_data,main_data_sick4[['Date','RaceNo','HorseNo','sickhistory_before_thisraceday']].copy(),how='left',on=['Date','RaceNo','HorseNo'])


main_data_check=main_data.loc[main_data['Horse_Brand']=="limitless_A115",['Date','RaceNo','HorseNo','Horse_Brand','sickhistory_within_lastraceday','sickhistory_before_thisraceday']]

main_data_check=main_data.head(5000)

























































main_data_check=main_data.head(30000)






#output to sql db
connection=create_engine("mysql+pymysql://root:pp222#@localhost:3306/horseracing")
main_data.to_sql('main_data',connection,if_exists='replace',chunksize=1000,index=False)


#save as h5
store = pd.HDFStore("./h5_data/main_data.hdf5", "w", complib=str("zlib"), complevel=5)
store.put("main_data_dataframe", main_data, data_columns=main_data.columns)
store.close()




#read data
connection=create_engine("mysql+pymysql://root:pp222#@localhost:3306/horseracing")
query='select * from main_data'
main_data= sql.read_sql(query,connection)

main_data_check=main_data.head(5000)



import sys
SPARK_HOME='/usr/software/spark/spark-3.2.1-bin-hadoop3.2'
sys.path.append(SPARK_HOME+'/python')
sys.path.append(SPARK_HOME+'/python/pyspark')
sys.path.append(SPARK_HOME+'/python/lib/py4j-0.10.9.3-src.zip')
import pyspark
#save as parquet
main_data.to_parquet("./parquet_data/main_data.parquet")













topic_track pnl_report_v2




from __future__ import division
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
# os.chdir("%s/.." % os.getcwd())
import math

import itertools
import random
from sqlalchemy import create_engine
import pymysql
import pandas.io.sql as sql
from pandas import HDFStore,DataFrame

connection_odd=create_engine("mysql+pymysql://root:pp222#@localhost:3306/Odds_store")  


fn=os.path.join('/home/simon/Dropbox/notebooks/horse/factor','factorDB.hdf5')
store = pd.HDFStore(fn)
factorDB= store.select('factorDB_dataframe')
store.close()


tier2_factor_only=True











#all_bet_type=['Win','Pla','Qin','Qpl','Tri','Tie']#,'Qur']
#all_bet_type=['Win']
#all_bet_type=['Tri','Tie']
#all_bet_type=['Win','Pla']
#all_bet_type=['Tri']
#all_bet_type=['Win','Pla','Qin','Qpl','Tri']
all_bet_type=['Win','Pla','Qin','Qpl','Tri']
all_bet_type=['Qin','Qpl','Tri']#,'Tie']
all_bet_type=['Win','Qin','Qpl','Tri']
all_bet_type=['Win','Qin','Qpl']#,'Tri']
#all_bet_type=['Win','Pla','Qin','Qpl']
#all_bet_type=['Win','Pla']
#all_bet_type=['Tie']
#all_bet_type=['Win','Pla','Qin']
#all_bet_type=['Win','Pla']
#all_bet_type=['Qin','Qpl','Tri']
#all_bet_type=['Win','Pla']
#all_bet_type=['Win']



#factor_use=['pick1','pick2','pick3']
#factor_use=['pick1']
#factor_use=['abnormal_factor1']#,'abnormal_factor5']
factor_use=['pick3']
factor_use=['pick1_exotic']
#factor_use=[]
factor_use=['pick1_exotic','pick2_exotic','pick3_exotic','abnormal_factor1']  #best exotic
#factor_use=['pick3']
#factor_use=['pick2_exotic','pick2_lbdiff']
#factor_use=['pick1_lbdiff']
#factor_use=['pick1_exotic']
#factor_use=['pick1','pick2','pick3','pick4']
#factor_use=['pick1_exotic','pick3_exotic']
#factor_use=['pick3']
#factor_use=['abnormal_factor1']
#factor_use=['pick1','pick2','pick3','pick4']
#factor_use=['pick3']
factor_use=['pick1','pick2','pick3','pick4','pick3_more']
#factor_use=['pick2_more']




#factor_use_substraction=['no_of_races_joined_less_than']
#factor_use_substraction=['sick_indicator']
factor_use_substraction=[]












if len(factor_use_substraction)>=1:
    factorDB['factor_deduction']=factorDB[factor_use_substraction[0]].copy()
    if len(factor_use_substraction)>=2:
        for p in factor_use_substraction[1:]:
            factorDB['factor_deduction']=factorDB['factor_deduction']+factorDB[p]
            
    factorDB_check=factorDB.head(10000)
    factorDB.loc[factorDB['factor_deduction']>0,'factor_deduction']=-1



#need to mask factor one by one, if have signal in pick1_exotic, for other factor if hv signal, not to add to this race.
if len(factor_use)>0:
#    factorDB['tier2_factor']=np.sum(factorDB[factor_use].values,axis=1)
#    factorDB['tier2_factor']=(factorDB['tier2_factor']>0)*1
    
    tier2=factorDB[['Date','RaceNo','HorseNo']].copy()
    tier2['tier2_factor']=factorDB[factor_use[0]].copy()
    
    temp=factorDB[['Date','RaceNo']+[factor_use[0]]].groupby(['Date','RaceNo'])[factor_use[0]].sum()
    temp=temp.reset_index(drop=False)
    temp.loc[temp[factor_use[0]]>0,'mased']=1
    temp['mased']=temp['mased'].fillna(0)
    del temp[factor_use[0]]
    tier2=pd.merge(tier2,temp,how='left',on=['Date','RaceNo'])
    
    
    
    #x='pick2_exotic'
    for x in factor_use[1:]:
        temp1=factorDB[['Date','RaceNo']+[x]].groupby(['Date','RaceNo'])[x].sum()
        temp1=temp1.reset_index(drop=False)
        temp1.loc[temp1[x]>0,'mased_temp']=1
        temp1['mased_temp']=temp1['mased_temp'].fillna(0)
        del temp1[x]
        tier2=pd.merge(tier2,temp1,how='left',on=['Date','RaceNo'])
        tier2=pd.merge(tier2,factorDB[['Date','RaceNo','HorseNo']+[x]],how='left',on=['Date','RaceNo','HorseNo'])
        tier2.loc[tier2['mased']==0,'tier2_factor']=tier2['tier2_factor']+tier2[x]
        
        tier2['mased']=tier2['mased']+tier2['mased_temp']
        del tier2['mased_temp']
        del tier2[x]
    
    
    factorDB=pd.merge(factorDB,tier2[['Date','RaceNo','HorseNo','tier2_factor']].copy(),how='left',on=['Date','RaceNo','HorseNo'])

    
    

else:
    factorDB['tier2_factor']=0

factorDB_check=factorDB[['Date','RaceNo','HorseNo','year_cohord']+factor_use+['tier2_factor','leg_factor']].copy()
factorDB_check=factorDB_check.loc[factorDB_check['year_cohord']==2021,:].copy()


if len(factor_use_substraction)>=1:
    factorDB['tier2_factor']=factorDB['tier2_factor']+factorDB['factor_deduction']
    factorDB['tier2_factor']=(factorDB['tier2_factor']>0)*1

    factorDB['leg_factor']=factorDB['leg_factor']+factorDB['factor_deduction']
    factorDB['leg_factor']=(factorDB['leg_factor']>0)*1

    factorDB_check=factorDB[['Date','RaceNo','HorseNo','year_cohord']+factor_use+['factor_deduction','tier2_factor','leg_factor']].copy()
    factorDB_check=factorDB_check.loc[factorDB_check['year_cohord']==2019,:].copy()
























def nPr(n,r):
    f = math.factorial
    return f(n) / f(n-r)


def mix_prob2(model_prob, pub_prob, coef_trimodel=0.5, coef_ppub=0.6):      
    if (bet_type=='Pla')|(bet_type=='Qpl'):
        output=model_prob*coef_trimodel+pub_prob*(1.0-coef_trimodel)
        
    if (bet_type=='Win')|(bet_type=='Qin'):        
        a = (model_prob**coef_trimodel)*(pub_prob**coef_ppub)
        output=a/sum(a)
    return output


#df=trif.loc[trif['racekey']=='2016-647',:];min_prob_scale=2.0;min_adv=coef_min_adv
def pick_bet_minprob(df,min_prob_scale=2.0,max_prob=0.015,min_adv=1.1):
    number_of_horses=df.sts.values[0]
    total_combination=nPr(number_of_horses,3)
    natural_guess_withscale=min_prob_scale*(1.0/total_combination)
    v=(df['advantage']>=min_adv)&(df['trimix']>=natural_guess_withscale)&(df['trimix']<=max_prob)
    #v=True
    #v=(df['gu_amount_original']>=0)&(df['trimix']>=natural_guess_withscale)
    df['bet'] = v 
    return df['bet']
     
#p=trif.loc[(trif['Date']=='2015-10-10')&(trif['RaceNo']==6),'trimix']
#o=trif.loc[(trif['Date']=='2015-10-10')&(trif['RaceNo']==6),capture_public_odd]
#o=trif.loc[(trif['Date']=='2018-11-18')&(trif['RaceNo']==6),capture_public_odd]
#rho = 0.9;min_adv=1.1;payout_ratio=0.825

def gu_kelly(p, o, rho = 0.9, min_adv = 1.1, payout_ratio=0.75):
    numerator = rho*p*o + (1-rho)*payout_ratio - min_adv
    numerator[numerator<0]=0
    output=numerator/(o-1)
    output[o<1.01]=0
    #output=1
    return output







#df=trif.copy()
#df.columns.values
def summary(df, bet,df_grouped_pnl):
    output_col=['Strategy','TotalNo.ofRace','TotalNo.ofRaceBetted','AverageNo.ofBetsPerRace','Max.No.ofBetsPerRace','AveragBetAmountPerRace',
                'MaxTotalBetAmountPerRace','TotalInvestment','AverageBetAmountPerBet','Min.BetAmountPerBet','Max.BetAmountPerBet',
                'Max_bet_ratio', 'total_return', 'Profit', 'ROI','DrawDown','Max_CumPnL','Min_CumPnL', 'No.of.Bet', 'Hit', 'HitRate']

    df1 = pd.DataFrame(columns=output_col)
    betting_strategy='bet_minprobscale20'
    define_amount='gu_amount'
    for betting_strategy in bet:
        for define_amount in ['gu_amount']:
            df=trif.copy()
            strategy = betting_strategy+"_"+define_amount
            
            
#            #read training win probability
#            filename=os.path.join(input_path,str(runno_list[year_list.index(year)]),'temp','WinPlaProb_training.csv')
#            raw_win_training = pd.read_csv(filename)            
#            raw_win_training.columns.values
#            raw_win_training_describe=raw_win_training.describe()
            
            use_field='win_prob_original'

            prob_cutoff=mean_prob#0.4988
            model_field='win_prob_original'
            
            #use new strategy
            filename=os.path.join(input_path,str(runno_list[year_list.index(year)]),'temp','Win_HK_data_'+str(year)+'.csv')
            raw_win_0 = pd.read_csv(filename)
            raw_win_0.columns.values
            
            raw_win_0=pd.merge(raw_win_0,factorDB[['Date','RaceNo','HorseNo','tier2_factor']].copy(),how='left',on=['Date','RaceNo','HorseNo'])
            
            if tier2_factor_only==True:
                raw_win_0[model_field]=0
                
            raw_win_0[model_field]=raw_win_0[model_field]+raw_win_0['tier2_factor']
            

            
            

            query='SELECT * from '+'win_odds_25s_nice_format'
            win_nice=sql.read_sql(query,connection_odd)
            win_nice=win_nice.reset_index(drop=True)
            win_nice['odd_300s_change']=(win_nice['win_odd_25s']-win_nice['win_odd_before90s'])/win_nice['win_odd_before90s']
            win_nice['RaceNo']=win_nice['RaceNo'].astype(str)
            win_nice['Date_RaceNo']=win_nice['Date']+'_'+win_nice['RaceNo']
            win_nice.columns.values

            query='SELECT * from '+'pla_odds_25s_nice_format'
            pla_nice=sql.read_sql(query,connection_odd)
            pla_nice=pla_nice.reset_index(drop=True) 
            pla_nice['odd_300s_change']=(pla_nice['pla_odd_25s']-pla_nice['pla_odd_before90s'])/pla_nice['pla_odd_before90s']
            pla_nice['RaceNo']=pla_nice['RaceNo'].astype(str)
            pla_nice['Date_RaceNo']=pla_nice['Date']+'_'+pla_nice['RaceNo']
            pla_nice.columns.values


#            odd_min_win=3             #a bit better
#            odd_min_pla=2             #a bit better
#            odd_min_qin=5
#            odd_min_qpl=2

            odd_min_win=0       
            odd_min_pla=0  
            odd_min_qin=0
            odd_min_qpl=0


            odd_cap_win=300
            odd_cap_pla=300    
            odd_cap_qin=300
            odd_cap_qpl=300
            odd_cap_tri=300
            odd_cap_tie=300
            odd_cap_qur=300
            
            win_model_use=9
            pla_model_use=9
            qin_model_use=9
            qpl_model_use=9
            tri_model_use=9
            tie_model_use=9
            qur_model_use=9

#            win_model_use=3
#            pla_model_use=3
#            qin_model_use=3
#            qpl_model_use=3
#            tri_model_use=3
#            tie_model_use=3
#            qur_model_use=2


#            win_model_use=20
#            pla_model_use=20
#            qin_model_use=20
#            qpl_model_use=20
#            tri_model_use=20
#            tie_model_use=20
#            qur_model_use=20


            bet_amount_set=10

            bet_ratio={'Win':5,'Pla':1,'Qin':2,'Qpl':3,'Tri':1,'Tie':1,'Qur':1}
            bet_ratio={'Win':1.5,'Pla':1,'Qin':2,'Qpl':2,'Tri':1,'Tie':1,'Qur':1}


            if ii=='Win':
                
                raw_win=raw_win_0.copy()
                raw_win=raw_win.sort_values(by=['Date','RaceNo',model_field],ascending=[True,True,False])
                raw_win_select2=raw_win.groupby(['Date','RaceNo']).head(win_model_use)
                raw_win_select2['indicator_model']=1

                raw_win=pd.merge(raw_win,raw_win_select2[['Date_RaceNo','HorseNo','indicator_model']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_model']=raw_win['indicator_model'].fillna(999)    


                win_nice=win_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                win_nice_select2=win_nice.groupby(['Date','RaceNo']).head(1)
                win_nice_select2=win_nice_select2.loc[win_nice_select2['odd_300s_change']<0,:].copy()
                win_nice_select2['indicator_odd_300s_change_win']=1  
                
                pla_nice=pla_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                pla_nice_select2=pla_nice.groupby(['Date','RaceNo']).head(3)
                pla_nice_select2=pla_nice_select2.loc[pla_nice_select2['odd_300s_change']<0,:].copy()
                pla_nice_select2['indicator_odd_300s_change_pla']=1
            
                raw_win=pd.merge(raw_win,win_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_win']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_win']=raw_win['indicator_odd_300s_change_win'].fillna(999)
                
                raw_win=pd.merge(raw_win,pla_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_pla']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_pla']=raw_win['indicator_odd_300s_change_pla'].fillna(999) 
                    
#                bet_criteria=(raw_win['win_odd_25s']<20) & \
#                             (raw_win['indicator_model']==1) & (raw_win['indicator_odd_300s_change_win']==1) & (raw_win['indicator_odd_300s_change_pla']==1)

#                bet_criteria=(raw_win['win_odd_25s']<20) & \
#                             (raw_win['indicator_model']==1)&(raw_win[use_field]>prob_cutoff)
                

                bet_criteria=(raw_win['indicator_model']==1)&(raw_win[use_field]>prob_cutoff)&(raw_win['win_odd_25s']<odd_cap_win)

    
                raw_win=raw_win[bet_criteria].copy()
                
                
                #find largest n prob horse
                raw_win_temp=raw_win.loc[raw_win['Date_RaceNo']=='2016-07-10_1',:].copy()
                
                raw_win_temp.columns.values
                def find_n_horse_win(raw_win_temp):
                    effective_hr=raw_win_temp.shape[0]
                    #use_horse=math.ceil(effective_hr/2) 
                    #use_horse=math.ceil(effective_hr/3) 
                    use_horse=20
                    output_horse=pd.DataFrame(raw_win_temp['HorseNo'].values[0:use_horse])
                    output_horse.columns=['HorseNo']
                    output_horse['Date']=raw_win_temp['Date'].values[0]
                    output_horse['RaceNo']=raw_win_temp['RaceNo'].values[0]
                    output_horse['Date_RaceNo']=raw_win_temp['Date_RaceNo'].values[0]
                    
                    output_horse_win=output_horse.copy()
                    return output_horse_win 
                
                win_horse_use=raw_win.groupby(['Date','RaceNo']).apply(lambda x: find_n_horse_win(x.reset_index(drop=True)))
                win_horse_use=win_horse_use.reset_index(drop=True)
                win_horse_use=win_horse_use.sort_values(by=['Date','RaceNo','HorseNo'],ascending=[True,True,True])                
                
                
                
                #find bets combination
                win_horse_use['one']=1
            
                df=pd.merge(df,win_horse_use[['Date_RaceNo','HorseNo','one']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                df_check=df.head(1000)
                
                #remove bet with 25s odd <
                bet_key=(df['one']==1)&(df['win_odd_25s']>odd_min_win)
                #bet_key=(df['one']==1)

                del df[define_amount]
                df.loc[bet_key,define_amount]=bet_amount_set
                df[define_amount]=df[define_amount].fillna(0)
                
    
                del df[betting_strategy]
                df.loc[bet_key,betting_strategy]=True
                df[betting_strategy]=df[betting_strategy].fillna(False)
                
                
                #check win odd distribution
                df_check=df[['Date','RaceNo','win_odd_25s','tri']].copy()
                df_check=df_check.loc[df_check['tri']==1,:].copy()
                df_check.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+"_"+time_now+"_"+'odds_distribution.csv'), index=False) 




            if ii=='Pla':

                raw_win=raw_win_0.copy()
                raw_win=raw_win.sort_values(by=['Date','RaceNo',model_field],ascending=[True,True,False])
                raw_win_select2=raw_win.groupby(['Date','RaceNo']).head(pla_model_use)
                raw_win_select2['indicator_model']=1

                raw_win=pd.merge(raw_win,raw_win_select2[['Date_RaceNo','HorseNo','indicator_model']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_model']=raw_win['indicator_model'].fillna(999)
                

                win_nice=win_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                win_nice_select2=win_nice.groupby(['Date','RaceNo']).head(1)
                win_nice_select2=win_nice_select2.loc[win_nice_select2['odd_300s_change']<0,:].copy()
                win_nice_select2['indicator_odd_300s_change_win']=1  
                
                pla_nice=pla_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                pla_nice_select2=pla_nice.groupby(['Date','RaceNo']).head(3)
                pla_nice_select2=pla_nice_select2.loc[pla_nice_select2['odd_300s_change']<0,:].copy()
                pla_nice_select2['indicator_odd_300s_change_pla']=1
    

                
                raw_win=pd.merge(raw_win,win_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_win']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_win']=raw_win['indicator_odd_300s_change_win'].fillna(999)
                
                raw_win=pd.merge(raw_win,pla_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_pla']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_pla']=raw_win['indicator_odd_300s_change_pla'].fillna(999)  

    
                bet_criteria=(raw_win['indicator_model']==1)&(raw_win[use_field]>prob_cutoff)&(raw_win['win_odd_25s']<odd_cap_pla)
    
                raw_win=raw_win[bet_criteria].copy()
                
                
                #find largest n prob horse
                raw_win_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-10-09_3',:].copy()
                raw_win_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-09-11_5',:].copy()
                raw_win_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-11-03_6',:].copy()
                
                raw_win_temp.columns.values
                def find_n_horse_win(raw_win_temp):
                    effective_hr=raw_win_temp.shape[0]
                    #use_horse=math.ceil(effective_hr/2) 
                    #use_horse=math.ceil(effective_hr/3) 
                    use_horse=20
                    output_horse=pd.DataFrame(raw_win_temp['HorseNo'].values[0:use_horse])
                    output_horse.columns=['HorseNo']
                    output_horse['Date']=raw_win_temp['Date'].values[0]
                    output_horse['RaceNo']=raw_win_temp['RaceNo'].values[0]
                    output_horse['Date_RaceNo']=raw_win_temp['Date_RaceNo'].values[0]
                    
                    output_horse_win=output_horse.copy()
                    return output_horse_win 
                
                win_horse_use=raw_win.groupby(['Date','RaceNo']).apply(lambda x: find_n_horse_win(x.reset_index(drop=True)))
                win_horse_use=win_horse_use.reset_index(drop=True)
                win_horse_use=win_horse_use.sort_values(by=['Date','RaceNo','HorseNo'],ascending=[True,True,True])                
                
                
                
                #find bets combination
                win_horse_use['one']=1
            
                df=pd.merge(df,win_horse_use[['Date_RaceNo','HorseNo','one']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                df_check=df.head(1000)
                
                #remove bet with 25s odd <
                bet_key=(df['one']==1)&(df['pla_odd_25s']>odd_min_pla)
                #bet_key=(df['one']==1)               
                del df[define_amount]
                df.loc[bet_key,define_amount]=bet_amount_set
                df[define_amount]=df[define_amount].fillna(0)
                
    
                del df[betting_strategy]
                df.loc[bet_key,betting_strategy]=True
                df[betting_strategy]=df[betting_strategy].fillna(False)
                
                #take a look on win 25s odds
                raw_win=raw_win_0.copy()
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','HorseNo'],right_on=['Date','RaceNo','HorseNo'])


                #check win odd distribution
                df_check=df[['Date','RaceNo','win_odd_25s','pla_odd_25s','tri']].copy()
                df_check=df_check.loc[df_check['tri']==1,:].copy()
                df_check.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+"_"+time_now+"_"+'odds_distribution.csv'), index=False) 


            
            
            if ii=='Qin':    
                raw_win=raw_win_0.copy()
                raw_win=raw_win.sort_values(by=['Date','RaceNo',model_field],ascending=[True,True,False])
                raw_win_select2=raw_win.groupby(['Date','RaceNo']).head(qin_model_use)
                raw_win_select2['indicator_model']=1

                raw_win=pd.merge(raw_win,raw_win_select2[['Date_RaceNo','HorseNo','indicator_model']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_model']=raw_win['indicator_model'].fillna(999)

                leg_select=raw_win_0.copy()
                leg_select2=pd.merge(leg_select,factorDB[['Date_RaceNo','HorseNo','leg_factor']].copy(),how='left',on=['Date_RaceNo','HorseNo'])  
                leg_select2=leg_select2.sort_values(by=['Date','RaceNo','leg_factor'],ascending=[True,True,False])
                leg_select2=leg_select2.groupby(['Date','RaceNo']).head(3)
                leg_select2.loc[leg_select2['leg_factor']==1,'indicator_leg']=1



        
                win_nice=win_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                win_nice_select2=win_nice.groupby(['Date','RaceNo']).head(2)
                win_nice_select2=win_nice_select2.loc[win_nice_select2['odd_300s_change']<0,:].copy()
                win_nice_select2['indicator_odd_300s_change_win']=1
                
                pla_nice=pla_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                pla_nice_select2=pla_nice.groupby(['Date','RaceNo']).head(1)
                pla_nice_select2=pla_nice_select2.loc[pla_nice_select2['odd_300s_change']<0,:].copy()
                pla_nice_select2['indicator_odd_300s_change_pla']=1
    
                
                raw_win=pd.merge(raw_win,win_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_win']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_win']=raw_win['indicator_odd_300s_change_win'].fillna(999)
                
                raw_win=pd.merge(raw_win,pla_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_pla']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_pla']=raw_win['indicator_odd_300s_change_pla'].fillna(999) 

                raw_win=pd.merge(raw_win,leg_select2[['Date_RaceNo','HorseNo','indicator_leg']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_leg']=raw_win['indicator_leg'].fillna(999) 


#                core_criteria=(raw_win['win_odd_25s']<30) & \
#                              ((raw_win['indicator_model']==1) | (raw_win['indicator_odd_300s_change_win']==1))

#                core_criteria=(raw_win['win_odd_25s']<30) & \
#                              (raw_win['indicator_model']==1)&(raw_win[use_field]>prob_cutoff)

                core_criteria=(raw_win['indicator_model']==1)&(raw_win[use_field]>prob_cutoff)


        
                sum(core_criteria)
    
                raw_win_core=raw_win[core_criteria].copy()
                raw_win_core['core']=1
                raw_win=pd.merge(raw_win,raw_win_core[['Date_RaceNo','HorseNo','core']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['core']=raw_win['core'].fillna(999)

                #raw_win=raw_win.sort_values(by=['Date','RaceNo','core','indicator_odd_300s_change_win','win_odd_25s'],ascending=[True,True,True,True,True])

                
                #raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_odd_300s_change_win']==1))&(raw_win['win_odd_25s']<odd_cap_qin),:].copy()
                raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_leg']==1))&(raw_win['win_odd_25s']<odd_cap_tri),:].copy()
                raw_win=raw_win.reset_index(drop=True)
                
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-09-01_1',:].copy()
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-10-09_3',:].copy()
                
                def find_n_horse_qin(win_horse_use_temp):
                    effective_hr=win_horse_use_temp.sts.values[0]
                    use_horse=100 if effective_hr>=12 else 100
                    all_comb=[]
                    core_use=win_horse_use_temp.loc[win_horse_use_temp['core']==1,'HorseNo'].values.tolist()
                    leg_use=win_horse_use_temp.loc[win_horse_use_temp['core']==999,'HorseNo'].values.tolist()
                    leg_use=leg_use[0:use_horse-len(core_use)]
#                    if (len(core_use)==2) & (len(leg_use)==0) : 
#                        all_comb=[str(core_use[0])+','+str(core_use[1])]
                        
                    if (len(core_use)==1) & (len(leg_use)>=1) : 
                        all_comb=[str(core_use[0])+','+str(p) for p in leg_use[0:use_horse-1]]

                    if (len(core_use)>=2) & (len(leg_use)>=0) : 
                        t1=list(itertools.combinations(core_use, 2))
                        all_comb=[str(pp[0])+','+str(pp[1]) for pp in t1]
                        
                        if len(leg_use)>0:
                            leg_use2=leg_use[0:use_horse-len(core_use)]
                            for m in core_use:
                                all_comb=all_comb+[str(m)+','+str(q) for q in leg_use2]
                    
                    all_comb=[str(int(g.split(',')[1]))+','+str(int(g.split(',')[0])) if int(g.split(',')[0])>int(g.split(',')[1]) else g for g in all_comb]
                    
                    output_horse=pd.DataFrame([])                   
                    output_horse['horse_comb']=all_comb
                    
                    output_horse['Date']=win_horse_use_temp['Date'].values[0]
                    output_horse['RaceNo']=win_horse_use_temp['RaceNo'].values[0]
                    output_horse['Date_RaceNo']=win_horse_use_temp['Date_RaceNo'].values[0]
                    
                    return output_horse            


            
                #find bets combination
                qin_horse_use=raw_win.groupby(['Date','RaceNo']).apply(lambda x: find_n_horse_qin(x.reset_index(drop=True)))
                qin_horse_use=qin_horse_use.reset_index(drop=True)
                qin_horse_use['one']=1
            
                df=pd.merge(df,qin_horse_use[['Date_RaceNo','horse_comb','one']].copy(),how='left',on=['Date_RaceNo','horse_comb'])
                df_check=df.head(1000)
                
                #remove bet with 25s odd <10
                bet_key=(df['one']==1)&(df['qin_odd_25s']>odd_min_qin)
                #bet_key=(df['one']==1)
                
                del df[define_amount]
                df.loc[bet_key,define_amount]=bet_amount_set
                df[define_amount]=df[define_amount].fillna(0)
                
    
                del df[betting_strategy]
                df.loc[bet_key,betting_strategy]=True
                df[betting_strategy]=df[betting_strategy].fillna(False)
                
                #take a look on hr1 hr2 odds at 25s
                raw_win=raw_win_0.copy()
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_1'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_1_25s_odd'})
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_2'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_2_25s_odd'})
                
                #check qin odd distribution
                df_check=df[['Date','RaceNo','horse_no_1_25s_odd','horse_no_2_25s_odd','qin_odd_25s','tri']].copy()
                df_check=df_check.loc[df_check['tri']==1,:].copy()
                
                df_check.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+"_"+time_now+"_"+'odds_distribution.csv'), index=False)



            #ii='Qpl'
            if ii=='Qpl':    
                raw_win=raw_win_0.copy()
                raw_win=raw_win.sort_values(by=['Date','RaceNo',model_field],ascending=[True,True,False])
                raw_win_select2=raw_win.groupby(['Date','RaceNo']).head(qpl_model_use)
                raw_win_select2['indicator_model']=1

                raw_win=pd.merge(raw_win,raw_win_select2[['Date_RaceNo','HorseNo','indicator_model']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_model']=raw_win['indicator_model'].fillna(999)

                leg_select=raw_win_0.copy()
                leg_select2=pd.merge(leg_select,factorDB[['Date_RaceNo','HorseNo','leg_factor']].copy(),how='left',on=['Date_RaceNo','HorseNo'])  
                leg_select2=leg_select2.sort_values(by=['Date','RaceNo','leg_factor'],ascending=[True,True,False])
                leg_select2=leg_select2.groupby(['Date','RaceNo']).head(3)
                leg_select2.loc[leg_select2['leg_factor']==1,'indicator_leg']=1
                


                win_nice=win_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                win_nice_select2=win_nice.groupby(['Date','RaceNo']).head(1)
                win_nice_select2=win_nice_select2.loc[win_nice_select2['odd_300s_change']<0,:].copy()
                win_nice_select2['indicator_odd_300s_change_win']=1  
                
                pla_nice=pla_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                pla_nice_select2=pla_nice.groupby(['Date','RaceNo']).head(2)
                pla_nice_select2=pla_nice_select2.loc[pla_nice_select2['odd_300s_change']<0,:].copy()
                pla_nice_select2['indicator_odd_300s_change_pla']=1
    

                
                raw_win=pd.merge(raw_win,win_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_win']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_win']=raw_win['indicator_odd_300s_change_win'].fillna(999)
                
                raw_win=pd.merge(raw_win,pla_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_pla']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_pla']=raw_win['indicator_odd_300s_change_pla'].fillna(999) 

                raw_win=pd.merge(raw_win,leg_select2[['Date_RaceNo','HorseNo','indicator_leg']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_leg']=raw_win['indicator_leg'].fillna(999) 

                
                
#                core_criteria=(raw_win['win_odd_25s']<30) & \
#                              ((raw_win['indicator_model']==1) | ((raw_win['indicator_odd_300s_change_win']==1) & (raw_win['indicator_odd_300s_change_pla']==1)))

#                core_criteria=(raw_win['win_odd_25s']<30) & \
#                              (raw_win['indicator_model']==1) & (raw_win[use_field]>prob_cutoff)

                core_criteria=(raw_win['indicator_model']==1)&(raw_win[use_field]>prob_cutoff)

        
                sum(core_criteria)
    
                raw_win_core=raw_win[core_criteria].copy()
                raw_win_core['core']=1
                raw_win=pd.merge(raw_win,raw_win_core[['Date_RaceNo','HorseNo','core']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['core']=raw_win['core'].fillna(999)

                #raw_win=raw_win.sort_values(by=['Date','RaceNo','core','win_odd_25s'],ascending=[True,True,True,True])
                #raw_win=raw_win.sort_values(by=['Date','RaceNo','core','indicator_odd_300s_change_win','win_odd_25s'],ascending=[True,True,True,True,True])


                #raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_odd_300s_change_win']==1))&(raw_win['win_odd_25s']<odd_cap_qpl),:].copy()
                raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_leg']==1))&(raw_win['win_odd_25s']<odd_cap_tri),:].copy()
                raw_win=raw_win.reset_index(drop=True)
                
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-03-18_8',:].copy()
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-10-09_3',:].copy()
                
                def find_n_horse_qpl(win_horse_use_temp):
                    effective_hr=win_horse_use_temp.sts.values[0]
                    use_horse=100 if effective_hr>=12 else 100
                    all_comb=[]
                    core_use=win_horse_use_temp.loc[win_horse_use_temp['core']==1,'HorseNo'].values.tolist()
                    leg_use=win_horse_use_temp.loc[win_horse_use_temp['core']==999,'HorseNo'].values.tolist()
                    if (len(core_use)==1) & (len(leg_use)>=1) : 
                        all_comb=[str(core_use[0])+','+str(p) for p in leg_use[0:use_horse-1]]

                    if (len(core_use)>=2) & (len(leg_use)>=0) : 
                        t1=list(itertools.combinations(core_use, 2))
                        all_comb=[str(pp[0])+','+str(pp[1]) for pp in t1]
                        
                        leg_use2=leg_use[0:use_horse-len(core_use)]
                        for m in core_use:
                            all_comb=all_comb+[str(m)+','+str(q) for q in leg_use2]
                    
                    all_comb=[str(int(g.split(',')[1]))+','+str(int(g.split(',')[0])) if int(g.split(',')[0])>int(g.split(',')[1]) else g for g in all_comb]
                    
                    output_horse=pd.DataFrame([])                   
                    output_horse['horse_comb']=all_comb
                    
                    output_horse['Date']=win_horse_use_temp['Date'].values[0]
                    output_horse['RaceNo']=win_horse_use_temp['RaceNo'].values[0]
                    output_horse['Date_RaceNo']=win_horse_use_temp['Date_RaceNo'].values[0]
                    
                    return output_horse            


            
                #find bets combination
                qpl_horse_use=raw_win.groupby(['Date','RaceNo']).apply(lambda x: find_n_horse_qpl(x.reset_index(drop=True)))
                qpl_horse_use=qpl_horse_use.reset_index(drop=True)
                qpl_horse_use['one']=1
            
                df=pd.merge(df,qpl_horse_use[['Date_RaceNo','horse_comb','one']].copy(),how='left',on=['Date_RaceNo','horse_comb'])
                df_check=df.head(1000)
                
                #remove bet with 25s odd <10
                bet_key=(df['one']==1)&(df['qpl_odd_25s']>odd_min_qpl)
                #bet_key=(df['one']==1)               
                
                del df[define_amount]
                df.loc[bet_key,define_amount]=bet_amount_set
                df[define_amount]=df[define_amount].fillna(0)
                
    
                del df[betting_strategy]
                df.loc[bet_key,betting_strategy]=True
                df[betting_strategy]=df[betting_strategy].fillna(False)
                
                #take a look on hr1 hr2 odds at 25s
                raw_win=raw_win_0.copy()
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_1'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_1_25s_odd'})
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_2'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_2_25s_odd'})
                
                #check qpl odd distribution
                df_check=df[['Date','RaceNo','horse_no_1_25s_odd','horse_no_2_25s_odd','qpl_odd_25s','tri']].copy()
                df_check=df_check.loc[df_check['tri']==1,:].copy()
                
                df_check.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+"_"+time_now+"_"+'odds_distribution.csv'), index=False)




            if ii=='Tri':    
                raw_win=raw_win_0.copy()
                raw_win=raw_win.sort_values(by=['Date','RaceNo',model_field],ascending=[True,True,False])
                raw_win_select2=raw_win.groupby(['Date','RaceNo']).head(tri_model_use)
                raw_win_select2['indicator_model']=1




                leg_select=raw_win_0.copy()
                leg_select2=pd.merge(leg_select,factorDB[['Date_RaceNo','HorseNo','leg_factor']].copy(),how='left',on=['Date_RaceNo','HorseNo'])  
                leg_select2=leg_select2.sort_values(by=['Date','RaceNo','leg_factor'],ascending=[True,True,False])
                leg_select2=leg_select2.groupby(['Date','RaceNo']).head(4)
                leg_select2.loc[leg_select2['leg_factor']==1,'indicator_leg']=1

                raw_win=pd.merge(raw_win,raw_win_select2[['Date_RaceNo','HorseNo','indicator_model']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_model']=raw_win['indicator_model'].fillna(999)


                win_nice=win_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                win_nice_select2=win_nice.groupby(['Date','RaceNo']).head(3)
                win_nice_select2=win_nice_select2.loc[win_nice_select2['odd_300s_change']<0,:].copy()
                win_nice_select2['indicator_odd_300s_change_win']=1  
                
                pla_nice=pla_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                pla_nice_select2=pla_nice.groupby(['Date','RaceNo']).head(3)
                pla_nice_select2=pla_nice_select2.loc[pla_nice_select2['odd_300s_change']<0,:].copy()
                pla_nice_select2['indicator_odd_300s_change_pla']=1


            
                raw_win=pd.merge(raw_win,win_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_win']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_win']=raw_win['indicator_odd_300s_change_win'].fillna(999)
                
                raw_win=pd.merge(raw_win,pla_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_pla']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_pla']=raw_win['indicator_odd_300s_change_pla'].fillna(999) 

                raw_win=pd.merge(raw_win,leg_select2[['Date_RaceNo','HorseNo','indicator_leg']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_leg']=raw_win['indicator_leg'].fillna(999) 


#                core_criteria=(raw_win['indicator_model']==1) | ((raw_win['indicator_odd_300s_change_win']==1) & (raw_win['indicator_odd_300s_change_pla']==1))

                core_criteria=(raw_win['indicator_model']==1) & (raw_win[use_field]>prob_cutoff)


        
                sum(core_criteria)
    
                raw_win_core=raw_win[core_criteria].copy()
                raw_win_core['core']=1
                raw_win=pd.merge(raw_win,raw_win_core[['Date_RaceNo','HorseNo','core']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['core']=raw_win['core'].fillna(999)

#                raw_win=raw_win.sort_values(by=['Date','RaceNo','core','indicator_odd_300s_change_pla','win_odd_25s'],ascending=[True,True,True,True,True])
#                raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_odd_300s_change_pla']==1))&(raw_win['win_odd_25s']<odd_cap_tri),:].copy()

                raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_leg']==1))&(raw_win['win_odd_25s']<odd_cap_tri),:].copy()

                
                raw_win=raw_win.reset_index(drop=True)
                
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-01-29_3',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-01-29_2',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-04-08_4',:].copy().reset_index(drop=True)
                
                def find_n_horse_tri(win_horse_use_temp):
                    effective_hr=win_horse_use_temp.sts.values[0]
                    use_horse=100 if effective_hr>=12 else 100
                    all_comb=[]
                    core_use=win_horse_use_temp.loc[win_horse_use_temp['core']==1,'HorseNo'].values.tolist()
                    leg_use=win_horse_use_temp.loc[win_horse_use_temp['core']==999,'HorseNo'].values.tolist()
                    
                    if (len(core_use)==1) & (len(leg_use)>=2) : 
                        t1=list(itertools.combinations(leg_use, 2))
                        all_comb=[str(core_use[0])+','+str(pp[0])+','+str(pp[1]) for pp in t1]

                    if (len(core_use)==2) & (len(leg_use)>=1) : 
                        t1=list(itertools.combinations(leg_use, 1))
                        all_comb=[str(core_use[0])+','+str(core_use[1])+','+str(pp[0]) for pp in t1]
                        
                    if (len(core_use)>=3) & (len(leg_use)>=0) : 
                        t1=list(itertools.combinations(core_use+leg_use[0:1], 3))         #only use one leg
                        all_comb=[str(pp[0])+','+str(pp[1])+','+str(pp[2]) for pp in t1]
                        
                    #g=all_comb[0]                    
                    t_array=[]
                    for g in all_comb:
                        t_array.append((np.array([int(g.split(',')[0]),int(g.split(',')[1]),int(g.split(',')[2])])))
                    t_array=[list(np.sort(k)) for k in t_array]
                    
                    all_comb=[]
                    for g in t_array:
                        out_temp=','.join(str(r) for r in g)
                        all_comb.append(out_temp)
                    
                    output_horse=pd.DataFrame([])                   
                    output_horse['horse_comb']=all_comb
                    
                    output_horse['Date']=win_horse_use_temp['Date'].values[0]
                    output_horse['RaceNo']=win_horse_use_temp['RaceNo'].values[0]
                    output_horse['Date_RaceNo']=win_horse_use_temp['Date_RaceNo'].values[0]
                    
                    return output_horse            


            
                #find bets combination
                tri_horse_use=raw_win.groupby(['Date','RaceNo']).apply(lambda x: find_n_horse_tri(x.reset_index(drop=True)))
                tri_horse_use=tri_horse_use.reset_index(drop=True)
                tri_horse_use['one']=1
            
                df=pd.merge(df,tri_horse_use[['Date_RaceNo','horse_comb','one']].copy(),how='left',on=['Date_RaceNo','horse_comb'])
                df_check=df.head(1000)
                
                #remove bet with 25s odd <10
                bet_key=(df['one']==1)
                
                del df[define_amount]
                df.loc[bet_key,define_amount]=bet_amount_set
                df[define_amount]=df[define_amount].fillna(0)
                
    
                del df[betting_strategy]
                df.loc[bet_key,betting_strategy]=True
                df[betting_strategy]=df[betting_strategy].fillna(False)
                
                #take a look on hr1 hr2 odds at 25s
                raw_win=raw_win_0.copy()
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_1'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_1_25s_odd'})
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_2'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_2_25s_odd'})
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_3'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_3_25s_odd'})
                
                #check qin odd distribution
                df_check=df[['Date','RaceNo','horse_no_1_25s_odd','horse_no_2_25s_odd','horse_no_3_25s_odd','dividend','tri']].copy()
                df_check=df_check.loc[df_check['tri']==1,:].copy()
                
                df_check.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+"_"+time_now+"_"+'odds_distribution.csv'), index=False)





            if ii=='Tie':    
                raw_win=raw_win_0.copy()
                raw_win=raw_win.sort_values(by=['Date','RaceNo',model_field],ascending=[True,True,False])
                raw_win_select2=raw_win.groupby(['Date','RaceNo']).head(tie_model_use)
                raw_win_select2['indicator_model']=1

                raw_win=pd.merge(raw_win,raw_win_select2[['Date_RaceNo','HorseNo','indicator_model']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_model']=raw_win['indicator_model'].fillna(999)
                
                leg_select=raw_win_0.copy()
                leg_select2=pd.merge(leg_select,factorDB[['Date_RaceNo','HorseNo','leg_factor']].copy(),how='left',on=['Date_RaceNo','HorseNo'])  
                leg_select2=leg_select2.sort_values(by=['Date','RaceNo','leg_factor'],ascending=[True,True,False])
                leg_select2=leg_select2.groupby(['Date','RaceNo']).head(4)
                leg_select2.loc[leg_select2['leg_factor']==1,'indicator_leg']=1


    
                win_nice=win_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                win_nice_select2=win_nice.groupby(['Date','RaceNo']).head(3)
                win_nice_select2=win_nice_select2.loc[win_nice_select2['odd_300s_change']<0,:].copy()
                win_nice_select2['indicator_odd_300s_change_win']=1  
                
                pla_nice=pla_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                pla_nice_select2=pla_nice.groupby(['Date','RaceNo']).head(3)
                pla_nice_select2=pla_nice_select2.loc[pla_nice_select2['odd_300s_change']<0,:].copy()
                pla_nice_select2['indicator_odd_300s_change_pla']=1
    

                
                raw_win=pd.merge(raw_win,win_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_win']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_win']=raw_win['indicator_odd_300s_change_win'].fillna(999)
                
                raw_win=pd.merge(raw_win,pla_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_pla']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_pla']=raw_win['indicator_odd_300s_change_pla'].fillna(999) 

                raw_win=pd.merge(raw_win,leg_select2[['Date_RaceNo','HorseNo','indicator_leg']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_leg']=raw_win['indicator_leg'].fillna(999) 

#                core_criteria=(raw_win['indicator_model']==1) | ((raw_win['indicator_odd_300s_change_win']==1) & (raw_win['indicator_odd_300s_change_pla']==1))

                core_criteria=(raw_win['indicator_model']==1) & (raw_win[use_field]>prob_cutoff)


        
                sum(core_criteria)
    
                raw_win_core=raw_win[core_criteria].copy()
                raw_win_core['core']=1
                raw_win=pd.merge(raw_win,raw_win_core[['Date_RaceNo','HorseNo','core']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['core']=raw_win['core'].fillna(999)

                #raw_win=raw_win.sort_values(by=['Date','RaceNo','core','win_odd_25s'],ascending=[True,True,True,True])
                #raw_win=raw_win.sort_values(by=['Date','RaceNo','core','indicator_odd_300s_change_pla','win_odd_25s'],ascending=[True,True,True,True,True])
                
                #only select out these rule
                #raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_odd_300s_change_pla']==1))&(raw_win['win_odd_25s']<odd_cap_tie),:].copy()
                raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_leg']==1))&(raw_win['win_odd_25s']<odd_cap_tri),:].copy()
                
                raw_win=raw_win.reset_index(drop=True)
                
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-09-01_7',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-03-29_5',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2021-04-17_3',:].copy().reset_index(drop=True)
                
                def find_n_horse_tie(win_horse_use_temp):
                    effective_hr=win_horse_use_temp.sts.values[0]
                    use_horse=100 if effective_hr>=12 else 100
                    all_comb=[]
                    core_use=win_horse_use_temp.loc[win_horse_use_temp['core']==1,'HorseNo'].values.tolist()
                    leg_use=win_horse_use_temp.loc[win_horse_use_temp['core']==999,'HorseNo'].values.tolist()
                    leg_use=leg_use[0:use_horse-len(core_use)]
                    if (len(core_use)==1) & (len(leg_use)>=2) :
                        t1=list(itertools.combinations(leg_use, 2))
                        t1=[list(pp) for pp in t1]                       
                        #pp=t1[0]
                        for pp in t1:
                            t2=core_use+pp
                            t2=list(itertools.permutations(t2, 3))
                            all_comb_temp=[str(pp[0])+','+str(pp[1])+','+str(pp[2]) for pp in t2]
                            all_comb=all_comb+all_comb_temp

                    if (len(core_use)==2) & (len(leg_use)>=1) :                     
                        #pp=leg_use[0]
                        for pp in leg_use:
                            t2=core_use+[pp]
                            t2=list(itertools.permutations(t2, 3))
                            all_comb_temp=[str(pp[0])+','+str(pp[1])+','+str(pp[2]) for pp in t2]
                            all_comb=all_comb+all_comb_temp

#                    if (len(core_use)==3):  
#                        t1=list(itertools.combinations(core_use, 1));t1=[list(pp) for pp in t1] 
#                        t2=list(itertools.combinations(core_use, 2));t2=[list(pp) for pp in t2] 
#                        t3=list(itertools.combinations(core_use, 3));t3=[list(pp) for pp in t3] 
#                        #pp=t1[0]
#                        for pp in t1:
#                            if len(leg_use)>=2:
#                                qq=list(itertools.combinations(leg_use, 2))
#                                qq=[list(x) for x in qq] 
#                                
#                                xx=qq[0]
#                                for xx in qq:
#                                    t4=pp+list(xx)
#                                    t4=list(itertools.permutations(t4, 3))
#                                    all_comb_temp=[str(zz[0])+','+str(zz[1])+','+str(zz[2]) for zz in t4]
#                                    all_comb=all_comb+all_comb_temp
#                                
#                        #pp=t2[0]
#                        for pp in t2:
#                            if len(leg_use)>=1:
#                                qq=list(itertools.combinations(leg_use, 1))
#                                qq=[list(x) for x in qq] 
#                                
#                                xx=qq[0]
#                                for xx in qq:
#                                    t4=pp+xx
#                                    t4=list(itertools.permutations(t4, 3))
#                                    all_comb_temp=[str(zz[0])+','+str(zz[1])+','+str(zz[2]) for zz in t4]
#                                    all_comb=all_comb+all_comb_temp
#                    
#                        t4=list(itertools.permutations(t3[0], 3))
#                        all_comb_temp=[str(zz[0])+','+str(zz[1])+','+str(zz[2]) for zz in t4]
#                        all_comb=all_comb+all_comb_temp
                        
                    if (len(core_use)>=3): 
                        t1=list(itertools.permutations(core_use+leg_use[0:1], 3))                 #only use one leg
                        all_comb=[str(pp[0])+','+str(pp[1])+','+str(pp[2]) for pp in t1]
                    
                    
                    output_horse=pd.DataFrame([])                   
                    output_horse['horse_comb']=all_comb
                    
                    output_horse['Date']=win_horse_use_temp['Date'].values[0]
                    output_horse['RaceNo']=win_horse_use_temp['RaceNo'].values[0]
                    output_horse['Date_RaceNo']=win_horse_use_temp['Date_RaceNo'].values[0]
                    
                    return output_horse            


            
                #find bets combination
                tie_horse_use=raw_win.groupby(['Date','RaceNo']).apply(lambda x: find_n_horse_tie(x.reset_index(drop=True)))
                tie_horse_use=tie_horse_use.reset_index(drop=True)
                tie_horse_use['one']=1
            
                df=pd.merge(df,tie_horse_use[['Date_RaceNo','horse_comb','one']].copy(),how='left',on=['Date_RaceNo','horse_comb'])
                df_check=df.head(1000)
                
                #remove bet with 25s odd <10
                bet_key=(df['one']==1)
                
                del df[define_amount]
                df.loc[bet_key,define_amount]=bet_amount_set
                df[define_amount]=df[define_amount].fillna(0)
                
    
                del df[betting_strategy]
                df.loc[bet_key,betting_strategy]=True
                df[betting_strategy]=df[betting_strategy].fillna(False)
                
                #take a look on hr1 hr2 odds at 25s
                raw_win=raw_win_0.copy()
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_1'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_1_25s_odd'})
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_2'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_2_25s_odd'})
                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_3'],right_on=['Date','RaceNo','HorseNo'])
                df=df.rename(columns={'win_odd_25s':'horse_no_3_25s_odd'})
                
                #check qin odd distribution
                df_check=df[['Date','RaceNo','horse_no_1_25s_odd','horse_no_2_25s_odd','horse_no_3_25s_odd','dividend','tri']].copy()
                df_check=df_check.loc[df_check['tri']==1,:].copy()
                
                df_check.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+"_"+time_now+"_"+'odds_distribution.csv'), index=False)





            if ii=='Qur':    
                raw_win=raw_win_0.copy()
                raw_win=raw_win.sort_values(by=['Date','RaceNo',model_field],ascending=[True,True,False])
                raw_win_select2=raw_win.groupby(['Date','RaceNo']).head(qur_model_use)
                raw_win_select2['indicator_model']=1

    
                raw_win=pd.merge(raw_win,raw_win_select2[['Date_RaceNo','HorseNo','indicator_model']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_model']=raw_win['indicator_model'].fillna(999)

                leg_select=raw_win_0.copy()
                leg_select2=pd.merge(leg_select,factorDB[['Date_RaceNo','HorseNo','leg_factor']].copy(),how='left',on=['Date_RaceNo','HorseNo'])  
                leg_select2=leg_select2.sort_values(by=['Date','RaceNo','leg_factor'],ascending=[True,True,False])
                leg_select2=leg_select2.groupby(['Date','RaceNo']).head(4)
                leg_select2.loc[leg_select2['leg_factor']==1,'indicator_leg']=1
  
                win_nice=win_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                win_nice_select2=win_nice.groupby(['Date','RaceNo']).head(3)
                win_nice_select2=win_nice_select2.loc[win_nice_select2['odd_300s_change']<0,:].copy()
                win_nice_select2['indicator_odd_300s_change_win']=1  
                
                pla_nice=pla_nice.sort_values(by=['Date','RaceNo','odd_300s_change'],ascending=[True,True,True])
                pla_nice_select2=pla_nice.groupby(['Date','RaceNo']).head(3)
                pla_nice_select2=pla_nice_select2.loc[pla_nice_select2['odd_300s_change']<0,:].copy()
                pla_nice_select2['indicator_odd_300s_change_pla']=1

                
                raw_win=pd.merge(raw_win,win_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_win']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_win']=raw_win['indicator_odd_300s_change_win'].fillna(999)
                
                raw_win=pd.merge(raw_win,pla_nice_select2[['Date_RaceNo','HorseNo','indicator_odd_300s_change_pla']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_odd_300s_change_pla']=raw_win['indicator_odd_300s_change_pla'].fillna(999) 

                raw_win=pd.merge(raw_win,leg_select2[['Date_RaceNo','HorseNo','indicator_leg']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['indicator_leg']=raw_win['indicator_leg'].fillna(999) 

#                core_criteria=(raw_win['indicator_model']==1) | ((raw_win['indicator_odd_300s_change_win']==1) & (raw_win['indicator_odd_300s_change_pla']==1))

                core_criteria=(raw_win['indicator_model']==1) & (raw_win[use_field]>prob_cutoff)


        
                sum(core_criteria)
    
                raw_win_core=raw_win[core_criteria].copy()
                raw_win_core['core']=1
                raw_win=pd.merge(raw_win,raw_win_core[['Date_RaceNo','HorseNo','core']].copy(),how='left',on=['Date_RaceNo','HorseNo'])
                raw_win['core']=raw_win['core'].fillna(999)

                #raw_win=raw_win.sort_values(by=['Date','RaceNo','core','win_odd_25s'],ascending=[True,True,True,True])
                raw_win=raw_win.sort_values(by=['Date','RaceNo','core','indicator_odd_300s_change_pla','win_odd_25s'],ascending=[True,True,True,True,True])
                
                #only select out these rule
                raw_win=raw_win.loc[((raw_win['core']==1)|(raw_win['indicator_leg']==1))&(raw_win['win_odd_25s']<odd_cap_qur),:].copy()


                
                raw_win=raw_win.reset_index(drop=True)
                
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2019-09-01_7',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-03-29_5',:].copy().reset_index(drop=True)
                
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-02-08_7',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-04-08_2',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-04-29_1',:].copy().reset_index(drop=True)
                win_horse_use_temp=raw_win.loc[raw_win['Date_RaceNo']=='2020-07-12_5',:].copy().reset_index(drop=True)
                
                
                
                def find_n_horse_qur(win_horse_use_temp):
                    effective_hr=win_horse_use_temp.sts.values[0]
                    use_horse=100 if effective_hr>=12 else 100
                    all_comb=[]
                    core_use=win_horse_use_temp.loc[win_horse_use_temp['core']==1,'HorseNo'].values.tolist()
                    leg_use=win_horse_use_temp.loc[win_horse_use_temp['core']==999,'HorseNo'].values.tolist()
                    leg_use=leg_use[0:use_horse-len(core_use)]
                    if (len(core_use)==1) & (len(leg_use)>=3) :
                        t1=list(itertools.combinations(leg_use, 3))
                        t1=[list(pp) for pp in t1]                       
                        #pp=t1[0]
                        for pp in t1:
                            t2=core_use+pp
                            t2=list(itertools.permutations(t2, 4))
                            all_comb_temp=[str(pp[0])+','+str(pp[1])+','+str(pp[2])+','+str(pp[3]) for pp in t2]
                            all_comb=all_comb+all_comb_temp

                    if (len(core_use)==2) & (len(leg_use)>=2) :                     
                        t1=list(itertools.combinations(leg_use, 2))
                        t1=[list(pp) for pp in t1]                       
                        #pp=t1[0]
                        for pp in t1:
                            t2=core_use+pp
                            t2=list(itertools.permutations(t2, 4))
                            all_comb_temp=[str(pp[0])+','+str(pp[1])+','+str(pp[2])+','+str(pp[3]) for pp in t2]
                            all_comb=all_comb+all_comb_temp

                    if (len(core_use)==3) & (len(leg_use)>=1) :                     
                        t1=list(itertools.combinations(leg_use, 1))
                        t1=[list(pp) for pp in t1]                       
                        #pp=t1[0]
                        for pp in t1:
                            t2=core_use+pp
                            t2=list(itertools.permutations(t2, 4))
                            all_comb_temp=[str(pp[0])+','+str(pp[1])+','+str(pp[2])+','+str(pp[3]) for pp in t2]
                            all_comb=all_comb+all_comb_temp
                            
                    if (len(core_use)>=4) & (len(leg_use)>=0):  
#                        t1=list(itertools.combinations(core_use, 1));t1=[list(pp) for pp in t1] 
#                        t2=list(itertools.combinations(core_use, 2));t2=[list(pp) for pp in t2] 
#                        t3=list(itertools.combinations(core_use, 3));t3=[list(pp) for pp in t3]
#                        t4=list(itertools.combinations(core_use, 4));t4=[list(pp) for pp in t4]
#                        #pp=t1[0]
#                        for pp in t1:
#                            if len(leg_use)>=3:
#                                qq=list(itertools.combinations(leg_use, 3))
#                                qq=[list(x) for x in qq] 
#                                
#                                xx=qq[0]
#                                for xx in qq:
#                                    t5=pp+list(xx)
#                                    t5=list(itertools.permutations(t5, 4))
#                                    all_comb_temp=[str(zz[0])+','+str(zz[1])+','+str(zz[2])+','+str(zz[3]) for zz in t5]
#                                    all_comb=all_comb+all_comb_temp
#                                
#                        #pp=t2[0]
#                        for pp in t2:
#                            if len(leg_use)>=2:
#                                qq=list(itertools.combinations(leg_use, 2))
#                                qq=[list(x) for x in qq] 
#                                
#                                xx=qq[0]
#                                for xx in qq:
#                                    t5=pp+xx
#                                    t5=list(itertools.permutations(t5, 4))
#                                    all_comb_temp=[str(zz[0])+','+str(zz[1])+','+str(zz[2])+','+str(zz[3]) for zz in t5]
#                                    all_comb=all_comb+all_comb_temp
#
#                        #pp=t2[0]
#                        for pp in t3:
#                            if len(leg_use)>=1:
#                                qq=list(itertools.combinations(leg_use, 1))
#                                qq=[list(x) for x in qq] 
#                                
#                                xx=qq[0]
#                                for xx in qq:
#                                    t5=pp+xx
#                                    t5=list(itertools.permutations(t5, 4))
#                                    all_comb_temp=[str(zz[0])+','+str(zz[1])+','+str(zz[2])+','+str(zz[3]) for zz in t5]
#                                    all_comb=all_comb+all_comb_temp                    
                    
                    
                    
                    
                        t1=list(itertools.permutations(core_use, 4))
                        all_comb_temp=[str(zz[0])+','+str(zz[1])+','+str(zz[2])+','+str(zz[3]) for zz in t1]
                        all_comb=all_comb+all_comb_temp
                    
                    
                    output_horse=pd.DataFrame([])                   
                    output_horse['horse_comb']=all_comb
                    
                    output_horse['Date']=win_horse_use_temp['Date'].values[0]
                    output_horse['RaceNo']=win_horse_use_temp['RaceNo'].values[0]
                    output_horse['Date_RaceNo']=win_horse_use_temp['Date_RaceNo'].values[0]
                    
                    return output_horse            


            
                #find bets combination
                qur_horse_use=raw_win.groupby(['Date','RaceNo']).apply(lambda x: find_n_horse_qur(x.reset_index(drop=True)))
                qur_horse_use=qur_horse_use.reset_index(drop=True)
                qur_horse_use['one']=1
            
                df=pd.merge(df,qur_horse_use[['Date_RaceNo','horse_comb','one']].copy(),how='left',on=['Date_RaceNo','horse_comb'])
                df_check=df.head(1000)
                
                #remove bet with 25s odd <10
                bet_key=(df['one']==1)
                
                del df[define_amount]
                df.loc[bet_key,define_amount]=bet_amount_set
                df[define_amount]=df[define_amount].fillna(0)
                
    
                del df[betting_strategy]
                df.loc[bet_key,betting_strategy]=True
                df[betting_strategy]=df[betting_strategy].fillna(False)
                
#                #take a look on hr1 hr2 odds at 25s
#                raw_win=raw_win_0.copy()
#                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_1'],right_on=['Date','RaceNo','HorseNo'])
#                df=df.rename(columns={'win_odd_25s':'horse_no_1_25s_odd'})
#                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_2'],right_on=['Date','RaceNo','HorseNo'])
#                df=df.rename(columns={'win_odd_25s':'horse_no_2_25s_odd'})
#                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_3'],right_on=['Date','RaceNo','HorseNo'])
#                df=df.rename(columns={'win_odd_25s':'horse_no_3_25s_odd'})
#                df=pd.merge(df,raw_win[['Date','RaceNo','HorseNo','win_odd_25s']].copy(),how='left',left_on=['Date','RaceNo','horse_no_4'],right_on=['Date','RaceNo','HorseNo'])
#                df=df.rename(columns={'win_odd_25s':'horse_no_4_25s_odd'})
#                
#                #check qin odd distribution
#                df_check=df[['Date','RaceNo','horse_no_1_25s_odd','horse_no_2_25s_odd','horse_no_3_25s_odd','horse_no_4_25s_odd','dividend','tri']].copy()
#                df_check=df_check.loc[df_check['tri']==1,:].copy()
#                
#                df_check.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+"_"+time_now+"_"+'odds_distribution.csv'), index=False)


            #scale ratio



            
            df['betting_amount'] = df[betting_strategy] * df[define_amount]*bet_ratio[bet_type]
            #bet amount cap
            if (bet_type=='Win')|(bet_type=='Pla')|(bet_type=='Qin')|(bet_type=='Qpl'):
                df['bet_ratio']=df['betting_amount']/df[capture_public_pooltotal]
            else:
                df['bet_ratio']=999
            #df['check']=np.where(df['betting_amount2']!=df['betting_amount'],False,True)
            df['return'] = df['betting_amount'] * df[win_comb] * df['dividend']
            df['payoff'] = df['return'] - df['betting_amount']
            
            if use_rebate==True:
                df.loc[df['betting_amount']>=10000,'rebate'] =rebate_rate*df['betting_amount']
            else:
                df['rebate']=0
            
            TotalNoofRace=len(df['Date_RaceNo'].unique())
            
            #adjust the odds
            #xx=df.loc[(df['Date']=='2018-09-02')&(df['RaceNo']==10),:]
            def adjust_dividend(xx):
                bet_amount_total=xx['betting_amount'].sum()
                original_pool=xx[final_pooltotal][0]
                adjusted_pool=original_pool+bet_amount_total
                
                xx.loc[xx[win_comb]==1,'pool_individual']=track_tick*original_pool/(mul*xx['dividend'])
                xx['pool_individual']=xx['pool_individual'].fillna(0)
                
                xx['pool_plus_betamount']=xx['pool_individual']+xx['betting_amount']

                xx.loc[xx[win_comb]==1,'adjusted_dividend']=track_tick*adjusted_pool/(mul*xx['pool_plus_betamount'])
                xx['adjusted_dividend']=xx['adjusted_dividend'].fillna(0)

                return xx

            
            if (bet_type=='Win')|(bet_type=='Pla')|(bet_type=='Qin')|(bet_type=='Qpl'):
                df=df.groupby(['Date_RaceNo']).apply(lambda xx:adjust_dividend(xx.reset_index(drop=True)))
                df=df.reset_index(drop=True)
                df['adjusted_dividend']=df['adjusted_dividend'].apply(lambda x:np.float(x))
            else:
                df['adjusted_dividend']=df['dividend'].copy()
            
            df['return'] = df['betting_amount'] * df[win_comb] * df['adjusted_dividend']
            df['payoff'] = df['return'] - df['betting_amount']+df['rebate']
            TotalNoofRace=len(df['Date_RaceNo'].unique())
            
            #information per bet
            df2=df.loc[df['betting_amount']!=0,:]
            df2=df2.sort_values(['Date_RaceNo'],)
            df2.is_copy=False
            poff=np.cumsum(df2.payoff.values)
            df2['CumPnL']=poff
            Average_amount=df2['betting_amount'].mean()
            Min_Amount=df2['betting_amount'].min()
            Max_amount=df2['betting_amount'].max()
            no_of_bet = df2.shape[0]
            Max_bet_ratio=df2['bet_ratio'].max()
            max_no_of_bet_per_race=df2['Date_RaceNo'].value_counts().max()
            
            #information per race
            df_race=df2[['Date_RaceNo','horse_comb','Date','betting_amount','payoff']].copy()
            df_race=df_race.groupby(['Date_RaceNo']).sum()
            df_race['Date_RaceNo']=df_race.index
            cols = df_race.columns.tolist();cols.insert(0, cols.pop(cols.index('Date_RaceNo')))
            df_race = df_race.reindex(columns= cols)
            df_race.is_copy=False
            poff=np.cumsum(df_race.payoff.values)
            df_race['CumPnL']=poff
            Average_bet_amount_per_race=df_race['betting_amount'].mean()       
            Max_bet_amount_per_race=df_race['betting_amount'].max()
            TotalNoofRaceBetted=df_race.shape[0]                        
            if TotalNoofRaceBetted==0:
                average_bets_per_race=0
            else:
                average_bets_per_race=no_of_bet/TotalNoofRaceBetted
            

            #information per Date
            df_date=df2[['Date_RaceNo','horse_comb','Date','betting_amount','payoff']].copy()
            df_date=df_date.groupby(['Date']).sum()
            df_date['Date']=df_date.index
            cols = df_date.columns.tolist();cols.insert(0, cols.pop(cols.index('Date')))
            df_date = df_date.reindex(columns= cols)
            df_date.is_copy=False
            poff=np.cumsum(df_date.payoff.values)
            df_date['CumPnL']=poff
            if len(np.maximum.accumulate(df_date.CumPnL) - df_date.CumPnL)==0:
                DrawDown=0
            else:
                DrawDown=max(np.maximum.accumulate(df_date.CumPnL) - df_date.CumPnL)

            Max_CumPnL=df_date['CumPnL'].max()
            Min_CumPnL=df_date['CumPnL'].min()
            
            df_date=df_date.reset_index(drop=True)

            total_amount = df['betting_amount'].sum()
            total_return = df['return'].sum()
            profit = total_return - total_amount
            roi = (total_return - total_amount)/total_amount if total_amount!=0 else 0

            
            hit = (df[win_comb] * (df['gu_amount'] > 0)).sum()
            hit_rate = 1.0*hit/no_of_bet if no_of_bet!=0 else 0
            
            #find grouped pnl
            if df_grouped_pnl.shape[0]==0:                
                df_grouped_pnl=df_date[['Date','betting_amount','payoff']].copy()
                df_grouped_pnl=df_grouped_pnl.rename(columns={'betting_amount':'betting_amount'+'_'+bet_type,'payoff':'pnl'+'_'+bet_type})
            else:
                temp_pnl=df_date[['Date','betting_amount','payoff']].copy()
                temp_pnl=temp_pnl.rename(columns={'betting_amount':'betting_amount'+'_'+bet_type,'payoff':'pnl'+'_'+bet_type})

                df_grouped_pnl=df_grouped_pnl.append(temp_pnl)
                

            if (plotyes==True)&(df_date.shape[0]!=0):
                name=strategy+'_'+str(year)
                plt.xlabel('Date')
                plt.ylabel('Cumulative P&L HKD$')
                plt.title(betting_strategy+" Year "+str(year), fontsize=14)
                #my_dpi=700
                #plt.figure(figsize=(8/my_dpi, 4/my_dpi), dpi=my_dpi)
                plt.gcf().set_size_inches(5, 2)
                
                x = [dt.datetime.strptime(d,'%Y-%m-%d').date() for d in df_date['Date']]
                plt.plot(x,df_date['CumPnL'],linestyle='-', marker=',',markersize=3, color='blue')
                plt.axhline(y=0.0, color='r', linestyle='-')
               
                
                plt.gcf().autofmt_xdate()
                plt.gca().yaxis.set_major_formatter(tkr.FuncFormatter(lambda y,  p: format(int(y), ',')))
                text_size=7
                #'TotalNo.ofRace','AverageNo.ofBetsPerRace','AveragBetAmountPerRace','MaxBetAmountPerRace'
                plt.annotate("P&L "+str(format(int(profit),',')), xy=(1, 0.9), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Investment "+str(format(int(total_amount),',')), xy=(1, 0.82), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("ROI "+str(round(roi*100,2))+'%', xy=(1, 0.74), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("DrawDown "+str(format(int(DrawDown),',')), xy=(1, 0.66), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Min.Cum.P&l (Loss) "+str(format(int(Min_CumPnL),',')), xy=(1, 0.58), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Total No.of Races "+str(format(int(TotalNoofRace),',')), xy=(1, 0.50), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Total No.of Races Betted "+str(format(int(TotalNoofRaceBetted),',')), xy=(1, 0.42), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                
                plt.annotate("Total No.of Bets "+str(format(int(no_of_bet),',')), xy=(1, 0.29), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Total No.of Winning Bets "+str(format(int(hit),',')), xy=(1, 0.21), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Hit Rate "+str(round(hit_rate*100,2))+'%', xy=(1, 0.13), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                
                plt.annotate("Average No.of Bets Per Race "+str(format(int(average_bets_per_race),',')), xy=(1, 0.0), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Max. No.of Bets Per Race "+str(format(int(max_no_of_bet_per_race),',')), xy=(1, -0.08), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Average Bet Amount Per Race "+str(format(int(Average_bet_amount_per_race),',')), xy=(1, -0.16), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Max. Total Bet Amount Per Race "+str(format(int(Max_bet_amount_per_race),',')), xy=(1, -0.24), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Average Bet Amount Per Bet "+str(format(int(Average_amount),',')), xy=(1, -0.32), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)
                plt.annotate("Max. Bet Amount Per Bet "+str(format(int(Max_amount),',')), xy=(1, -0.40), xytext=(8, 0),xycoords=('axes fraction', 'axes fraction'), textcoords='offset points',color='black',size=text_size)                
                
                
                plt.savefig(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+'_'+name+"_"+time_now+"_"+'.png'), bbox_inches='tight',dpi=700,figsize=(10,2))
                plt.close()
                #plot bet amount
                # change outlier point symbols
                #target='betting_amount'
                #target='advantage'
                def plot_multiple(target):
                    if len(df[target].values)!=0:
                        plt.figure()
                        betting_amount_all=df.loc[:,target].values
                    else:
                        betting_amount_all=pd.Series([])
                        
                    if len(df.loc[df[win_comb]==1,target].values)!=0:
                        plt.figure()
                        betting_amount_win_combination_only=df.loc[df[win_comb]==1,target].values
                    else:
                        betting_amount_win_combination_only=pd.Series([])
                        
                    if len(df.loc[(df[win_comb]==1)&(df['betting_amount']!=0),target].values)!=0:
                        plt.figure()
                        betting_amount_model_successful_bet=df.loc[(df[win_comb]==1)&(df['betting_amount']!=0),target].values
                    else:
                        betting_amount_model_successful_bet=pd.Series([])
                        
                    plt.boxplot ([betting_amount_all,betting_amount_win_combination_only,betting_amount_model_successful_bet])
                    plt.annotate('1-'+target+' all,'+'2-'+target+' win combination only,'+'3-'+target+' model successful bet', xy=(0.05, -0.15), xycoords='axes fraction')
                    plt.savefig(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+'_'+name+"_"+time_now+"_"+target+'.png'), bbox_inches='tight',dpi=700,figsize=(10,2))
                    plt.close()
                    
                plot_multiple('betting_amount')
                plot_multiple('advantage')
                plot_multiple(model_prob)
                plot_multiple('trimix')
                #plot_multiple('trifectaProb_Gu')
   
                if len(df2['Date_RaceNo'].value_counts())!=0:
                    #%matplotlib inline
                    plt.hist(df2['Date_RaceNo'].value_counts(), density=False, bins=30)
                    plt.ylabel('race counts');plt.xlabel('No. of Bets per race')
                    plt.savefig(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+'_'+name+"_"+time_now+"_"+'no_of_bets_per_race.png'), bbox_inches='tight',dpi=700,figsize=(10,2))
                    plt.close()
                    

                df.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+'_'+name+"_"+time_now+"_"+'raw_data_df.csv'), index=False)
                df2.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+'_'+name+"_"+time_now+"_"+'raw_data_df2.csv'), index=False)
                df_race.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+'_'+name+"_"+time_now+"_"+'raw_data_df_race.csv'), index=False)
                df_date.to_csv(os.path.join(output_path,'detail_plot',bet_type+'_'+str(year)+'_'+name+"_"+time_now+"_"+'raw_data_df_date.csv'), index=False)

            df1 = df1.append(pd.Series([strategy,TotalNoofRace,TotalNoofRaceBetted,average_bets_per_race,max_no_of_bet_per_race,Average_bet_amount_per_race,
                                        Max_bet_amount_per_race,total_amount,Average_amount,Min_Amount,Max_amount,
                                        Max_bet_ratio,total_return, profit, roi,DrawDown,Max_CumPnL,Min_CumPnL, no_of_bet, hit, hit_rate],
                                       index=output_col), ignore_index=True) 

    return df1,df_grouped_pnl





# trif = pd.read_csv("prob/TriProb_Predict_2017.csv")
#import matplotlib.pyplot as plt
import os
#import matplotlib.dates as mdates
#import matplotlib.patches as mpatches
import numpy as np
import datetime as dt
import matplotlib.ticker as tkr
import matplotlib.pyplot as plt
import sys
import datetime
now = datetime.datetime.now()
time_now=str(now.year)+"_"+str(now.month)+"_"+str(now.day)+"_"+str(now.hour)+"_"+str(now.minute)+"_"+str(now.second)

plotyes=True
use_rebate=False
new_horse_handle=False
use_advantage_cap=True
per_bet_scale_up_to=10
capital=10000000
portion=0.2
multiplier=1.0 #1.0
rebate_rate=0.12
expert='BenterNote'


use_new_betting_strategy=True



#year_list=[2015,2016,2017]
#runno_list=[2440,2441,2442]


year_list=[2015,2016,2017,2018,2019,2020]


runno_list=[3561,3562,3563]
runno_list=[4004,4005,4006]
runno_list=[3609,3610,3611]
runno_list=[3561,3562,3563]
runno_list=[1000,1001,1002]
runno_list=[3606,3607,3608]

year_list=[2015,2016,2017]
runno_list=[5000,5001,5002]
runno_list=[5000,5001,5002,5003,5004,5005]


year_list=[2019,2020,2021]
runno_list=[5004,5005]
runno_list=[5004,5005]

runno_list=[5000,5001,5002]



year_list=[2020]
runno_list=[4995]


year_list=[2015,2016,2017,2018,2019,2020,2021]
runno_list=[4990,4991,4992,4993,4994,4995,4996]

#year_list=[2015,2016,2017,2018]
#runno_list=[4990,4991,4992,4993]


year_list=[2015]
runno_list=[5000]

year_list=[2015,2016,2017,2018,2019,2020,2021]
runno_list=[5000,5001,5002,5003,5004,5005,5006,5007]





try:
    year_list=[int(k) for k in sys.argv[1].split(':')]
    runno_list=[int(k) for k in sys.argv[2].split(':')]
    all_bet_type=sys.argv[3].split(':')
except:
    pass





input_path=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot')

output_path =os.path.join(input_path,str(runno_list[0]),'temp') 

output_path_plot =os.path.join(output_path,'detail_plot')
if not os.path.exists(output_path_plot):
    os.makedirs(output_path_plot)

        
##make folder writeable
#after making output_path writeable, everything created inside later can be removed/edited by other users
    
import os

#def change_permissions_recursive(temp_folder1, mode):
#    for root, dirs, files in os.walk(temp_folder1, topdown=False):
#        for dir in [os.path.join(root,d) for d in dirs]:
#            os.chmod(dir, mode)
#        for file in [os.path.join(root, f) for f in files]:
#            os.chmod(file, mode)
#change_permissions_recursive(output_path, 0o777) 


os.chmod(output_path_plot, 0o777)




outsub = pd.DataFrame([])

model_coef_start=1.0#0.44#0.4
model_coef_end=1.3#0.6#0.5
model_coef_increment=3.0#0.02#0.02#1.0#0.05

model_adv_start=1.0#1.3
model_adv_end=1.3#1.5
model_adv_increment=3.0#1.0#0.02#0.1

p_adj_coef_start=0.005#1.1#1.0
p_adj_coef_end=0.01#1.5
p_adj_coef_increment=1.0#0.001#0.0005#1.0#0.001#0.005#0.005#0.03#0.1#0.1


year=2011
min_adv=1.0#1.0#1.1
model_coef=1.0#0.44
p_adj_coef=0.005
ii='Pla'
ii='Qpl'
ii='Tie'
ii='Qin'
ii='Win'
ii='Qur'
ii='Tri'




for ii in all_bet_type:
#for ii in ['Win']:
    vars()['df_grouped_pnl'+'_'+ii]=pd.DataFrame([])
    if ii=='Qin':
        bet_type='Qin'
        main_filename=input_path
        model_prob=ii+'Prob_model_conversion'
        capture_public_prob=bet_type.lower()+'_prob_25s'
        capture_public_odd=bet_type.lower()+'_odd_25s'
        capture_public_pooltotal='pool_25s'
        final_pooltotal='pool_final'
        track_tick=0.825
        win_comb='tri'
    
    if ii=='Qpl':    
        bet_type='Qpl'
        main_filename=input_path
        model_prob=ii+'Prob_model_conversion'
        capture_public_prob=bet_type.lower()+'_prob_25s'
        capture_public_odd=bet_type.lower()+'_odd_25s'
        capture_public_pooltotal='pool_25s'
        final_pooltotal='pool_final'
        track_tick=0.825
        win_comb='tri'
    
    if ii=='Win':    
        bet_type='Win'
        main_filename=input_path
        model_prob=ii+'Prob_model_conversion'#'win_prob_25s'#
        capture_public_prob=bet_type.lower()+'_prob_25s'
        capture_public_odd=bet_type.lower()+'_odd_25s'
        capture_public_pooltotal='pool_25s'
        final_pooltotal='pool_final'
        track_tick=0.825
        win_comb='tri'



    
    if ii=='Pla':
        bet_type='Pla'
        main_filename=input_path
        model_prob=ii+'Prob_model_conversion'
        capture_public_prob=bet_type.lower()+'_prob_25s'
        capture_public_odd=bet_type.lower()+'_odd_25s'
        capture_public_pooltotal='pool_25s'
        final_pooltotal='pool_final'
        track_tick=0.825
        win_comb='tri'

    if ii=='Tri':
        bet_type='Tri'
        main_filename=input_path
        model_prob=ii+'Prob_model_conversion'
        capture_public_prob=bet_type.lower()+'_prob_25s'
        capture_public_odd=bet_type.lower()+'_odd_25s'
        capture_public_pooltotal='pool_25s'
        final_pooltotal='pool_final'
        track_tick=0.75
        win_comb='tri'

    if ii=='Tie':
        bet_type='Tie'
        main_filename=input_path
        model_prob=ii+'Prob_model_conversion'
        capture_public_prob=bet_type.lower()+'_prob_25s'
        capture_public_odd=bet_type.lower()+'_odd_25s'
        capture_public_pooltotal='pool_25s'
        final_pooltotal='pool_final'
        track_tick=0.75
        win_comb='tri'

    if ii=='Qur':
        bet_type='Qur'
        main_filename=input_path
        model_prob=ii+'Prob_model_conversion'
        capture_public_prob=bet_type.lower()+'_prob_25s'
        capture_public_odd=bet_type.lower()+'_odd_25s'
        capture_public_pooltotal='pool_25s'
        final_pooltotal='pool_final'
        track_tick=0.75
        win_comb='tri'

    
    mul=3 if (bet_type=='Qpl')|(bet_type=='Pla') else 1
    
    for year in year_list:#,2016,2017]:
        filename=os.path.join(input_path,str(runno_list[year_list.index(year)]),'temp',bet_type+'_HK_data_'+str(year)+'.csv') 
        raw = pd.read_csv(filename)
        list(raw.columns.values)

        mean_prob=raw['mean_prob'].values[0]
        
        filename=os.path.join(input_path,str(runno_list[year_list.index(year)]),'temp','tier2_factor_'+str(year)+'.csv')
        tier2_factor=factorDB.loc[factorDB['year_cohord']==year,['Date','RaceNo','HorseNo']+factor_use].copy()
        tier2_factor.to_csv(filename)

#        #read sum of square and loss
#        sse_path=os.path.join(input_path,str(runno_list[year_list.index(year)]),'temp','sse_loss_summary_'+'all'+'_'+str(runno_list[year_list.index(year)])+'.xlsx')     
#        sse=pd.read_excel(sse_path,engine='openpyxl')
#        sse_use=sse.loc[sse['year']==year,:].reset_index(drop=True)
#        
#        sse_train_win=sse_use['sse_train_win'][0]
#        sse_test_win=sse_use['sse_test_win'][0]
#        sse_train_notwin=sse_use['sse_train_notwin'][0]
#        sse_test_notwin=sse_use['sse_test_notwin'][0]

       
        for min_adv in np.arange(model_adv_start,model_adv_end,model_adv_increment):
            for model_coef in np.arange(model_coef_start,model_coef_end,model_coef_increment):
                coef_min_adv=min_adv
                ppub_coef = 1.0-model_coef#1.05-model_coef
                for p_adj_coef in np.arange(p_adj_coef_start,p_adj_coef_end,p_adj_coef_increment):
                    trif = raw.copy()
                    bet=['bet_minprobscale20']
                    
                    if use_new_betting_strategy==False:
                        #trif.dtypes
                        #bet=['bet_stan']
                        
                        if new_horse_handle==True:
                            #trif_check=trif.head(10000)
                            trif['TotalNewH']=trif['h1_newH']+trif['h2_newH']
                            trif[model_prob]=trif[model_prob]*((1-trif['TotalNewH'])/2)+trif[capture_public_prob]*(trif['TotalNewH']/2)
                            
                        trif['trimix'] = trif.groupby(['Date_RaceNo'], group_keys=False).apply(lambda x:mix_prob2(x[model_prob], x[capture_public_prob],coef_trimodel=model_coef, coef_ppub=ppub_coef))
                        trif['advantage'] = trif['trimix'] * trif[capture_public_odd]
                        #rho = 0.9;payout_ratio=0.75
    
                        if (bet_type=='Qin')|(bet_type=='Qpl'):                     
                            trif=trif.sort_values(['Date','RaceNo','horse_no_1','horse_no_2'],ascending=[False,True,True,True])
                        if (bet_type=='Win')|(bet_type=='Pla'):
                            trif=trif.sort_values(by=['Date_RaceNo','HorseNo'],ascending=[False,True])
                        
                        
                        
                        #this part use simple kelly
                        trif['gu_prob'] = gu_kelly(trif['trimix'], trif[capture_public_odd],rho = 1.0,min_adv=coef_min_adv,payout_ratio=track_tick)
                        
                        trif[['gu_amount']] = trif[[ 'gu_prob']].applymap(lambda x:x*capital*portion)
                        trif[['gu_amount_old']]=trif[['gu_amount']].copy()
                        #squeeze amount
                        #xx=trif.loc[(trif['Date']=='2016-07-01')&(trif['RaceNo']==10),:]
    
                        #squeeze_limit=100;amount_limit=300
                        def squeeze_amount(xx,squeeze_limit=500,amount_limit=1000):
                            temp=np.around(xx['gu_amount'].values) #round up to integer
                            temp_min=temp.min() if len(temp)!=0 else 0
                            temp_max=temp.max() if len(temp)!=0 else 0
                            #x=1409
                            if temp_max>squeeze_limit:
                                if temp_min==temp_max:
                                    temp_function= lambda x:10 if x!=0 else x
                                else:
                                    temp_function= lambda x:squeeze_limit*(x-temp_min)/(temp_max-temp_min)
                                cfun=np.vectorize(temp_function)
                                xx['gu_amount']=cfun(temp)
    
                            total_bet_amount=xx['gu_amount'].sum()
                            if total_bet_amount>=amount_limit:
                                divisor=total_bet_amount//amount_limit+1
                                xx['gu_amount']=xx['gu_amount']/divisor
                            return xx
    
                        #xx=trif.loc[(trif['Date']=='2016-07-01')&(trif['RaceNo']==10),:]
                        #squeeze_limit=100;amount_limit=300
                        def squeeze_amount_range(xx,squeeze_limit_lower=10000,squeeze_limit_upper=50000):
                            temp=np.around(xx['gu_amount'].values) #round up to integer
                            temp_min=np.min(temp[np.nonzero(temp)]) if len(temp[np.nonzero(temp)])!=0 else 0
                            temp_max=np.max(temp[np.nonzero(temp)]) if len(temp[np.nonzero(temp)])!=0 else 0
                            #x=1409
                            if temp_min==temp_max:
                                temp_function= lambda x:(squeeze_limit_lower+squeeze_limit_upper)/2 if x!=0 else x
                            else:
                                temp_function= lambda x:squeeze_limit_lower+((x-temp_min)/(temp_max-temp_min))*(squeeze_limit_upper-squeeze_limit_lower) if x!=0 else 0
                            cfun=np.vectorize(temp_function)
                            xx['gu_amount']=cfun(temp)
                            return xx
                        
                        #squeeze limit is to squeeze all bet amount less thana value
                        #amount_limit is to confine total bet amount per race
                        if use_rebate==False:
                            trif=trif.groupby(['Date_RaceNo']).apply(lambda x:squeeze_amount(x.reset_index(drop=True),squeeze_limit=1000,amount_limit=10000))  #1000,10000#500 50000
                        else:
                            trif=trif.groupby(['Date_RaceNo']).apply(lambda x:squeeze_amount_range(x.reset_index(drop=True),squeeze_limit_lower=10000,squeeze_limit_upper=30000))  #1000,10000#500 50000
                        #500*(1.51-0.177)/(1327.16-0.177)
                        trif=trif.reset_index(drop=True)
                        trif[['gu_amount']]=multiplier*trif[['gu_amount']]
                        
                        #cap bet amount by having advantage, advantage set must be >=1 othewise cap at negative, so no bet
                        if use_advantage_cap==True:
                            if (bet_type=='Win')|(bet_type=='Qin'):
                                trif['capture_pool_individual']=trif[capture_public_prob]*trif[capture_public_pooltotal]
                                trif['bet_amount_adv_cap']=(trif['trimix']*track_tick*trif[capture_public_pooltotal]-trif['capture_pool_individual']*coef_min_adv)/(coef_min_adv-trif['trimix']*track_tick)
                            
                            if (bet_type=='Pla')|(bet_type=='Qpl'):
                                trif['capture_pool_individual']=trif[capture_public_prob]*trif[capture_public_pooltotal]/mul
                                trif['bet_amount_adv_cap']=(trif['trimix']*track_tick*trif[capture_public_pooltotal]-trif['capture_pool_individual']*coef_min_adv*mul)/(coef_min_adv*mul-trif['trimix']*track_tick)
                                                            
                            trif['bet_amount_adv_cap']=trif['bet_amount_adv_cap'].apply(lambda x:max(0,x))
                            trif['bet_capped']=trif['gu_amount']>trif['bet_amount_adv_cap']
                            trif[['gu_amount']] =trif[['bet_amount_adv_cap','gu_amount']].min(axis=1)
                        
                        trif['gu_amount_before_round']=trif['gu_amount'].copy()
                        #trif['gu_amount']=trif['gu_amount'].apply(lambda x: int(math.ceil(x/10.0))*10.0)
                        trif['gu_amount']=trif['gu_amount'].apply(lambda x: math.ceil(x/10.0)*10.0)
                        
                        trif['bet_minprobscale20'] = trif.groupby('Date_RaceNo', group_keys=False).apply(lambda x:pick_bet_minprob(x,min_prob_scale=0.0,max_prob=100.0,min_adv=coef_min_adv)) #0.015                
                        #trif['bet_minprobscale20'] = trif.groupby('Date_RaceNo', group_keys=False).apply(lambda x:pick_bet_minprob(x,min_prob_scale=0.0,max_prob=100.0,min_adv=-100))
                        trif_check=trif.head(1000)
                        
                        #add lambda parameter to out_BenterNote_selected
                        lambda_read= pd.read_csv(os.path.join(input_path,str(runno_list[year_list.index(year)]),'temp','lambdas'+'_'+bet_type+'_'+str(year)+'.txt'), header=None)
                        lambda1=lambda_read[0].values[0]
                        lambda2=lambda_read[1].values[0]
                        lambda3=lambda_read[2].values[0]
                        lambda4=lambda_read[3].values[0]   

                        
                        
                    else:
                        trif['trimix']=0.5
                        trif['gu_prob']=0.5
                        trif['gu_amount']=0.5
                        trif['bet_minprobscale20']=0.5
                        trif['advantage']=0.5                  
                    
                        lambda1=0.5
                        lambda2=0.5
                        lambda3=0.5
                        lambda4=0.5
                    
 
                    
                    outdf,vars()['df_grouped_pnl'+'_'+ii] = summary(trif, bet,vars()['df_grouped_pnl'+'_'+ii])#,name=str(expert)+''+'%.1f'%coef_min_adv)
                    outdf.columns = [str(col) +'_'+ expert if col!='Strategy' else str(col) for col in outdf.columns ]
    
                    # print outsub
                    outdf['bet_type']=bet_type
                    outdf['year'] = year
                    outdf['model_coef'] = model_coef
                    outdf['coef_min_adv']=coef_min_adv
                    outdf['p_adj_coef']=p_adj_coef
                    
                    

                    
                    outdf['lambda1']=lambda1
                    outdf['lambda2']=lambda2
                    outdf['lambda3']=lambda3
                    outdf['lambda4']=lambda4
                    
#                    outdf['sse_train_win']=sse_train_win
#                    outdf['sse_test_win']=sse_test_win
#                    outdf['sse_train_notwin']=sse_train_notwin
#                    outdf['sse_test_notwin']=sse_test_notwin

                    
                    
                    outsub = outsub.append(outdf)
                    print (outsub[['Strategy','bet_type','year','model_coef','coef_min_adv','p_adj_coef','TotalNo.ofRaceBetted_BenterNote','Profit_BenterNote','ROI_BenterNote','HitRate_BenterNote']])


                    






col_list=list(outsub.columns.values)
col_list_BenterNote=['Strategy','bet_type','year','model_coef','coef_min_adv','p_adj_coef',
                     'lambda1','lambda2','lambda3','lambda4']+[x for x in col_list if 'BenterNote' in x]
out_BenterNote=outsub[col_list_BenterNote]

out_BenterNote = out_BenterNote.reindex(columns= col_list_BenterNote)




#x=out_Gu.loc[(out_Don['Strategy']=='gu_bet_minprobscale20'),:]
#who='Gu'
def find_profitable(x,who):
    output=pd.DataFrame()
    if 'Profit_'+who in list(x.columns.values):
        total_row=x.shape[0]
        x_selected=x.loc[x['Profit_'+who]>0,:].shape[0]
        if total_row==x_selected:
            output=x
    return output




out_BenterNote_selected=out_BenterNote.groupby(['Strategy','model_coef','coef_min_adv','p_adj_coef'],sort=False).apply(lambda x:find_profitable(x.reset_index(),'BenterNote'))
out_BenterNote_selected=out_BenterNote_selected[col_list_BenterNote] if out_BenterNote_selected.shape[0]!=0 else pd.DataFrame([])







from pandas import ExcelWriter
from pandas import read_excel

#output_file_name='Result_model_original_modelcoef_'+str(model_coef_start)+'to'+str(model_coef_end)+'_increment_'+str(model_coef_increment)+'_min_adv_'+str(model_adv_start)+'to'+str(model_adv_end)+'_increment_'+str(model_adv_increment)+"_"+time_now+'_v3_plot.xlsx'
output_file_name='Result'+"_"+"RunNo_"+str(runno_list[0])+'_'+time_now+'_v3_plot.xlsx'


output_path2=os.path.join(output_path,output_file_name)
writer = ExcelWriter(output_path2)

out_BenterNote.to_excel(writer,'All',index=False)
out_BenterNote_selected.to_excel(writer,'BenterNote_selected',index=False)

writer.save()
















df_grouped_pnl_all=pd.DataFrame([])

#all_bet_type=['Win','Pla','Qin','Qpl']
#all_bet_type=['Qin','Qpl','Tri']
#all_bet_type=['Win','Pla','Qin','Qpl']

bet_type='Win'
for bet_type in all_bet_type:
    if df_grouped_pnl_all.shape[0]==0:
        df_grouped_pnl_all=vars()['df_grouped_pnl'+'_'+bet_type].copy()

    else:
        df_grouped_pnl_all=pd.merge(df_grouped_pnl_all,vars()['df_grouped_pnl'+'_'+bet_type],how='outer',on=['Date'])

df_grouped_pnl_all=df_grouped_pnl_all.fillna(0)



pnl_all=df_grouped_pnl_all[['Date','betting_amount_'+all_bet_type[0],'pnl_'+all_bet_type[0]]].copy()
pnl_all=pnl_all.rename(columns={'betting_amount_'+all_bet_type[0]:'betting_amount_all','pnl_'+all_bet_type[0]:'pnl_all'})
for bet_type in all_bet_type[1:]:
    pnl_all['pnl_all']=pnl_all['pnl_all']+df_grouped_pnl_all['pnl_'+bet_type]
    pnl_all['betting_amount_all']=pnl_all['betting_amount_all']+df_grouped_pnl_all['betting_amount_'+bet_type]    


df_grouped_pnl_all.sum(axis=0)
pnl_all.sum(axis=0)




from sqlalchemy import create_engine
import pymysql
import pandas.io.sql as sql
#read data
connection=create_engine("mysql+pymysql://root:pp222#@localhost:3306/horseracing")
query='select Date, year_cohord from main_data'
main_data= sql.read_sql(query,connection)
main_data=main_data.drop_duplicates()

pnl_all=pd.merge(pnl_all,main_data,how='left',on=['Date'])




d0=pnl_all.copy()


#sharpe ratio and accuracy
xx=d0.loc[d0['year_cohord']==2015,:].copy()
pnl_col_name='pnl_all'
def sharpe_ratio(xx,pnl_col_name='pnl_all'):

    
    #Cum_pnl
    cum_pnl=sum(xx[pnl_col_name])

    #max cum loss
    xx['cum_pnl']=xx[pnl_col_name].cumsum()
    max_cum_loss=min(xx['cum_pnl'].values)
    
    #drawdown
    mdd=max(np.maximum.accumulate(xx['cum_pnl'])-xx['cum_pnl'])

    capital=mdd+xx['betting_amount_all'].max()
    risk_free_rate=0

    xx['margin']=capital
    xx['return']=xx[pnl_col_name]/xx['margin']
    risk_free_rate=0

    year_output=xx['year_cohord'].values[0]
    SR=((xx['return'].mean()-risk_free_rate)/xx['return'].std())*np.sqrt(252)
    
    max_bet=xx['betting_amount_all'].max()
    roi1=cum_pnl/(mdd+max_bet)
    roi2=cum_pnl/(abs(max_cum_loss)+max_bet)
    
    return pd.Series([year_output,SR,max_bet,max_cum_loss,mdd,cum_pnl,roi1,roi2])

#PnL_after_commission    PnL_after_commission_adjusted_6min_mean
d0_SR=d0.groupby(["year_cohord"]).apply(lambda x:sharpe_ratio(x.reset_index(drop=True),'pnl_all'))
d0_SR=d0_SR.rename(columns={0:'year',1:'SR',2:'max_bet',3:'max_cum_loss',4:'mdd',5:'cum_pnl',6:'return_mdd',7:'return_mcl'})

d0_SR.loc['Mean',:]= d0_SR.mean(axis=0)

d0_SR.to_csv(os.path.join(output_path,"yearly_grouped_stat_"+time_now+".csv"))












def change_permissions_recursive(temp_folder1, mode):
    for root, dirs, files in os.walk(temp_folder1, topdown=False):
        for dir in [os.path.join(root,d) for d in dirs]:
            os.chmod(dir, mode)
        for file in [os.path.join(root, f) for f in files]:
            os.chmod(file, mode)
change_permissions_recursive(output_path, 0o777) 











topic_track model prob multiprocess



#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 27 23:24:45 2019

@author: root
"""

from subprocess import Popen, PIPE
import glob
import os
import numpy as np
from pandas import HDFStore,DataFrame
import pandas as pd
import sys

from itertools import compress
from pandas import ExcelWriter
from pandas import read_excel

import ast


#/root/anaconda3/bin/python /home/simon/Dropbox/notebooks/horse/model_prob_multiprocess.py '2645' 'yes' 2>&1 &

#cd /home/simon/Dropbox/notebooks/horse/model && /root/anaconda3/bin/python /home/simon/Dropbox/notebooks/horse/model_prob_multiprocess.py '2382' 'no' > temp.log 2>&1 &


#four parameters are runnumber,  use_R_model,use_mixing,use_auto_factor_set
#cd /home/simon/Dropbox/notebooks/horse/model && /root/anaconda3/bin/python /home/simon/Dropbox/notebooks/horse/model_prob_multiprocess.py '3133' 'bayneural' 'True' 'True'> temp.log 2>&1 &






#year_used_list=['2015','2016','2017']#'2015'
year_used_list=['2015','2016','2017','2018','2019','2020','2021']
#year_used_list=['2019','2020']
#year_used_list=['2019','2020']
#year_used_list=['2019','2020','2021']
#year_used_list=['2015']


start_runno=5000#int(sys.argv[1])#2258
use_R_model='bayneural'#'bayneural' # no is to use python bayesian neural network, yes is to use mcmc in R, new is to use damien method in python 
#key parameter
use_mixing=False
use_lambda=False
use_auto_factor_set='False'
gen_pnl=True
use_model=False



#start_runno=int(sys.argv[1])#2258
#use_R_model=sys.argv[2]
#use_mixing=ast.literal_eval(sys.argv[3])
#use_auto_factor_set=sys.argv[4]  #for this true false, need to be string only
















runno_list=[str(x) for x in range(start_runno,start_runno+len(year_used_list))]#"1955"






sse_loss_summary=pd.DataFrame([])




#year_used='2011'
for year_used in year_used_list:
    
    if use_model==True:
        run_no=str(runno_list[year_used_list.index(year_used)])
        if use_R_model=='bayneural':
            input_value = np.arange(1,9).tolist() #5 (5 is stable but need long time 40mins)
        else:
            input_value = np.arange(1,2).tolist()
            
        sample_used=5000#20000#20000#10000#3000#10000#00
        #input_value = np.arange(1,4).tolist() #4
        #sample_used=10000#00
        
        #script_file=os.path.join("/home/simon/Dropbox/notebooks/horse/model_edward_subprocess.py")
        script_file=os.path.join("/home/simon/Dropbox/notebooks/horse/model_edward_subprocess_v2_tf2.py")
        #script_file=os.path.join("/home/simon/Dropbox/notebooks/horse/model_edward_subprocess_v2_tf2_NO_BIAS.py")
        
        temp_folder1=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',str(run_no))
        if not os.path.exists(temp_folder1):
            os.makedirs(temp_folder1)
        
        temp_folder2=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',str(run_no),'temp')
        if not os.path.exists(temp_folder2):
            os.makedirs(temp_folder2)
        
        
        #read output
        input_path=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',run_no,'temp')
        
        
        #make copy of hdf5 file
        fn = '/home/simon/Dropbox/notebooks/horse/factor'
    
        for i in input_value:
            
            cp_target=os.path.join(fn,'factorDB.hdf5')
            cp_dest=os.path.join(input_path,'factorDB'+str(i)+'.hdf5')
            if os.path.exists(cp_dest):
                command="rm -rf "+cp_dest
                os.system(command)
            command="cp " +cp_target+" "+cp_dest
            os.system(command)
        
        os.chdir(temp_folder1)
        
        
        import threading
        import subprocess
        #i=1
        cmds_list = ['/root/anaconda3/bin/python'+' '+script_file+' '+run_no+' '+str(i)+' '+str(sample_used)+' '+year_used+' '+use_R_model+' ' +use_auto_factor_set for i in input_value]
        #cmd=cmds_list[0]
        def run(cmd):
            name = cmd.split()[2]+'_'+cmd.split()[3]
            out = open("%s_log.txt" % name, 'w')
            err = open("%s_error_log.txt" % name, 'w')#open('/dev/null', 'w')
            p = subprocess.Popen(cmd.split(), stdout=out, stderr=err)
            p.wewet()
            print (name + " completed, return code: " + str(p.returncode))
        
        
        
        proc = [threading.Thread(target=run,args=(cmd,)) for cmd in cmds_list]
        [p.start() for p in proc]  ## 
        # join 
        [p.join() for p in proc]  ###  t 
        
        
        print ("Done!")
        
        
        #make folder writeable
        import os
        
        #def change_permissions_recursive(temp_folder1, mode):
        #    for root, dirs, files in os.walk(temp_folder1, topdown=False):
        #        for dir in [os.path.join(root,d) for d in dirs]:
        #            os.chmod(dir, mode)
        #        for file in [os.path.join(root, f) for f in files]:
        #            os.chmod(file, mode)
        #            
        #change_permissions_recursive(temp_folder1, 0o777)
        os.chmod(temp_folder1, 0o777)
        os.chmod(temp_folder2, 0o777)
        
        
    
         
        
        
        
        #read mean prob
        if use_R_model=='bayneural':
            mean_prob=0
            i=1
            for i in input_value:
                mean_prob_temp=pd.read_csv(os.path.join(input_path,"mean_prob_"+str(i)+".csv"))
                mean_prob=mean_prob+mean_prob_temp.mean_prob.values[0]
            mean_prob=mean_prob/len(input_value)
        else:
            mean_prob=0.5
         
        
        #read model output for both traing and test
        #i=1
        for i in input_value:
            input_hdf5_train_path=os.path.join(input_path,"train"+str(i)+".hdf5")        
            store=pd.HDFStore(input_hdf5_train_path)
            name_temp='train'+str(i)+'_dataframe'
            train_temp=store.select(name_temp)
            store.close()
            
            if (i==1):
                train_predict=np.array([]).reshape(train_temp.shape[0],0)
        
            train_predict=np.hstack((train_predict,train_temp))
        
        
            input_hdf5_test_path=os.path.join(input_path,"test"+str(i)+".hdf5")     
            store=pd.HDFStore(input_hdf5_test_path)
            name_temp='test'+str(i)+'_dataframe'
            test_temp=store.select(name_temp)
            store.close()
            
            if (i==1):
                test_predict=np.array([]).reshape(test_temp.shape[0],0)
        
            test_predict=np.hstack((test_predict,test_temp))
            
            print("finished ",str(i))
         
         
        #find mean of prediction
        train_predict2=np.mean(train_predict,axis=1)
        test_predict2=np.mean(test_predict,axis=1)
        
        
        
        
        
        #read testing data (e.g. horse name, win_indicate, public prob etc)
        testdata_path=os.path.join(input_path,"testingdata1"+".hdf5")     
        store=pd.HDFStore(testdata_path)
        name_temp="testingdata1"+'_dataframe'
        testing_data=store.select(name_temp)
        store.close()
        testing_data=testing_data.reset_index(drop=True)
        
        
        
        
        
        #read training data (e.g. horse name, win_indicate, public prob etc)
        traindata_path=os.path.join(input_path,"trainingdata1"+".hdf5")     
        store=pd.HDFStore(traindata_path)
        name_temp="trainingdata1"+'_dataframe'
        training_data=store.select(name_temp)
        store.close()
        training_data=training_data.reset_index(drop=True)
        
        #testing_data['HorseNo']=testing_data['HorseNo'].astype(float)
        
        
        
        data_winpla=testing_data.copy()
        data_train_winpla=training_data.copy()
        
        
        #concat together
        temp=pd.DataFrame(test_predict2)
        data_winpla=pd.concat([data_winpla,temp],axis=1)
        data_winpla=data_winpla.rename(columns={0:'win_prob_original'})
        
        temp=pd.DataFrame(train_predict2)
        data_train_winpla=pd.concat([data_train_winpla,temp],axis=1)
        data_train_winpla=data_train_winpla.rename(columns={0:'win_prob_original'})
        
        #check sum of square
    #    np.sum(np.square(data_train_winpla['win_prob_25s_mix_final'].values-data_train_winpla['fitted_all'].values))
    #    np.sum(np.square(data_train_winpla['win_prob_25s_mix_final'].values-data_train_winpla['win_prob_original'].values))    
        
        
        #prediction may be negative, so round to 0.0000001
        sum(data_winpla['win_prob_original']<=0)
        sum(data_train_winpla['win_prob_original']<=0)
        data_winpla.loc[data_winpla['win_prob_original']<=0,'win_prob_original']=0.0000001
        data_train_winpla.loc[data_train_winpla['win_prob_original']<=0,'win_prob_original']=0.0000001
        
        
        #normalize probability
        data_winpla['win_prob_original_normalized']=data_winpla.groupby(['Date_RaceNo'])['win_prob_original'].apply(lambda x:x/np.sum(x))
        data_train_winpla['win_prob_original_normalized']=data_train_winpla.groupby(['Date_RaceNo'])['win_prob_original'].apply(lambda x:x/np.sum(x))
    
    
        
    
    
        #find sum of error in training and naive win pnl
        data_train_winpla_temp=data_train_winpla.copy()
        data_train_winpla_temp_win_only=data_train_winpla_temp.loc[data_train_winpla_temp['win_indicator']==1,:].copy()
        data_train_winpla_temp_notwin=data_train_winpla_temp.loc[data_train_winpla_temp['win_indicator']!=1,:].copy()
        n_train_win=data_train_winpla_temp_win_only.shape[0]
        n_train_notwin=data_train_winpla_temp_notwin.shape[0]
        sse_train_win=sum(1-data_train_winpla_temp_win_only['win_prob_original_normalized'].values)/n_train_win
        sse_train_notwin=sum(data_train_winpla_temp_notwin['win_prob_original_normalized'].values)/n_train_notwin
        
        data_train_winpla_temp['advantage']=data_train_winpla_temp['win_prob_original_normalized']*data_train_winpla_temp['win_odd_25s_mix_final']
        data_train_winpla_temp.loc[(data_train_winpla_temp['win_indicator']==1)&(data_train_winpla_temp['advantage']>1),'div_if_bet_and_win']=data_train_winpla_temp['win_odd_25s_mix_final']/10
        data_train_winpla_temp['div_if_bet_and_win']=data_train_winpla_temp['div_if_bet_and_win'].fillna(0)
        naive_pnl_train=sum(data_train_winpla_temp['div_if_bet_and_win'].values)-sum(data_train_winpla_temp['advantage']>1)
        
        #find sum of error in training and naive win pnl
        data_winpla_temp=data_winpla.copy()
        data_winpla_temp_win_only=data_winpla_temp.loc[data_winpla_temp['win_indicator']==1,:].copy()
        data_winpla_temp_notwin=data_winpla_temp.loc[data_winpla_temp['win_indicator']!=1,:].copy()
        n_test_win=data_winpla_temp_win_only.shape[0]
        n_test_notwin=data_winpla_temp_notwin.shape[0]
        sse_test_win=sum(1-data_winpla_temp_win_only['win_prob_original_normalized'].values)/n_test_win
        sse_test_notwin=sum(data_winpla_temp_notwin['win_prob_original_normalized'].values)/n_test_notwin
    
        data_winpla_temp['advantage']=data_winpla_temp['win_prob_original_normalized']*data_winpla_temp['win_odd_25s_mix_final']
        data_winpla_temp.loc[(data_winpla_temp['win_indicator']==1)&(data_winpla_temp['advantage']>1),'div_if_bet_and_win']=data_winpla_temp['win_odd_25s_mix_final']/10
        data_winpla_temp['div_if_bet_and_win']=data_winpla_temp['div_if_bet_and_win'].fillna(0)
        naive_pnl_test=sum(data_winpla_temp['div_if_bet_and_win'].values)-sum(data_winpla_temp['advantage']>1)
        
        testing_data['sse_train_win']=sse_train_win
        testing_data['sse_test_win']=sse_test_win
        testing_data['sse_train_notwin']=sse_train_notwin
        testing_data['sse_test_notwin']=sse_test_notwin
    
        testing_data['naive_pnl_train']=naive_pnl_train
        testing_data['naive_pnl_test']=naive_pnl_test
        
        
        #find R square in training
        data_train_winpla_temp=data_train_winpla.loc[data_train_winpla['win_indicator']==1,:].copy()
        data_train_winpla_temp['log_win_model']=data_train_winpla_temp['win_prob_original_normalized'].apply(lambda x: np.log(x))
        data_train_winpla_temp['log_win_public']=data_train_winpla_temp['win_prob_25s_mix_final'].apply(lambda x: np.log(x))
        data_train_winpla_temp['log_sts']=data_train_winpla_temp['sts'].apply(lambda x: np.log(1/x))
        training_r_square_win_model=1-sum(data_train_winpla_temp['log_win_model'])/sum(data_train_winpla_temp['log_sts'])
        training_r_square_win_public=1-sum(data_train_winpla_temp['log_win_public'])/sum(data_train_winpla_temp['log_sts'])
    
        #find R square in testing
        data_winpla_temp=data_winpla.loc[data_winpla['win_indicator']==1,:].copy()
        data_winpla_temp['log_win_model']=data_winpla_temp['win_prob_original_normalized'].apply(lambda x: np.log(x))
        data_winpla_temp['log_win_public']=data_winpla_temp['win_prob_25s_mix_final'].apply(lambda x: np.log(x))
        data_winpla_temp['log_sts']=data_winpla_temp['sts'].apply(lambda x: np.log(1/x))
        testing_r_square_win_model=1-sum(data_winpla_temp['log_win_model'])/sum(data_winpla_temp['log_sts'])
        testing_r_square_win_public=1-sum(data_winpla_temp['log_win_public'])/sum(data_winpla_temp['log_sts'])    
        
        #store sse and loss for both training and testing data
        sse_loss_summary_temp=pd.DataFrame({'year':[year_used],
                                            'sse_train_win':[sse_train_win],
                                            'sse_test_win':[sse_test_win],
                                            'sse_train_notwin':[sse_train_notwin],
                                            'sse_test_notwin':[sse_test_notwin],
                                            'naive_pnl_train':[naive_pnl_train],
                                            'training_r_square_win_model':[training_r_square_win_model],
                                            'training_r_square_win_public':[training_r_square_win_public],
                                            'testing_r_square_win_model':[testing_r_square_win_model],
                                            'testing_r_square_win_public':[testing_r_square_win_public]})
        sse_loss_summary=sse_loss_summary.append(sse_loss_summary_temp)
        
        
        
        
        
        
        
        
        #look at testing data
        data_winpla.groupby(['Date_RaceNo'])['win_prob_original'].sum().max()
        data_winpla.groupby(['Date_RaceNo'])['win_prob_original'].sum().min()
        
        import matplotlib.pyplot as plt
        plt.hist(data_winpla.groupby(['Date_RaceNo'])['win_prob_original'].sum(),density=False,bins=30,range=(0.5,1.2))  #ravel=flatten
        
        
        
        
        
        
        
        
        
        
        data_winpla['mean_prob']=mean_prob
        #below two files is only for R to use
        #output
        data_winpla.to_csv(os.path.join(input_path,"WinPlaProb.csv"),index=False)
        list(data_winpla.columns.values)
        
        data_train_winpla.to_csv(os.path.join(input_path,"WinPlaProb_training.csv"),index=False)
        
        #a=sum(data_train_winpla.win_probs_final)
        
        
        
        
        
        if use_mixing==True:
            #generate mixing coef
            bet_type='Win'
            for bet_type in ['Win']:
                #find conversion parameter and mixing coef
                import subprocess
                #r_path="/root/anaconda3/envs/new_1/lib/R/bin/Rscript"
                r_path="/root/anaconda3/envs/myRenv3_4/lib/R/bin/Rscript"
                #r_path='/root/anaconda3/pkgs/r-base-3.6.1-h9bb98a2_1/bin/Rscript'
                #r_path='/root/anaconda3/pkgs/r-base-3.6.1-h9bb98a2_1/lib/R/bin/Rscript'
                arg='--vanilla'
                lambda_folder=os.path.join('/home/simon/Dropbox/notebooks/horse')
                path2script=os.path.join(lambda_folder,'coeff_mixing_v2.r')
                
                #for here shell must be false, otherwise can't run R
                subprocess.call([r_path,arg,path2script,str(year_used),bet_type,run_no],shell=False)
            
    
            filename_mixing_coef=os.path.join(input_path,'mixing_coef'+'_'+bet_type+'_'+str(year_used)+'.txt') 
            mixing_coef= pd.read_csv(filename_mixing_coef, header=None)
        
    
            model_coef=mixing_coef[0].values[0]
            ppub_coef=mixing_coef[1].values[0]
        else:
            model_coef=1 #mixing_coef[0].values[0]
            ppub_coef=0 #mixing_coef[1].values[0]
            
    
        #mix prob
        def mix_prob2(model_prob, pub_prob, coef_trimodel=0.5, coef_ppub=0.6):             
            a = (model_prob**coef_trimodel)*(pub_prob**coef_ppub)
            output=a/sum(a)
            return output
        
        temp=data_winpla.groupby(['Date_RaceNo'], group_keys=False).apply(lambda x:mix_prob2(x['win_prob_original_normalized'], x['win_prob_25s_mix_final'],coef_trimodel=model_coef, coef_ppub=ppub_coef))
        data_winpla['win_mixed']=temp
        data_winpla.to_csv(os.path.join(input_path,"WinPlaProb.csv"),index=False)
        
        temp=data_train_winpla.groupby(['Date_RaceNo'], group_keys=False).apply(lambda x:mix_prob2(x['win_prob_original_normalized'], x['win_prob_25s_mix_final'],coef_trimodel=model_coef, coef_ppub=ppub_coef))
        data_train_winpla['win_mixed']=temp
        data_train_winpla.to_csv(os.path.join(input_path,"WinPlaProb_training.csv"),index=False)
        
        
        
        if use_lambda==True:
            #find lambda
            for bet_type in ['Win','Pla','Qin','Qpl']:
                #find lambda
                import subprocess
                #r_path="/root/anaconda3/envs/new_1/lib/R/bin/Rscript"
                r_path="/root/anaconda3/envs/myRenv3_4/lib/R/bin/Rscript"
                #r_path='/root/anaconda3/envs/rstudio/lib/R/bin/Rscript'
                #r_path='/root/anaconda3/pkgs/r-base-3.6.1-h9bb98a2_1/bin/Rscript'
                #r_path='/root/anaconda3/pkgs/r-base-3.6.1-h9bb98a2_1/lib/R/bin/Rscript'
                arg='--vanilla'
                lambda_folder=os.path.join('/home/simon/Dropbox/notebooks/horse')
                path2script=os.path.join(lambda_folder,'coeff_lambda_v2.r')
                
                #for here shell must be false, otherwise can't run R
                subprocess.call([r_path,arg,path2script,str(year_used),bet_type,run_no],shell=False)
                print("finished lambda ",str(year_used),' ',bet_type)
        
        
        
        
        
        
        
        ###win prob evaluation
        
        capture_public_prob='win_prob_25s_mix_final'
        model_prob='win_prob_original_normalized'
        
        win_comb='win_indicator'
        
    #    threshold1=0.5;threshold2=1
    #    base_name='win_prob_original_normalized'
    #    data=data_winpla.copy()
        def cal_per_set(threshold1,threshold2,base_name,data):
            data_select=data.loc[(data[base_name]>=threshold1)&(data[base_name]<=threshold2),:]
            n=data_select.shape[0]
            
            name=base_name+'_range'
            name_value=str(threshold1)+'-'+str(threshold2)
            
            if n>0:
                ave_prob=np.mean(data_select[base_name].values)
                actual_mean=sum(data_select[win_comb])/n
                actual_freq=sum(data_select[win_comb])
                diff=ave_prob-actual_mean   
                                 
                output=pd.DataFrame({name:[name_value],'no.sample':[n],'ave_prob':[ave_prob],'actual_prob':[actual_mean],'actual_freq':[actual_freq],'ave_minor_actual':[diff]})
            else:
                output=pd.DataFrame({name:[None],'no.sample':[None],'ave_prob':[None],'actual_prob':[None],'actual_freq':[None],'ave_minor_actual':[None]})
            
            return output
        
        #data=data_winpla.loc[data_winpla[model_prob]>data_winpla[capture_public_prob],:].copy()
        #data=data_winpla.copy()
        #base_name=model_prob
        #data.shape[0]
        def freq_compare(data,base_name):
            output1=pd.DataFrame([])
            output1=output1.append(cal_per_set(0.0,0.01,base_name,data))
            output1=output1.append(cal_per_set(0.01,0.02,base_name,data))
            output1=output1.append(cal_per_set(0.02,0.04,base_name,data))
            output1=output1.append(cal_per_set(0.04,0.06,base_name,data))
            output1=output1.append(cal_per_set(0.06,0.08,base_name,data))
            output1=output1.append(cal_per_set(0.08,0.1,base_name,data))
            output1=output1.append(cal_per_set(0.1,0.25,base_name,data))
            output1=output1.append(cal_per_set(0.25,0.5,base_name,data))
            output1=output1.append(cal_per_set(0.5,1.0,base_name,data))
            name=base_name+'_range'
            
            d=abs(output1['ave_minor_actual'].values)
            d[np.isnan(d)]=0
            sum_diff=sum(d)
            output1['sum_diff']=sum_diff
            
            output1.loc[pd.isnull(output1['no.sample']),'no.sample']=0
            output1['weighted_sum_diff']=sum(np.multiply(output1['no.sample'].values,d))/sum(output1['no.sample'].values)
            output1=output1.reset_index(drop=True)
            output1.loc['sum',:]=output1.sum(axis=0,skipna=True)
            
            return output1[[name,'no.sample','ave_prob','actual_prob','actual_freq','ave_minor_actual','sum_diff','weighted_sum_diff']].copy()
        
        capture_table=freq_compare(data_winpla,capture_public_prob)
        capture_table
        
        model_table=freq_compare(data_winpla,model_prob)
        model_table
        mixprob_table=freq_compare(data_winpla,'win_mixed')
        mixprob_table
        
        
        #output to xlsx
        
        output_file_name='win_frequency_compare_'+year_used+'_'+run_no+'.xlsx'
        output_path2=os.path.join(input_path,output_file_name)
        writer = ExcelWriter(output_path2)
        model_table.to_excel(writer,'model_table',index=False)
        mixprob_table.to_excel(writer,'mixprob_table',index=False)
        writer.save()
        
        
        
        
        
        model_table1=freq_compare(data_winpla.loc[data_winpla[model_prob]<data_winpla[capture_public_prob],:].copy(),model_prob)
        model_table1
        
        
        model_table2=freq_compare(data_winpla.loc[data_winpla[model_prob]>data_winpla[capture_public_prob],:].copy(),model_prob)
        model_table2   
        
        
        model_table1=freq_compare(data_train_winpla.loc[data_train_winpla[model_prob]<data_train_winpla[capture_public_prob],:].copy(),model_prob)
        model_table1
        
        
        model_table2=freq_compare(data_train_winpla.loc[data_train_winpla[model_prob]>data_train_winpla[capture_public_prob],:].copy(),model_prob)
        #model_table2   
        
        
        
        #remove factorDB to save space
        for i in input_value:
            cp_dest=os.path.join(input_path,'factorDB'+str(i)+'.hdf5')
            if os.path.exists(cp_dest):
                command="rm -rf "+cp_dest
                os.system(command)
    
            cp_dest=os.path.join(input_path,'train_xy_for_cookdist_'+str(i)+'.csv')
            if os.path.exists(cp_dest):
                command="rm -rf "+cp_dest
                os.system(command)
                
        #remove trainingdata and testingdata
        for i in input_value:
            rm_path=os.path.join(input_path,"testingdata"+str(i)+".hdf5")  
            if os.path.exists(rm_path):
                command="rm -rf "+rm_path
                os.system(command)    
        for i in input_value:
            rm_path=os.path.join(input_path,"trainingdata"+str(i)+".hdf5")  
            if os.path.exists(rm_path):
                command="rm -rf "+rm_path
                os.system(command)        

    else:
        run_no=str(runno_list[year_used_list.index(year_used)])
        temp_folder1=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',str(run_no))
        input_path=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',run_no,'temp')
        
        if not os.path.exists(temp_folder1):
            os.makedirs(temp_folder1)
        
        temp_folder2=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',str(run_no),'temp')
        if not os.path.exists(temp_folder2):
            os.makedirs(temp_folder2)

        os.chmod(temp_folder1, 0o777)
        os.chmod(temp_folder2, 0o777)
        
        
        fn=os.path.join('/home/simon/Dropbox/notebooks/horse/factor','factorDB.hdf5')
        store = pd.HDFStore(fn)
        factorDB= store.select('factorDB_dataframe')
        store.close()

        factorDB=factorDB.reset_index(drop=True)
        
        test_year=int(year_used)
        test_start=str(test_year)+'-09-01';test_end=str(test_year+1)+'-08-31'
        
        testing_data=factorDB.loc[(factorDB['Date']>=test_start)&(factorDB['Date']<=test_end)&(factorDB['late_withdrawn_race']!=1)&(factorDB['sts']>=8),:]
        


        testing_data['mean_prob']=0.5
        testing_data['win_probs_final']=0.5
        testing_data['win_prob_original']=0.5
        testing_data['win_prob_original_normalized']=0.5
        testing_data['win_mixed']=0.5        
        

        
        testing_data.to_csv(os.path.join(input_path,"WinPlaProb.csv"),index=False)





    
    
    
    
#    #remove WinPlaProb_training.csv to save space
#    rm_path=os.path.join(input_path,"WinPlaProb_training.csv")
#    command="rm -rf "+rm_path
#    os.system(command)
    
    
    

    
    

if use_model==True:
    #output sse_loss_summary for all year
    #r='3133'
    for r in runno_list:
        file_name='sse_loss_summary_'+'all'+'_'+r+'.xlsx'
        writer = ExcelWriter(os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',r,'temp',file_name))
        sse_loss_summary.to_excel(writer,'sse_loss_summary',index=False)
        writer.save()
    
    
    
    #merge win_frequency_compare together
    for k in range(0,len(year_used_list)):
        read_file_name='win_frequency_compare_'+year_used_list[k]+'_'+runno_list[k]+'.xlsx'
        read_path=os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',runno_list[k],'temp',read_file_name)
        d1 = pd.read_excel(read_path, sheet_name=None,engine='openpyxl')['mixprob_table']
        d1['year']=year_used_list[k]
        if k==0:
            d0=d1
        else:
            d0=pd.concat([d0,d1],axis=0)
    
    
    #rearrange columns
    cols = d0.columns.tolist()
    cols.insert(0, cols.pop(cols.index('year')))
    d0 = d0.reindex(columns= cols)
    
    #output
    file_name='win_frequency_compare_'+'all'+'_'+runno_list[0]+'.xlsx'
    writer = ExcelWriter(os.path.join('/home/simon/Dropbox/notebooks/horse/model/plot',runno_list[0],'temp',file_name))
    d0.to_excel(writer,'all_frequency',index=False)
    writer.save()
    
    
    
    
    
    
    
    #only after and include 2015 having capture odds
    sub_key=[int(k)>=2015 for k in year_used_list]
    
    year_used_list_pnl=list(compress(year_used_list,sub_key))
    runno_list_pnl=list(compress(runno_list,sub_key))

else:
    year_used_list_pnl=year_used_list
    runno_list_pnl=runno_list






if (len(year_used_list_pnl)!=0)&(gen_pnl==True):
    #below is to generate pnl
    print("generate data for pnl and pnl")       
    year_list2=':'.join([str(k) for k in year_used_list_pnl])
    runno_list2=':'.join([str(k) for k in runno_list_pnl])
    
    
    
    import subprocess
    subprocess.call(['/root/anaconda3/bin/python','/home/simon/Dropbox/notebooks/horse/DataPreparation_for_backtest_v2.py',year_list2,runno_list2],shell=False)
            
        










topic_track
extract pre post linux 









#############read me#################
#divident and race result
#this program will compare, if date/race is in racing_date_raceno_track.csv but not in mysql, it will download the records and update mysql
#if already in mysql, this program will not download them. so need to manually remove record in mysql then run this program

#trackwork, sick history, racecard, mset
#this program will compare, if date/race is in racing_date_raceno_track.csv but not in mysql, it will download the records and update mysql
#if there is "pre" indicator in racing_date_raceno_track.csv, below program will remove the records in mysql and download new one.

#issue log
#in 2021-10-13, typhoon 8 so no wednesday night race
#i removed sick historical record by selecting Date_racecard=2021-10-13
#this is an mistake, becasue when each time update sickhistory, it is by horse. so if i remove this day
#mean this database won't have the horses(e.g. horse AB) record in this day.
#in 2021-10-17, when update sick, program will find the horses in racecard and not in sick and download. e.g. horse AB
#but hkjc cannot find historical racecard, so fail to downloiad
#so need to wewet next time these horses pop out in raacecard then download it to sick history



#%reset -f
import os
import numpy as np
#os.chdir(r'C:\Users\simon\Desktop\python\WinPython-64bit-3.6.2.0Qt5\notebooks\index_analysis')
target_dir=r'/home/simon/Dropbox/notebooks/horse'

os.chdir(target_dir)

def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)
    
import re
from pandas import read_excel
#import pandas_datareader as pdr
from datetime import timedelta
from datetime import datetime as dt
import datetime
import pandas as pd
from time import sleep

from sqlalchemy import create_engine
import configparser
import re

import numpy as np
import pymysql
import pandas.io.sql as sql

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time

from bs4 import BeautifulSoup
import sys


import numpy as np
from pandas import HDFStore,DataFrame

from numba import jit, cuda 
import numpy as np 
# to measure exec time 
from timeit import default_timer as timer    



#output stan out
import sys
import time
time_now_save=time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")
stan_out_log=os.path.join('log','stan_out_pre_post_race_data_part_'+time_now_save+'_production'+'.log')
sys.stderr = open(stan_out_log, 'w')


#read races want to download
race_key_source=pd.read_csv("./racing_date_raceno_track.csv")



#below is special mwthod to extract html with javascript enabled
from math import cos, pi, floor
import requests

def parse_challenge(page):
    """
    Parse a challenge given by mmi and mavat's web servers, forcing us to solve
    some math stuff and send the result as a header to actually get the page.
    This logic is pretty much copied from https://github.com/R3dy/jigsaw-rails/blob/master/lib/breakbot.rb
    """
    #page=r.text
    top = page.split('<script>')[1].split('\n')
    challenge = top[1].split(';')[0].split('=')[1]
    challenge_id = top[2].split(';')[0].split('=')[1]
    return {'challenge': challenge, 'challenge_id': challenge_id, 'challenge_result': get_challenge_answer(challenge)}


def get_challenge_answer(challenge):
    """
    Solve the math part of the challenge and get the result
    """
    arr = list(challenge)
    last_digit = int(arr[-1])
    arr.sort()
    min_digit = int(arr[0])
    subvar1 = (2 * int(arr[2])) + int(arr[1])
    subvar2 = str(2 * int(arr[2])) + arr[1]
    power = ((int(arr[0]) * 1) + 2) ** int(arr[1])
    x = (int(challenge) * 3 + subvar1)
    y = cos(pi * subvar1)
    answer = x * y
    answer -= power
    answer += (min_digit - last_digit)
    answer = str(int(floor(answer))) + subvar2
    return answer


from sqlalchemy import create_engine
import configparser
import re
#import MySQLdb as mdb
import pandas as pd
import numpy as np
import pymysql
import pandas.io.sql as sql

connection=create_engine("mysql+pymysql://root:pp222#@localhost:3306/horseracing")

                         
                         
                         
                         
                         
                         
                         
                         

##read dividend, version 1 using selenium
#
##read race key
#import pandas as pd
#import datetime
#from datetime import datetime as dt
#
#from selenium import webdriver
#from selenium.webdriver.common.keys import Keys
#import time
#
#import pandas as pd
#from bs4 import BeautifulSoup
#
##date_key="2003/09/13"; raceno_key=str(1);track_key="ST"
##date_key="2008/12/03"; raceno_key=str(8);track_key='HV'
#def extract_div(date_key,raceno_key):
#    #https://racing.hkjc.com/racing/information/Chinese/Racing/LocalResults.aspx?RaceDate=2019/07/10&RaceNo=9
#    
#    url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate='+date_key+'&RaceNo='+raceno_key
#    
##    options = webdriver.ChromeOptions();
##    options.add_argument('headless');
#    
#    op = webdriver.ChromeOptions()
#    op.add_argument('headless')
#    browser = webdriver.Chrome(options=op,executable_path=r'C:\Users\simon\Desktop\python\chromedriver.exe')
#    
#    
#    #browser = webdriver.Chrome(executable_path=r'C:\Users\simon.chan\.Spyder\chromedriver.exe')
#    browser.get(url)
#    browser.set_window_position(-10000,0)
#    #find the html source code
#    html_string=browser.page_source
#    html_string=source_html
#     
#    browser.quit()
#    
#    soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
#    
#    #table = soup.find_all('table')[0] # Grab the first table
#    #find table html code with class table_bd f_tac f_fs13 f_fl
#    table = soup.find_all('table', {'class':'table_bd f_tac f_fs13 f_fl'})[0] # Grab the first table
#    
#    store=[]
#    i=0
#    for row in table.find_all('tr'):
#        #print("i is ",i)
#        #print(row)
#        columns = row.find_all('td')
#        #print(len(columns))
#        store_temp=''
#        #observe that only 3 columns in dividend table
#        if len(columns)==3:
#            count=0
#            for column in columns:
#                if count==0:
#                    store_temp=column.get_text()
#                    category=column.get_text()
#                else:
#                    store_temp= store_temp+'|'+column.get_text()
#                count=count+1
#    
#        if len(columns)==2:
#            count=0
#            for column in columns:
#                if count==0:
#                    store_temp=category+'|'+column.get_text()
#                else:
#                    store_temp=store_temp+'|'+column.get_text()
#                count=count+1
#        store.append(store_temp)
#        i=i+1
#    #    if i==2:
#    #        break
#    dividend=pd.DataFrame([])
#    def create_df(x):
#        return x.split('|')
#    target_type=['WIN','PLACE','QUINELLA','QUINELLA PLACE','TIERCE','TRIO','FIRST 4','QUARTET']
#    
#    output=[xx.split('|')  for xx in store if any(s==xx.split('|')[0]  for s in target_type)]
#    
#    dividend=pd.DataFrame(output)
#    dividend=dividend.rename(columns={0:'Pool',1:'Comb',2:'dividend'})
#    dividend['Date']=date_key
#    dividend['RaceNo']=raceno_key
#    print(date_key)
#    return dividend
#
#race_key=race_key_source.copy()
#race_key.dtypes
#
#race_key.loc[race_key['track3']=='SH','track3']='ST'
#
#race_key['Date_key']=race_key['Date'].apply(lambda x:dt.strptime(x,'%Y-%m-%d').strftime('%Y/%m/%d'))
#
#
##i=0
#div_table=pd.DataFrame([])
#for i in range(18,race_key.shape[0]):
##for i in range(0,3):
#    row_use=race_key[i:i+1]
#    div_temp=extract_div(date_key=row_use['Date_key'].values[0],
#                         raceno_key=str(row_use['RaceNo'].values[0]))
#    div_table=div_table.append(div_temp)






#selenium get html txt
#link='https://racing.hkjc.com/racing/information/English/Racing/DisplaySectionalTime.aspx?RaceDate=09/11/2019&RaceNo=1'
#link='https://racing.hkjc.com/racing/information/English/Racing/RaceCard.aspx?RaceDate=2008/04/27&Racecourse=ST&RaceNo=7'
       
def get_html_using_selenium(link):
    op = webdriver.ChromeOptions()
    op.add_argument('headless')
    op.add_argument('--no-sandbox')
    op.add_argument('--disable-dev-shm-usage')
    #op.add_argument("--enable-javascript")

    
    browser = webdriver.Chrome(options=op,executable_path='/home/simon/Dropbox/notebooks/horse/chrome_driver/chromedriver_linux64/chromedriver84')
    #browser.execute_script("return document.cookie")  
    browser.execute_script("return navigator.userAgent")  
    
    #browser = webdriver.Chrome(executable_path=r'C:\Users\simon.chan\.Spyder\chromedriver.exe')
    browser.get(link)
    
    #browser.set_window_position(-10000,0)
    #find the html source code
    #html_string=browser.page_source 
    
    #https://stackoverflow.com/questions/22739514/how-to-get-html-with-javascript-rendered-sourcecode-by-using-selenium
    #html_string=browser.execute_script("return document.getElementsByTagName('html')[0].innerHTML")
    sleep(1)  #this is very important, other fail to extract
    html_string=browser.execute_script("return document.body.innerHTML")

    browser.quit()
    return html_string






#need to download geckodriver and copy to /usr/local/bin
from selenium.webdriver.firefox.options import Options

def get_html_using_firefox(link):
    options = Options()
    options.headless = True
    
    driver = webdriver.Firefox(options=options)

    driver.get(link)
    sleep(1)
    html_string= driver.execute_script("return document.getElementsByTagName('html')[0].innerHTML")
    
    #html_string = driver.page_source
    #html_string 
    driver.quit()
    return html_string








##################################################################
##################################################################
##################################################################
##################################################################
#read div and race result using special method of requests with javascript enabled
##################################################################
##################################################################
##################################################################
##################################################################


    
#date_key="2003/08/31"; raceno_key=str(1);track_key='ST'
#date_key="2005/12/17"; raceno_key=str(2);track_key='ST'
#date_key="2008/03/30"; raceno_key=str(1)
#date_key="2008/04/02"; raceno_key=str(1)
#date_key="2013/02/12"; raceno_key=str(1)

#date_key="2004/06/20"; raceno_key=str(6)
#date_key="2019/09/15"; raceno_key=str(1)
#date_key="2019/09/25"; raceno_key=str(1)
#date_key="2019/10/30"; raceno_key=str(7)
#date_key="2019/10/16"; raceno_key=str(8)
#date_key="2021/01/21"; raceno_key=str(2)
#A,B,C,D=extract_div(date_key,raceno_key)    

#def extract_div(date_key,raceno_key):
#    eprint('doing '+date_key+' '+raceno_key)
#    #https://racing.hkjc.com/racing/information/Chinese/Racing/LocalResults.aspx?RaceDate=2019/07/10&RaceNo=9
#    url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate='+date_key+'&RaceNo='+raceno_key
#    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
#    problem_races=pd.DataFrame([])
#    dividend=pd.DataFrame([])
#    problem_races_1=pd.DataFrame([])
#    output1=pd.DataFrame([])
#    html_found=False
#
#    for ii in range(0,100):
#        s = requests.Session()
#        r = s.get(url)
#    
#        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
#        try:
#            source_html=''
#            if 'X-AA-Challenge' in r.text:
#                challenge = parse_challenge(r.text)
#                r = s.get(url, headers={
#                    'X-AA-Challenge': challenge['challenge'],
#                    'X-AA-Challenge-ID': challenge['challenge_id'],
#                    'X-AA-Challenge-Result': challenge['challenge_result']})
#            
#                yum = r.cookies
#                r = s.get(url, cookies=yum)
#                source_html=r.text#r.content
#
#            if not ('table_bd f_tac f_fs13 f_fl' in source_html)&('f_tac table_bd draggable' in source_html):
#                source_html=get_html_using_selenium(url)
#                
#            if ('table_bd f_tac f_fs13 f_fl' in source_html)&('f_tac table_bd draggable' in source_html):
#                html_found=True
#                break
#            sleep(0.2)
#        except:
#            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'Race_result_page','\n')
#            print("Oops!",sys.exc_info()[0],"occured when extracting ",'Race_result_page','\n')           
#    
#    no_of_trial=ii+1
#    no_of_trial
#    eprint('Race_result_page html code ',date_key.replace("/","-"),'_',raceno_key,' tried ',no_of_trial,' times extraction with success','\n')
#    #print('Race_result_page html code',' tried ',no_of_trial,' times extraction with success','\n')
#
#    if html_found==True:
#        html_string=source_html
#        
#        #save source code0
#        file_name='store_html'+'/'+"result_page_source_code_"+date_key.replace("/","-")+'_'+raceno_key+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
#        #sometime wb mode not working, so use w
#        try:
#            file = open(file_name,"wb") #open file in binary mode
#            file.write(html_string)
#            file.close()
#        except:
#            file = open(file_name,"w") #open file in binary mode
#            file.write(html_string)
#            file.close()  
#        
#
#        #open(r'C:\Users\simon.chan\Desktop\setting\try.txt', 'wb').write(html_string)
#        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
#    
#        #table = soup.find_all('table')[0] # Grab the first table
#        #find table html code with class table_bd f_tac f_fs13 f_fl
#
#        try:
#            table = soup.find_all('table', {'class':'table_bd f_tac f_fs13 f_fl'})[0] # Grab the first table     
#            table_found=True
#            eprint('dividend table is found',date_key.replace("/","-"),'_',raceno_key,' tried ',no_of_trial,' times extraction with success','\n')
#            print('dividend table is found',date_key.replace("/","-"),'_',raceno_key,' tried ',no_of_trial,' times extraction with success','\n')
#        except:
#            problem_races=problem_races.append(pd.Series([date_key,raceno_key]),ignore_index=True)
#            table_found=False
#            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'table_bd f_tac f_fs13 f_fl','\n')
#            print("Oops!",sys.exc_info()[0],"occured when extracting ",'table_bd f_tac f_fs13 f_fl','\n')
#
#
#                
#        if table_found==True:
#            store=[]
#            i=0
#            for row in table.find_all('tr'):
#                #print("i is ",i)
#                #print(row)
#                columns = row.find_all('td')
#                #print(len(columns))
#                store_temp=''
#                #observe that only 3 columns in dividend table
#                if len(columns)==3:
#                    count=0
#                    for column in columns:
#                        if count==0:
#                            store_temp=column.get_text()
#                            category=column.get_text()
#                        else:
#                            store_temp= store_temp+'|'+column.get_text()
#                        count=count+1
#            
#                if len(columns)==2:
#                    count=0
#                    for column in columns:
#                        if count==0:
#                            store_temp=category+'|'+column.get_text()
#                        else:
#                            store_temp=store_temp+'|'+column.get_text()
#                        count=count+1
#                store.append(store_temp)
#                i=i+1
#            #    if i==2:
#            #        break
#            
#            def create_df(x):
#                return x.split('|')
##            target_type=['WIN','PLACE','QUINELLA','QUINELLA PLACE','FORECAST','TIERCE','TRIO','FIRST 4','QUARTET']
##            
##            output=[xx.split('|')  for xx in store if any(s==xx.split('|')[0]  for s in target_type)]
#            #target_type=['WIN','PLACE','QUINELLA','QUINELLA PLACE','FORECAST','TIERCE','TRIO','FIRST 4','QUARTET','TRIPLE TRIO','TRIPLE TRIO(Consolation)','SIX UP']
#            
#            output=[xx.split('|')  for xx in store]
#            
#            dividend=pd.DataFrame(output)
#            dividend=dividend.rename(columns={0:'Pool',1:'Comb',2:'dividend'})
#            dividend['Date']=date_key
#            dividend['RaceNo']=raceno_key
#        else:
#            eprint('dividend table not found')
#            print('dividend table not found')
#    
#    
#    
#        #download race result table
#        try:
#            table_1 = soup.find_all('table', {'class':'f_tac table_bd draggable'})[0] # Grab the first table     
#            table_1_found=True
#            eprint('result table is found',date_key.replace("/","-"),'_',raceno_key,' tried ',no_of_trial,' times extraction with success','\n')
#            print('result table is found',date_key.replace("/","-"),'_',raceno_key,' tried ',no_of_trial,' times extraction with success','\n')
#        except:
#            problem_races_1=problem_races_1.append(pd.Series([date_key,raceno_key]),ignore_index=True)
#            table_1_found=False
#            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'f_tac table_bd draggable','\n')
#            print("Oops!",sys.exc_info()[0],"occured when extracting ",'f_tac table_bd draggable','\n')
#
#        if table_1_found==True:
#            store=[]
#            i=0
#            
#            #row=table_1.find_all('tr')[1]
#            for row in table_1.find_all('tr'):
#                #print("i is ",i)
#                #print(row)
#                columns = row.find_all('td')
#                #print(len(columns))
#                store_temp=''
#                #before 2008/04/02, there is 11 columns. after this day, there is 12 columns with one more running
#                #position colum, but i didn't waste time to extract it here because it is also at mset
#                for column in columns:
#                    #print(column)
#                    store_temp= store_temp+'|'+column.get_text()                    
#            
#                store.append(store_temp)
#                i=i+1
#    
#            #store=[m.replace('\n','').replace('\r','') for m in store]
#            header=store[0].split('|')[1:]
#            
#            output=[xx.split('|')[1:]  for xx in store][1:]
#            
#            output1=pd.DataFrame(output)
#            output1.columns=header
#            output1['Date']=date_key
#            output1['RaceNo']=raceno_key
#    
#        else:
#            eprint('result table not found')
#            print('result table not found')    
#    else:
#        eprint('result page html code not found')
#        print('result page html code not found')
#    
#    print(date_key)
#    return dividend,problem_races,output1,problem_races_1






#if sys.argv[1]=='post':
#    try:
#        div_sql = sql.read_sql('SELECT * from dividend',connection)
#    except:
#        div_sql=pd.DataFrame([])
#    
#    #read race key
#    race_key=race_key_source.copy()
#    race_key.dtypes
#    
#    race_key = pd.DataFrame(race_key.date_raceno_track.str.split('@').tolist(),columns = ['Date','RaceNo','track3'])
#    race_key.loc[race_key['track3']=='SH','track3']='ST'
#    race_key['Date_key']=race_key['Date'].apply(lambda x:dt.strptime(x,'%Y-%m-%d').strftime('%Y/%m/%d'))
#    race_key['date_raceno']=race_key['Date_key']+"|"+race_key['RaceNo']
#    
#        
#    #this part is to find records (date and raceno) in dividend table and remove
#    #them in race_key, which means that not to download them if already in mysql
#    if div_sql.shape[0]!=0:
#        #find what is currently in database
#        date_raceno_key_in_db=div_sql['Date']+"|"+div_sql['RaceNo']
#        date_raceno_key_in_db=date_raceno_key_in_db.tolist()
#        
#        #if already in db, then yes
#        race_key['in_db']=race_key['date_raceno'].apply(lambda x: 'yes' if x in date_raceno_key_in_db else 'no')
#        
#        # in race_key, remove races already in db
#        #so only download tables not in db
#        race_key=race_key.loc[race_key['in_db']=='no',:].reset_index(drop=True)
#    
#    #i=0
#    div_table=pd.DataFrame([])
#    problem_races_all=pd.DataFrame([])
#    
#    racemain_table=pd.DataFrame([])
#    problem_races_1_all=pd.DataFrame([])
#    
#    
#    
#
#    
#    
#    
#    for i in range(0,race_key.shape[0]):
#    #for i in range(0,2000):
#    #i=3342;i=3343;i=13381
#    #for i in range(0,4283):
#        row_use=race_key[i:i+1]
#        div_temp,problem_races_temp,race_main_temp,problem_races_1_temp=extract_div(date_key=row_use['Date_key'].values[0],raceno_key=str(row_use['RaceNo'].values[0]))
#        
#        div_table=div_table.append(div_temp)
#        racemain_table=racemain_table.append(race_main_temp)
#        
#        if problem_races_temp.shape[0]!=0:
#            problem_races_all=problem_races_all.append(problem_races_temp)
#    
#        if problem_races_1_temp.shape[0]!=0:
#            problem_races_1_all=problem_races_1_all.append(problem_races_1_temp)
#            
#        eprint("Finished ",i+1, "out of ",race_key.shape[0],'------------------------------------------------------------------------------------------------------------------------')
#        print("Finished ",i+1, "out of ",race_key.shape[0],'------------------------------------------------------------------------------------------------------------------------')
#
#    #save problem races
#    problem_races_all.to_csv("log/problem_races_"+"dividend"+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv",index=False)
#    problem_races_1_all.to_csv("log/problem_races_"+"race_result"+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv",index=False)
#    
#    #extract data from my sql
#    try:
#        raceresult_sql = sql.read_sql('SELECT * from race_result',connection)
#    except:
#        raceresult_sql=pd.DataFrame([])
#    
#    #using this method, if hkjc added more column, it can also be merged
#    racemain_table=raceresult_sql.append(racemain_table)
#    
#    
#    try:
#        div_sql = sql.read_sql('SELECT * from dividend',connection)
#    except:
#        div_sql=pd.DataFrame([])
#    
#    #using this method, if hkjc added more column, it can also be merged
#    div_table=div_sql.append(div_table)
#    
#    div_table=div_table.drop_duplicates()
#    racemain_table=racemain_table.drop_duplicates()
#    #racemain_table=racemain_table.drop_duplicates(subset=['Date','RaceNo','Horse'], keep="first")
#    
#    
#    div_table=div_table.sort_values(by=['Date','RaceNo'],ascending=[False,True])
#    racemain_table=racemain_table.sort_values(by=['Date','RaceNo'],ascending=[False,True])
#    
#    #div_table_name="log/temp_data_"+"div_table"+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv"
#    #div_table.to_csv(div_table_name,index=False)
#    
#    #racemain_table_name="log/temp_data_"+"racemain_table"+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv"
#    #racemain_table.to_csv(racemain_table_name,index=False)
#    
#    div_table.to_sql('dividend',connection,if_exists='replace',chunksize=1000,index=False) 
#    racemain_table.to_sql('race_result',connection,if_exists='replace',chunksize=1000,index=False)




































#date_key='2019/09/21';raceno_key=str(6)
#date_key='2020/09/06';raceno_key=str(2)
#a,b=extract_raceresult(date_key,raceno_key)

#date_key="2019/10/16"; raceno_key=str(8)
    
#data_extraction

def extract_race_result(date_key,raceno_key):
    eprint('doing '+date_key+' '+raceno_key)
    date_key_old=date_key
    #https://racing.hkjc.com/racing/information/Chinese/Racing/LocalResults.aspx?RaceDate=2019/07/10&RaceNo=9
    url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate='+date_key+'&RaceNo='+raceno_key
    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
    problem_races=pd.DataFrame([])
    raceresult=pd.DataFrame([])
    html_found=False

    for ii in range(0,10):
        s = requests.Session()
        r = s.get(url)
    
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        try:
            source_html=''
            if 'X-AA-Challenge' in r.text:
                challenge = parse_challenge(r.text)
                r = s.get(url, headers={
                    'X-AA-Challenge': challenge['challenge'],
                    'X-AA-Challenge-ID': challenge['challenge_id'],
                    'X-AA-Challenge-Result': challenge['challenge_result']})
            
                yum = r.cookies
                r = s.get(url, cookies=yum)
                source_html=r.text
            
            if not ('f_tac table_bd draggable' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('f_tac table_bd draggable' in source_html):
                html_found=True
                break
            sleep(0.2)
        except:
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'result_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
            print("Oops!",sys.exc_info()[0],"occured when extracting ",'result_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
    

        
    if html_found:
        no_of_trial=ii+1
        eprint('raceresult html code',' tried ',no_of_trial,' times extraction with success',date_key.replace("/","-"),'_',raceno_key,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key,raceno_key]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"raceresult_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
        
        
        
    if html_found:
        store=[]
        i=0
        soup = BeautifulSoup(html_string, 'lxml')
        table = soup.find_all('table', {'class':'f_tac table_bd draggable'})[0] 
        #row=table_1.find_all('tr')[1]
        try:
            for row in table.find_all('tr'):
                #row=table.find_all('tr')[1]
                columns = row.find_all('td')
                #print(len(columns))
                store_temp=''
                #before 2008/04/02, there is 11 columns. after this day, there is 12 columns with one more running
                #position colum, but i didn't waste time to extract it here because it is also at mset
                for column in columns:
                    print(column)
                    store_temp= store_temp+'|'+column.get_text()                    
            
                store.append(store_temp)
                i=i+1
    
            #store=[m.replace('\n','').replace('\r','') for m in store]
            header=store[0].split('|')[1:]
            
            output=[xx.split('|')[1:]  for xx in store][1:]
            
            raceresult=pd.DataFrame(output)
            raceresult.columns=header
            raceresult['Date']=date_key
            raceresult['RaceNo']=raceno_key
        except:
            eprint("raceresult table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key)
    
    else:
        eprint('result table not found')
        print('result table not found') 

    print(date_key)
    return raceresult,problem_races













#a,b=extract_div(date_key,raceno_key)
#date_key="2019/10/16"; raceno_key=str(8)
    
def extract_dividend(date_key,raceno_key):
    eprint('doing '+date_key+' '+raceno_key)
    date_key_old=date_key
    #https://racing.hkjc.com/racing/information/Chinese/Racing/LocalResults.aspx?RaceDate=2019/07/10&RaceNo=9
    url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate='+date_key+'&RaceNo='+raceno_key
    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
    problem_races=pd.DataFrame([])
    dividend=pd.DataFrame([])
    html_found=False

    for ii in range(0,10):
        s = requests.Session()
        r = s.get(url)
    
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        try:
            source_html=''
            if 'X-AA-Challenge' in r.text:
                challenge = parse_challenge(r.text)
                r = s.get(url, headers={
                    'X-AA-Challenge': challenge['challenge'],
                    'X-AA-Challenge-ID': challenge['challenge_id'],
                    'X-AA-Challenge-Result': challenge['challenge_result']})
            
                yum = r.cookies
                r = s.get(url, cookies=yum)
                source_html=r.text
            
            if not ('table_bd f_tac f_fs13 f_fl' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('table_bd f_tac f_fs13 f_fl' in source_html):
                html_found=True
                break
            sleep(0.2)
        except:
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'result_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
            print("Oops!",sys.exc_info()[0],"occured when extracting ",'result_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
    

        
    if html_found:
        no_of_trial=ii+1
        eprint('dividend html code',' tried ',no_of_trial,' times extraction with success',date_key.replace("/","-"),'_',raceno_key,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key,raceno_key]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"dividend_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
        
        
        
    if html_found:
        store=[]
        i=0
        soup = BeautifulSoup(html_string, 'lxml')
        
        try:
            table = soup.find_all('table', {'class':'table_bd f_tac f_fs13 f_fl'})[0] 
            for row in table.find_all('tr'):
                #print("i is ",i)
                #print(row)
                columns = row.find_all('td')
                #print(len(columns))
                store_temp=''
                #observe that only 3 columns in dividend table
                if len(columns)==3:
                    count=0
                    for column in columns:
                        if count==0:
                            store_temp=column.get_text()
                            category=column.get_text()
                        else:
                            store_temp= store_temp+'|'+column.get_text()
                        count=count+1
            
                if len(columns)==2:
                    count=0
                    for column in columns:
                        if count==0:
                            store_temp=category+'|'+column.get_text()
                        else:
                            store_temp=store_temp+'|'+column.get_text()
                        count=count+1
                store.append(store_temp)
                i=i+1
            #    if i==2:
            #        break
            
            def create_df(x):
                return x.split('|')
    #            target_type=['WIN','PLACE','QUINELLA','QUINELLA PLACE','FORECAST','TIERCE','TRIO','FIRST 4','QUARTET']
    #            
    #            output=[xx.split('|')  for xx in store if any(s==xx.split('|')[0]  for s in target_type)]
            #target_type=['WIN','PLACE','QUINELLA','QUINELLA PLACE','FORECAST','TIERCE','TRIO','FIRST 4','QUARTET','TRIPLE TRIO','TRIPLE TRIO(Consolation)','SIX UP']
            
            output=[xx.split('|')  for xx in store]
            
            dividend=pd.DataFrame(output)
            dividend=dividend.rename(columns={0:'Pool',1:'Comb',2:'dividend'})
            dividend['Date']=date_key
            dividend['RaceNo']=raceno_key
        except:
            eprint("dividend table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key)
    
    else:
        eprint('dividend table not found')
        print('dividend table not found') 

    print(date_key)
    return dividend,problem_races















##################################################################
##################################################################
##################################################################
##################################################################
#read Mset using special method of requests with javascript enabled
##################################################################
##################################################################
##################################################################
##################################################################
#date_key='2008/04/02';raceno_key=str(1)
#date_key='2019/09/21';raceno_key=str(6)
#date_key='2020/09/06';raceno_key=str(2)
#a,b=extract_mset(date_key,raceno_key)

def extract_mset(date_key,raceno_key):
    #date_key='2019/11/06';raceno_key=str(7)
    date_key_old=date_key
    date_key=dt.strptime(date_key,'%Y/%m/%d').strftime('%d/%m/%Y')
    url='https://racing.hkjc.com/racing/information/English/Racing/DisplaySectionalTime.aspx?RaceDate='+date_key+'&RaceNo='+raceno_key
    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
    #print(url)
    problem_races=pd.DataFrame([])
    mset=pd.DataFrame([])
    html_found=False

    for ii in range(0,50):
        s = requests.Session()
        r = s.get(url)
    
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        try:
            source_html=''
            if 'X-AA-Challenge' in r.text:
                challenge = parse_challenge(r.text)
                r = s.get(url, headers={
                    'X-AA-Challenge': challenge['challenge'],
                    'X-AA-Challenge-ID': challenge['challenge_id'],
                    'X-AA-Challenge-Result': challenge['challenge_result']})
            
                yum = r.cookies
                r = s.get(url, cookies=yum)
                source_html=r.text
            
            if not ('table_bd f_tac race_table' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('table_bd f_tac race_table' in source_html):
                html_found=True
                break
            sleep(0.2)
        except:
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'Mset_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
            print("Oops!",sys.exc_info()[0],"occured when extracting ",'Mset_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
    


    if html_found:
        no_of_trial=ii+1
        eprint('mset html code',' tried ',no_of_trial,' times extraction with success',date_key.replace("/","-"),'_',raceno_key,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key,raceno_key]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"Mset_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
    if html_found:
        #open(r'C:\Users\simon.chan\Desktop\setting\try.txt', 'wb').write(html_string)
        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
    
        #table = soup.find_all('table')[0] # Grab the first table
        #find table html code with class table_bd f_tac f_fs13 f_fl
        try:
            table = soup.find_all('table', {'class':'table_bd f_tac race_table'})[0] # Grab the first table     
            store=[]
            i=0
            for row in table.find_all('tr'):
                i=i+1
                columns = row.find_all('td')
    #                if i==4:
    #                    print("i is ",i)
    #                    print(row)
    #                    print(len(columns))
                store_temp=''         
    #                if i==4:
    #                    break
                #j=1
                if i==1:
                    for column in columns:
                        store_temp= store_temp+'|'+column.get_text()                
                #note that for i =1,2,3. they are the first three rows, which are text header, useless
                #only i=1 is useful
                if i>=4:
                    j=0
                    for column in columns:
                        #print(columns)
                        j=j+1
                        #print(j)
    #                        if j==7:
    #                            print(column)
    #                            break
                        #for 4 to 9 columns, they store the threes number for sec_position,sec_margin, sec_time
                        if (j>=4) & (j<=9):
                            #because hkjc use  sec1 to sec6 to store sectional data
                            #so it may be blank, if blank, append ''
                            if column.find_all('span')==[]:
                                store_temp= store_temp+'|'+''
                            else:
                                Sec_position=column.find_all('span')[0].get_text()
                                Sec_margin=column.find_all('i')[0].get_text()
                                Sec_time=column.find_all('p')[1].get_text()
                                store_temp= store_temp+'|'+Sec_position+'@'+Sec_margin+"@"+Sec_time
                        else:
                            store_temp= store_temp+'|'+column.get_text() 
                            
                store.append(store_temp)
    
            
            header=['FP','HorseNo','HorseName','sec1','sec2','sec3','sec4','sec5','sec6','Time']
            
            #the first one is '', so start from 1
            output=[xx.split('|')[1:]  for xx in store]
            
            #first three rows are useless, so start from 3
            mset=pd.DataFrame(output[3:])
            mset.columns=header
            mset['Date']=date_key_old
            mset['RaceNo']=raceno_key
        except:
            eprint("mset table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key)

    else:
        eprint('mset html code not found')

    print(date_key)
    return mset,problem_races









#date_key='2008/04/27';raceno_key=str(7);track_key='ST'
#date_key='2006/09/16';raceno_key=str(1);track_key='ST'
#date_key='2006/09/16';raceno_key=str(3);track_key='ST'
#date_key='2006/10/04';raceno_key=str(3);track_key='HV'
#date_key='2011/12/27';raceno_key=str(3);track_key='ST'
#date_key='2018/06/06';raceno_key=str(1);track_key='HV'
#date_key='2019/07/07';raceno_key=str(1);track_key='ST'
#date_key='2011/12/27';raceno_key=str(8);track_key='ST'
#date_key='2006/09/10';raceno_key=str(1);track_key='ST'
#date_key='2014/03/16';raceno_key=str(8);track_key='ST'
#date_key='2019/07/14';raceno_key=str(10);track_key='ST'
#date_key='2020/09/13';raceno_key=str(1);track_key='ST'
#date_key='2022/02/20';raceno_key=str(2);track_key='ST'
def extract_racecard(date_key,raceno_key,track_key):
    #date_key='2006/10/04';raceno_key=str(3);track_key='HV'
    date_key_old=date_key
    date_key=dt.strptime(date_key,'%Y/%m/%d').strftime('%Y/%m/%d')
    #url='https://racing.hkjc.com/racing/info/meeting/RaceCard/English/Local/'+date_key+'/'+track_key+'/'+raceno_key
    
    url='https://racing.hkjc.com/racing/information/English/Racing/RaceCard.aspx?RaceDate='+date_key+'&Racecourse='+track_key+'&RaceNo='+raceno_key
    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
    #print(url)
    problem_races=pd.DataFrame([])
    racecard=pd.DataFrame([])
    html_found=False

    for ii in range(0,60):
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        html_found=False
        source_html=''
        try:
            s = requests.Session()
            r = s.get(url)
            source_html=r.text            
            
            if not ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                html_found=True
                break
            sleep(0.2)
        
        except:
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'racecard ',date_key_old.replace("/","-"),'_',raceno_key,'\n')
        #sleep(0.1)
        
    if html_found:
        no_of_trial=ii+1
        eprint('racecard html code',' tried ',no_of_trial,' times extraction with success',date_key_old.replace("/","-"),'_',raceno_key,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key,raceno_key]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"racecard_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
        
    if html_found:
        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
    
        #table = soup.find_all('table')[0] # Grab the first table
        #find table html code with class tableBorderBlue tdAlignC

        try:
            table = soup.find_all('table', {'class':'starter f_tac f_fs13 draggable hiddenable'})[0] # Grab the first table     
            store=[]
            i=0
            for row in table.find_all('tr'):
                i=i+1
                columns = row.find_all('td')
    #                if i==3:
    #                    print("i is ",i, ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    #                    print(row)
    #                    print(len(columns))
                store_temp=''         
    #                if i==3:
    #                    break
                #j=1
                if i>=1:
                    for column in columns:
                        store_temp= store_temp+'|'+column.get_text()
                store.append(store_temp)
    
            header=store[0].split("|")[1:]
            
            #the first one is '', so start from 1
            output=[xx.split('|')[1:]  for xx in store]
            #first 3 rows are useless, so start from 3
            racecard=pd.DataFrame(output[1:])
            racecard.columns=header
            racecard['Date']=date_key_old
            racecard['RaceNo']=raceno_key
            racecard['track']=track_key
        except:
            eprint("racecard table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key)




        #find racecard aboveinfo
        table = soup.find_all('div', {'class':'f_fs13'})[0] # Grab the first table     
        #table = soup.find_all('div', {'class':'f_fs13'})[0] # Grab the first table     
        
        info_above="NOT OK"
        try:
            all_text2 = table.get_text(separator="<br/>").split("<br/>")
            
            racecourse=all_text2[1].split(",")[-2].strip()
            race_start_time=all_text2[1].split(",")[-1].strip()
            
            if all_text2[2].split(",")[0].strip()=='All Weather Track':
                truf_dirt='All Weather Track'
                course='All Weather Track'
                distance=all_text2[2].split(",")[1].strip()[:-1]
                surface_condition=all_text2[2].split(",")[2].strip()
                
                if re.search('Money:(.*)Rating:',all_text2[3]) is None:
                    prize_money=re.search('Money:(.*)-,',all_text2[3]).group(1).strip().replace(',','').replace('$','')    
                else:
                    prize_money=re.search('Money:(.*)Rating:',all_text2[3]).group(1).strip().replace(',','').replace('$','')
                
                rating='' if re.search('Rating:(.*),',all_text2[3]) is None else re.search('Rating:(.*),',all_text2[3]).group(1).strip()
                race_class=all_text2[3].split(",")[-1].strip()
                
#                prize_money=all_text2[3].split(",")[2].strip().split("Prize Money: $")[1]
#                rating=all_text2[3].split(",")[3].strip().replace("Rating:",'')
#                race_class=all_text2[3].split(",")[4].strip()[:-1]
            else:
                truf_dirt=all_text2[2].split(",")[0].strip()
                course=all_text2[2].split(",")[1].strip()
                distance=all_text2[2].split(",")[2].strip()[:-1]
                surface_condition=all_text2[2].split(",")[3].strip().split("Prize Money: $")[0]
                #because there is , in prize money, so cannot split by ,
                #if griffin race or G1 G2 G3, there is no rating, so cannot use "Rating" as key words
                
                if re.search('Money:(.*)Rating:',all_text2[3]) is None:
                    prize_money=re.search('Money:(.*)-,',all_text2[3]).group(1).strip().replace(',','').replace('$','')
                else:
                    prize_money=re.search('Money:(.*)Rating:',all_text2[3]).group(1).strip().replace(',','').replace('$','')
                
                rating='' if re.search('Rating:(.*),',all_text2[3]) is None else re.search('Rating:(.*),',all_text2[3]).group(1).strip()
                race_class=all_text2[3].split(",")[-1].strip()
#                prize_money=all_text2[3].split(",")[3].strip().split("Prize Money: $")[1]
#                rating=all_text2[3].split(",")[4].strip().replace("Rating:",'')
#                race_class=all_text2[3].split(",")[5].strip()[:-1]
            
            race_name=all_text2[0].split(" - ")[1].replace("\xa0",'').strip()
            info_above="OK"
        except:
            eprint("racecard aboveinfo string format may not be standard",date_key_old.replace("/","-"),'_',raceno_key,'\n')
            info_above="NOT OK"

        #merge into to racecard
        
        if info_above=="OK":
            racecard['racecourse']=racecourse
            racecard['race_start_time']=race_start_time
            racecard['truf_dirt']=truf_dirt
            racecard['course']=course
            racecard['distance']=distance
            racecard['surface_condition']=surface_condition
            racecard['prize_money']=prize_money
            racecard['rating']=rating
            racecard['race_class']=race_class
            racecard['race_name']=race_name
        else:
            racecard['racecourse']=None
            racecard['race_start_time']=None
            racecard['truf_dirt']=None
            racecard['course']=None
            racecard['distance']=None
            racecard['surface_condition']=None
            racecard['prize_money']=None
            racecard['rating']=None
            racecard['race_class']=None
            racecard['race_name']=None          

    else:
        eprint('required table in html code not found ',date_key_old.replace("/","-"),'_',raceno_key)

    print(date_key)
    return racecard,problem_races























#extract_trackwork_v2('2019/10/12',str(1),'ST','2','gold velvet_V400')

#date_key='2006/09/10';raceno_key=str(1);track_key='ST';horse_no='2'
#date_key='2019/10/12';raceno_key=str(1);track_key='ST';horse_no='2';HorseName_Brand='gold velvet_V400'

#date_key='2019/10/30';raceno_key=str(1);track_key='HV';horse_no='8';HorseName_Brand='romantic journey_B107'

#date_key='2019/10/30';raceno_key=str(2);track_key='HV';horse_no='5';HorseName_Brand='grand harbour_P421'

#date_key='2019/11/03';raceno_key=str(3);track_key='ST';horse_no='3';HorseName_Brand='amazing paradise_C518'
#date_key='2020/09/06';raceno_key=str(2);track_key='ST';horse_no='1';HorseName_Brand='adonis_A324'
#date_key='2021/09/05';raceno_key=str(1);track_key='ST';horse_no='1';HorseName_Brand='jolly forever_D331'

def extract_trackwork_v2(date_key,raceno_key,track_key,horse_no,HorseName_Brand):
    #date_key='2006/10/04';raceno_key=str(3);track_key='HV'
    date_key_old=date_key
    date_key=dt.strptime(date_key,'%Y/%m/%d').strftime('%Y%m%d')
    #url='https://racing.hkjc.com/racing/info/meeting/RaceCard/English/Local/'+date_key+'/'+track_key+'/'+raceno_key
    url='https://racing.hkjc.com/racing/information/English/Racing/RaceCard.aspx?RaceDate='+date_key+'&Racecourse='+track_key+'&RaceNo='+raceno_key
    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
    #print(url)
    horse_no_old=horse_no
    horse_brand=HorseName_Brand
    
    trackwork_all=pd.DataFrame([])
    problem_races=pd.DataFrame([])
    html_found=False

    for ii in range(0,100):
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        html_found=False
        source_html=''
        try:
            s = requests.Session()
            r = s.get(url)
            source_html=r.text            

            if not ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                html_found=True
                break            
            
            
            sleep(0.2)
        except:
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'trackwork ',date_key_old.replace("/","-"),'_',raceno_key,'\n')
        #sleep(0.1)
        
    if html_found:
        no_of_trial=ii+1
        eprint('trackwork html code',' tried ',no_of_trial,' times extraction with success',date_key_old.replace("/","-"),'_',raceno_key,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key,raceno_key,track_key,horse_no,HorseName_Brand]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"trackwork_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
        
    if html_found:
        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
    
        #table = soup.find_all('table')[0] # Grab the first table
        #find table html code with class tableBorderBlue tdAlignC

        try:
            table = soup.find_all('table', {'class':'tableBorderBlue tdAlignC'})[0] # Grab the first table     
            i=0
            #row=table.find_all('tr')[2]
            #row
            #i=4
            #a=table.find_all('tr')
            #len(table.find_all('tr')[1].find_all('tr'))
            for row in table.find_all('tr')[1].find_all('tr'):
                i=i+1
                #print(i)
                #print(row)
                #row=table.find_all('tr')[1].find_all('tr')[1]
                #row
                columns = row.find_all('td')
    #                if i==3:
    #                    print("i is ",i, ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    #                    print(row)
    #                    print(len(columns))
#                if i==5:
#                    break
                #j=1
                if (i>=1)&(columns[0].get_text()==horse_no_old):
                    print(i)
                    horse_link=columns[3].find('a').get('href')
                    horse_link='https://racing.hkjc.com'+horse_link

                    
                    for ii in range(0,50):
                        try:
                            s1 = requests.Session()
                            r1 = s1.get(horse_link)
                            if 'X-AA-Challenge' in r1.text:
                                challenge = parse_challenge(r1.text)
                                r1 = s1.get(horse_link, headers={
                                    'X-AA-Challenge': challenge['challenge'],
                                    'X-AA-Challenge-ID': challenge['challenge_id'],
                                    'X-AA-Challenge-Result': challenge['challenge_result']})
                            
                                yum = r1.cookies
                                r1 = s1.get(horse_link, cookies=yum)
                                source_html_horse_page=r1.text
                            else:
                                source_html_horse_page=r1.text
                        except:
                            eprint(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, horse page ',horse_brand)
                            print(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, horse page ',horse_brand)
                            pass
                        
                        if 'horseProfile' in source_html_horse_page:
                            break
                        sleep(0.2)
                        
                        
                    
                    soup2 = BeautifulSoup(source_html_horse_page, 'lxml') 
                    table2 = soup2.find_all('table', {'class':'horseProfile'})[0]
                    
                    all_link=table2.find_all('ul')[0].find_all('a')
                    #i=0
                    trackwork_link=''
                    for x in all_link:
    #                    i=i+1
    #                    if i==2:
    #                        break
                        if x.get_text()=='Trackwork Records':
                            trackwork_link='https://racing.hkjc.com'+x.get('href')


                    for ii in range(0,10):
                        try:    
                            s2 = requests.Session()
                            r2 = s2.get(trackwork_link)                
        
                            if 'X-AA-Challenge' in r2.text:
                                challenge = parse_challenge(r2.text)
                                r2 = s2.get(trackwork_link, headers={
                                    'X-AA-Challenge': challenge['challenge'],
                                    'X-AA-Challenge-ID': challenge['challenge_id'],
                                    'X-AA-Challenge-Result': challenge['challenge_result']})
                            
                                yum = r2.cookies
                                r2 = s2.get(trackwork_link, cookies=yum)
                                source_html_trackwork_page=r2.text
                            else:
                                source_html_trackwork_page=r2.text
                        except:
                            eprint(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, trackwork page ',horse_brand)
                            print(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, trackwork page ',horse_brand)
                            pass
                        #because for sometimes, r2.ok still True, but "table_bd f_tal f_fs13" not exist
                        #but after checking hkjc, there is table_bd f_tal f_fs13, so use below method to check
                        if 'table_bd f_tal f_fs13' in source_html_trackwork_page:
                            break

                        sleep(0.2)
                        
                    if 'table_bd f_tal f_fs13' in source_html_trackwork_page:
                        soup3 = BeautifulSoup(source_html_trackwork_page, 'lxml') 
                        table3 = soup3.find_all('table', {'class':'table_bd f_tal f_fs13'})[0]
                        
                        #i=0
                        store=[]
                        for row in table3.find_all('tr'):
    #                        i=i+1
    #                        if i==1:
    #                            break
                            columns = row.find_all('td')
                            store_temp=''
                            for column in columns:
                                store_temp= store_temp+'|'+column.get_text()
                            store.append(store_temp)
                    
        
                        header=store[0].split("|")[1:]
                        
                        #the first one is '', so start from 1
                        output=[xx.split('|')[1:]  for xx in store]
                        #first 3 rows are useless, so start from 3
                        trackwork=pd.DataFrame(output[1:])
                        trackwork.columns=header
                        trackwork['Date_racecard']=date_key_old
                        trackwork['RaceNo_racecard']=raceno_key
                        trackwork['HorseNo_racecard']=horse_no
                        trackwork['HorseName_Brand']=horse_brand
                        
                        trackwork_all=trackwork_all.append(trackwork)
                    
                    else:
                        header=[]
                        header.append('Date')
                        header.append('Type')
                        header.append('Racecourse/Track')
                        header.append('Workouts')
                        header.append('Gear')
                        header.append('Date_racecard')
                        header.append('RaceNo_racecard')
                        header.append('HorseNo_racecard')
                        header.append('HorseName_Brand')
                        trackwork=pd.DataFrame(columns=header)
                        trackwork.loc[0]=[None,None,None,None,None,date_key_old,raceno_key,horse_no,horse_brand]
                        
                        trackwork_all=trackwork_all.append(trackwork)
                        eprint("Error this horse no trackwork table please check",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
                        print("Error this horse no trackwork table please check",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
                    
                    

                    
            eprint('horse ',HorseName_Brand, 'ok')
            print('horse ',HorseName_Brand, 'ok')
        except:
            eprint("trackwork table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
            print("trackwork table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
            eprint(sys.exc_info()[0],sys.exc_info()[1])
            print(sys.exc_info()[0],sys.exc_info()[1])
            problem_races=problem_races.append(pd.Series([date_key,raceno_key,track_key,horse_no,HorseName_Brand]),ignore_index=True)


    else:
        eprint('required table in html code not found ',date_key_old.replace("/","-"),'_',raceno_key)
        print('required table in html code not found ',date_key_old.replace("/","-"),'_',raceno_key)

    return trackwork_all,problem_races


#A1,A2=extract_trackwork_v2(date_key,raceno_key,track_key,horse_no,HorseName_Brand)














#date_key='2019/10/12';raceno_key=str(1);track_key='ST';horse_no='2';HorseName_Brand='gold velvet_V400'
#date_key='2019/10/12';raceno_key=str(10);track_key='ST';horse_no='1';HorseName_Brand='scared ibis_A127'
#date_key='2019/11/03';raceno_key=str(1);track_key='ST';horse_no='4';HorseName_Brand='kwewe chung elite_C173'
#date_key='2020/09/06';raceno_key=str(2);track_key='ST';horse_no='1';HorseName_Brand='adonis_A324'
#date_key='2021/09/05';raceno_key=str(1);track_key='ST';horse_no='1';HorseName_Brand='jolly forever_D331'



def extract_sickhistory_v2(date_key,raceno_key,track_key,horse_no,HorseName_Brand):
    #date_key='2006/10/04';raceno_key=str(3);track_key='HV'
    date_key_old=date_key
    date_key=dt.strptime(date_key,'%Y/%m/%d').strftime('%Y%m%d')
    #url='https://racing.hkjc.com/racing/info/meeting/RaceCard/English/Local/'+date_key+'/'+track_key+'/'+raceno_key
    url='https://racing.hkjc.com/racing/information/English/Racing/RaceCard.aspx?RaceDate='+date_key+'&Racecourse='+track_key+'&RaceNo='+raceno_key
    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
    #print(url)
    horse_no_old=horse_no
    horse_brand=HorseName_Brand
    
    sickhistory_all=pd.DataFrame([])
    problem_races=pd.DataFrame([])
    html_found=False

    for ii in range(0,100):
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        html_found=False
        source_html=''
        try:
            s = requests.Session()
            r = s.get(url)
            source_html=r.text            

            if not ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                html_found=True
                break  
            sleep(0.2)
        except:
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'sickhistory ',date_key_old.replace("/","-"),'_',raceno_key,'\n')
        #sleep(0.1)
        
    if html_found:
        no_of_trial=ii+1
        eprint('sickhistory html code',' tried ',no_of_trial,' times extraction with success',date_key_old.replace("/","-"),'_',raceno_key,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key,raceno_key,track_key,horse_no,HorseName_Brand]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"sickhistory_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
        
    if html_found:
        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
    
        #table = soup.find_all('table')[0] # Grab the first table
        #find table html code with class tableBorderBlue tdAlignC

        try:
            table = soup.find_all('table', {'class':'tableBorderBlue tdAlignC'})[0] # Grab the first table     
            i=0
            #row=table.find_all('tr')[2]
            #row
            #i=4
            #a=table.find_all('tr')
            #len(table.find_all('tr')[1].find_all('tr'))
            for row in table.find_all('tr')[1].find_all('tr'):
                i=i+1
                #print(i)
                #print(row)
                #row=table.find_all('tr')[1].find_all('tr')[1]
                #row
                columns = row.find_all('td')
    #                if i==3:
    #                    print("i is ",i, ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    #                    print(row)
    #                    print(len(columns))
#                if i==5:
#                    break
                #j=1
                if (i>=1)&(columns[0].get_text()==horse_no_old):
                    horse_link=columns[3].find('a').get('href')
                    horse_link='https://racing.hkjc.com'+horse_link

                    
                    for ii in range(0,50):
                        try:
                            s1 = requests.Session()
                            r1 = s1.get(horse_link)
                            if 'X-AA-Challenge' in r1.text:
                                challenge = parse_challenge(r1.text)
                                r1 = s1.get(horse_link, headers={
                                    'X-AA-Challenge': challenge['challenge'],
                                    'X-AA-Challenge-ID': challenge['challenge_id'],
                                    'X-AA-Challenge-Result': challenge['challenge_result']})
                            
                                yum = r1.cookies
                                r1 = s1.get(horse_link, cookies=yum)
                                source_html_horse_page=r1.text
                            else:
                                source_html_horse_page=r1.text
                        except:
                            eprint(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, horse page ',horse_brand)
                            print(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, horse page ',horse_brand)
                            pass
                        
                        if 'horseProfile' in source_html_horse_page:
                            break
                        sleep(0.2)
                        
                        
                    
                    soup2 = BeautifulSoup(source_html_horse_page, 'lxml') 
                    table2 = soup2.find_all('table', {'class':'horseProfile'})[0]
                    
                    all_link=table2.find_all('ul')[0].find_all('a')
                    #i=0
                    trackwork_link=''
                    for x in all_link:
    #                    i=i+1
    #                    if i==2:
    #                        break
                        if x.get_text()=='Veterinary Records':
                            trackwork_link='https://racing.hkjc.com'+x.get('href')

                    for ii in range(0,10):
                        try:    
                            s2 = requests.Session()
                            r2 = s2.get(trackwork_link)                
        
                            if 'X-AA-Challenge' in r2.text:
                                challenge = parse_challenge(r2.text)
                                r2 = s2.get(trackwork_link, headers={
                                    'X-AA-Challenge': challenge['challenge'],
                                    'X-AA-Challenge-ID': challenge['challenge_id'],
                                    'X-AA-Challenge-Result': challenge['challenge_result']})
                            
                                yum = r2.cookies
                                r2 = s2.get(trackwork_link, cookies=yum)
                                source_html_trackwork_page=r2.text
                            else:
                                source_html_trackwork_page=r2.text
                        except:
                            eprint(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, trackwork page ',horse_brand)
                            print(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, trackwork page ',horse_brand)
                            pass
                        #so if a horse no sick record, will still run this loop 10 times
                        if 'table_bd' in source_html_trackwork_page:
                            break

                        sleep(0.2)
                        
                    if 'table_bd' in source_html_trackwork_page:                        
                        soup3 = BeautifulSoup(source_html_trackwork_page, 'lxml') 
                        table3 = soup3.find_all('table', {'class':'table_bd'})[0]
                        
                        #i=0
                        store=[]
                        for row in table3.find_all('tr'):
    #                        i=i+1
    #                        if i==1:
    #                            break
                            columns = row.find_all('td')
                            store_temp=''
                            for column in columns:
                                store_temp= store_temp+'|'+column.get_text()
                            store.append(store_temp)
                    
        
                        header=store[0].split("|")[1:]
                        
                        #the first one is '', so start from 1
                        output=[xx.split('|')[1:]  for xx in store]
                        #first 3 rows are useless, so start from 3
                        sickhistory=pd.DataFrame(output[1:])
                        sickhistory.columns=header
                        sickhistory['Date_racecard']=date_key_old
                        sickhistory['RaceNo_racecard']=raceno_key
                        sickhistory['HorseNo_racecard']=horse_no
                        sickhistory['HorseName_Brand']=horse_brand
                        
                        sickhistory_all=sickhistory_all.append(sickhistory)
                    else:
                        header=[]
                        header.append('Date')
                        header.append('Details')
                        header.append('Passed Date')
                        header.append('Date_racecard')
                        header.append('RaceNo_racecard')
                        header.append('HorseNo_racecard')
                        header.append('HorseName_Brand')
                        sickhistory=pd.DataFrame(columns=header)
                        sickhistory.loc[0]=[None,None,None,date_key_old,raceno_key,horse_no,horse_brand]
                        
                        sickhistory_all=sickhistory_all.append(sickhistory)
                    
                    
                    
                    
                    
            eprint('horse ',HorseName_Brand, 'ok')
            print('horse ',HorseName_Brand, 'ok')
        except:
            eprint("sickhistory table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
            print("sickhistory table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
            eprint(sys.exc_info()[0],sys.exc_info()[1])
            print(sys.exc_info()[0],sys.exc_info()[1])
            problem_races=problem_races.append(pd.Series([date_key,raceno_key,track_key,horse_no,HorseName_Brand]),ignore_index=True)


    else:
        eprint('required table in html code not found ',date_key_old.replace("/","-"),'_',raceno_key)
        print('required table in html code not found ',date_key_old.replace("/","-"),'_',raceno_key)

    return sickhistory_all,problem_races



#a1,a2=extract_sickhistory_v2(date_key,raceno_key,track_key,horse_no,HorseName_Brand)








#date_key='2006/09/10';raceno_key=str(1);track_key='ST'
#date_key='2019/07/01';raceno_key=str(1);track_key='ST'
#date_key='2006/10/25';raceno_key=str(4);track_key='ST'
#date_key='2019/09/01';raceno_key=str(1);track_key='ST'
#date_key='2019/09/15';raceno_key=str(1);track_key='ST'
#def extract_trackwork(date_key,raceno_key):
#    date_key_old=date_key
#    date_key=dt.strptime(date_key,'%Y/%m/%d').strftime('%d/%m/%Y')
#    url='https://racing.hkjc.com/racing/information/English/Racing/Localtrackwork.aspx?RaceDate='+date_key+'&RaceNo='+raceno_key
#    #url='https://r...content-available-to-author-only...c.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
#    #print(url)
#    problem_races=pd.DataFrame([])
#    trackwork=pd.DataFrame([])
#    html_found=False
# 
#    for ii in range(0,100):
#        s = requests.Session()
#        r = s.get(url)
# 
#        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
#        try:
#            source_html=''
#            if 'X-AA-Challenge' in r.text:
#                challenge = parse_challenge(r.text)
#                r = s.get(url, headers={
#                    'X-AA-Challenge': challenge['challenge'],
#                    'X-AA-Challenge-ID': challenge['challenge_id'],
#                    'X-AA-Challenge-Result': challenge['challenge_result']})
# 
#                yum = r.cookies
#                r = s.get(url, cookies=yum)
#                source_html=r.text
# 
#            if ('table_bd f_fs13' in source_html):
#                html_found=True
#                break
#        except:
#            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'trackwork_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
# 
# 
#    if html_found:
#        no_of_trial=ii+1
#        eprint('trackwork html code',' tried ',no_of_trial,' times extraction with success',date_key.replace("/","-"),'_',raceno_key,'\n')
#    else:
#        problem_races=problem_races.append(pd.Series([date_key,raceno_key]),ignore_index=True)
# 
# 
#
#    html_string=source_html
#    #save source code
#    file_name='store_html'+'/'+"trackwork_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
#    #sometime wb mode not working, so use w
#
#    file = open(file_name,"w",encoding="utf-8") #open file in binary mode
#    file.write(html_string)
#    file.close()  
#
#    if html_found:
#        #open(r'\try.txt', 'wb').write(html_string)
#        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
#        
#        #if necessary save soup (beautifulsoup object as txt
##        f=open("res.txt","w")
##        f.write(str(soup))
##        f.close()    
#
#        #table = soup.find_all('table')[0] # Grab the first table
#        #find table html code with class table_bd f_tac f_fs13 f_fl
#        try:
#            soup.find_all('div', {'class':'nav clearfix'})
#            #table=soup.find_all('table')[2]
#            #table.find_all('table')
#            
#            try:
#                table = soup.find_all('table', {'class':'table_bd f_fs13 '})[0] # Grab the first table 
#            except:
#                #after converting in soup(beautifulsoup object, the space at the right side may disappeared)
#                # so need to use below
#                table = soup.find_all('table', {'class':'table_bd f_fs13'})[0]
#            
#            
#            store=[]
#            i=0
#            for row in table.find_all('tr'):
#                i=i+1
#                columns = row.find_all('td')
#
##                if i<=4:
##                    print("i is ",i,">>>>>>>>>>>>>>>>>>>>>>>>")
##                    print(row)
##                    print(len(columns))
#                store_temp2=''
##                print("i is ",i,">>>>>>>>>>>>>>>>>>>>>>>>")
##                if i==5:
##                    break
#                #j=1
#                if i==1:
#                    for column in columns:
#                        store_temp2= store_temp2+'|'+column.get_text().strip()               
#                #note that for i =1,2,3. they are the first three rows, which are text header, useless
#                #only i=1 is useful
#                if i>=2:
#                    j=0
#                    store_temp2=''
#                    for column in columns:
#                        #print(columns)
#                        j=j+1
##                        print(j,">>>>>>>>>>>>>>>>>>>>>>")
##                        if j==3:
##                            print(column)
##                            break
#                        if j==1:
#                            store_temp2=store_temp2+"|"+column.get_text()
#                        if j==2:
#                            horse_name=column.find_all('a')[0].get_text()
#                            trainer_name=column.find_all('a')[1].get_text()
#                            if column.find_all('a')==[]:
#                                horse_name='';trainer_name=''
#                            store_temp2=store_temp2+"|"+horse_name+"##"+trainer_name
# 
#                        if (j==3)or(j==4) or (j==5) or(j==6):
#                            store_temp3=''
#                            if column.get_text()=='\n':
#                                store_temp2=store_temp2+"|"+store_temp3
#                            else:
#                                target_text=str(column.get_text()).replace('\n','')
#                                #\d is one digit from 0 to 9 so \d\d is two digit, need to esape \ so use /\
#                                #in barrier trial, there is 10/10 ,eaning fp=10 out of 10
#                                #so need to also include : to identify date
#                                date_dd_mm=re.findall('\d\d\/\d\d:',target_text)
#                                all_text=re.compile('\d\d\/\d\d: ').split(target_text)
#                                if '' in all_text:
#                                    all_text.remove('')
#                                all_text=[i.strip() for i in all_text]
#                                for ii in range(0,len(date_dd_mm)):
#                                    store_temp3=store_temp3+'##'+date_dd_mm[ii].replace(":","")+'@@'+all_text[ii]
#                                store_temp2=store_temp2+"|"+store_temp3
#                store.append(store_temp2)
#
#    
#            header=store[0].split('|')[1:]
#            header[1]='HorseName_Trainer'
#            header=header[0:6]
# 
#            #the first one is '', so start from 1
#            output=[xx.split('|')[1:]  for xx in store]
# 
#            trackwork=pd.DataFrame(output[1:])
#            trackwork.columns=header
#            trackwork['Date']=date_key_old
#            trackwork['RaceNo']=raceno_key
#        except:
#            eprint("trackwork format may not be standard",date_key_old.replace("/","-"),'_',raceno_key,'\n')
# 
#    else:
#        eprint('trackwork html code not found')
# 
#    print(date_key)
#    return trackwork,problem_races











#date_key='2006/09/10';raceno_key=str(1);track_key='ST'
#date_key='2019/07/14';raceno_key=str(7);track_key='ST'
#def extract_sickhistory(date_key,raceno_key,track_key):
#    date_key_old=date_key
#    date_key=date_key.replace("/",'')
#    url='https://racing.hkjc.com/racing/info/meeting/VeterinaryRecord/English/Local/'+date_key+'/'+track_key+'/'+raceno_key
#    #url='https://r...content-available-to-author-only...c.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
#    #print(url)
#    problem_races=pd.DataFrame([])
#    sickhistory=pd.DataFrame([])
#    html_found=False
#    for ii in range(0,100):
#        s = requests.Session()
#        r = s.get(url)
# 
#        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
#        try:
#            source_html=''
#            source_html=r.text
#            if ('tableBorder0 tdAlignL' in source_html):
#                html_found=True
#                break
#        except:
#            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'sick_history_page',date_key_old.replace("/","-"),'_',raceno_key,'\n')
# 
# 
#    if html_found:
#        no_of_trial=ii+1
#        eprint('sick history html code',' tried ',no_of_trial,' times extraction with success',date_key.replace("/","-"),'_',raceno_key,'\n')
#    else:
#        problem_races=problem_races.append(pd.Series([date_key,raceno_key]),ignore_index=True)
# 
# 
#    html_string=source_html
#    #save source code
#    file_name='store_html'+'/'+"sick_history_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
#    #sometime wb mode not working, so use w
#    try:
#        file = open(file_name,"wb") #open file in binary mode
#        file.write(html_string)
#        file.close()
#    except:
#        file = open(file_name,"w") #open file in binary mode
#        file.write(html_string)
#        file.close()  
# 
#    if html_found:
#        #open(r'\try.txt', 'wb').write(html_string)
#        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
# 
#        #table = soup.find_all('table')[0] # Grab the first table
#        #find table html code with class table_bd f_tac f_fs13 f_fl
#        try:
#            table = soup.find_all('table', {'class':'tableBorder0 tdAlignL'})[0] # Grab the first table     
#            store=[]
#            i=0
#            for row in table.find_all('tr'):
#                i=i+1
#                columns = row.find_all('td')
##                if i<=4:
##                    print("i is ",i,">>>>>>>>>>>>>>>>>>>>>>>>")
##                    print(row)
##                    print(len(columns))
#                store_temp=''         
##                if i==1:
##                    break
#                #j=1
#                #if i==1:
#                for column in columns:
#                    store_temp= store_temp+'|'+column.get_text()                
#                store.append(store_temp)
# 
#            if store[1]!='|No Information Provided':     
#                header=store[0].split("|")[1:]
#     
#                #the first one is '', so start from 1
#                output=[xx.split('|')[1:]  for xx in store]
#     
#                #first three rows are useless, so start from 3
#                sickhistory=pd.DataFrame(output[1:])
#                sickhistory.columns=header
#     
#                #i=2
#                target_name=sickhistory.loc[0,:].values[1]
#                horse_no=sickhistory.loc[0,:].values[0]
#                for i in range(0,sickhistory.shape[0]):
#                    row_use=sickhistory.loc[i:i+1]
#                    if row_use['Horse Name'].values[0]=='\xa0':
#                        sickhistory.loc[i,'Horse Name']=target_name
#                        sickhistory.loc[i,'Horse No.']=horse_no
#                    else:
#                        target_name=row_use['Horse Name'].values[0]
#                        horse_no=row_use['Horse No.'].values[0]
#                
#                sickhistory=sickhistory.rename(columns={'Date':'Date_sick'})
#                sickhistory['Date']=date_key_old
#                sickhistory['RaceNo']=raceno_key
#            
#            #if all horse no sick record, still make a empty record with date and raceno
#            if sickhistory.shape[0]==0:
#                header=store[0].split("|")[1:]
#                header[2]='Date_sick'
#                header.append('Date')
#                header.append('RaceNo')
#                sickhistory=pd.DataFrame(columns=header)
#                sickhistory.loc[0]=[None,None,None,None,None,date_key_old,raceno_key]
#                    
#                
#        except:
#            eprint("sick table may not be standard ",date_key_old.replace("/","-"),'_',raceno_key)
# 
#    else:
#        eprint('sick html code not found')
# 
#    print(date_key)
#    return sickhistory,problem_races








#define a function to do same thing, because many tables

#target_table_name='mset'
#extraction_function_name='extract_mset'
#target_table_data_start_in_hkjc='2008-04-02'
#no_of_races_extract=10
#    
#target_table_name='racecard'
#extraction_function_name='extract_racecard'
#target_table_data_start_in_hkjc='2006-09-10'
#no_of_races_extract=10000
#
#target_table_name='trackwork'
#extraction_function_name='extract_trackwork_v2'
#target_table_data_start_in_hkjc='2006-09-10'
#no_of_races_extract=1
#
#target_table_name='sickhistory'
#extraction_function_name='extract_sickhistory'
#target_table_data_start_in_hkjc='2006-09-10'
#no_of_races_extract=100000

#target_table_name='dividend'
#extraction_function_name='extract_dividend'
#target_table_data_start_in_hkjc='2003-08-31'
#no_of_races_extract=100000
#
#target_table_name='race_result'
#extraction_function_name='extract_raceresult'
#target_table_data_start_in_hkjc='2003-08-31'
#no_of_races_extract=100000


def data_extraction(target_table_name,extraction_function_name,target_table_data_start_in_hkjc,no_of_races_extract):
    query='SELECT * from '+target_table_name
    try:
        vars()[target_table_name+'_sql'] = sql.read_sql(query,connection)
        #check=vars()[target_table_name+'_sql'].copy()
    except:
        vars()[target_table_name+'_sql']=pd.DataFrame([])
    
    #read race key
    race_key=race_key_source.copy()
    race_key.dtypes
    
    race_key = pd.DataFrame(race_key.date_raceno_track.str.split('@').tolist(),columns = ['Date','RaceNo','track3'])
    race_key.loc[race_key['track3']=='SH','track3']='ST'
    
    #in racecard, in the hkjc hyperlink, only use HV or ST to identofy, so need to change AWT ro ST
    race_key['track_ST_HV_only']=race_key['track3']   
    race_key.loc[race_key['track_ST_HV_only']=='AWT','track_ST_HV_only']='ST'
    
    race_key['Date_key']=race_key['Date'].apply(lambda x:dt.strptime(x,'%Y-%m-%d').strftime('%Y/%m/%d'))
    race_key['date_raceno']=race_key['Date_key']+"|"+race_key['RaceNo']

    #copy pre_post column to race_key
    race_key['pre_post']=race_key_source['pre_post'].copy()
    
    #mset table in hkjc start from 2008-04-02
    race_key=race_key.loc[race_key['Date']>=target_table_data_start_in_hkjc,:].reset_index(drop=True)
    
    #remove races in database for pre races if already in database
    #databasepre race data, remove these data
    if sum(race_key['pre_post']=='pre')>0:
        vars()[target_table_name+'_sql']['date_raceno']=vars()[target_table_name+'_sql']['Date']+"|"+vars()[target_table_name+'_sql']['RaceNo'].astype(str)
        pre_race_key=race_key.loc[race_key['pre_post']=='pre','date_raceno'].tolist()
        vars()[target_table_name+'_sql']['temp_key']=vars()[target_table_name+'_sql']['date_raceno'].apply(lambda x: 'remove' if x in pre_race_key else 'keep')
        vars()[target_table_name+'_sql']=vars()[target_table_name+'_sql'].loc[vars()[target_table_name+'_sql']['temp_key']=='keep',:]
        del vars()[target_table_name+'_sql']['temp_key']
        del vars()[target_table_name+'_sql']['date_raceno']
        vars()[target_table_name+'_sql']=vars()[target_table_name+'_sql'].reset_index(drop=True)


    #in race_key, remove already exist in database
    if vars()[target_table_name+'_sql'].shape[0]!=0:
        #find what is currently in database
        date_raceno_key_in_db=vars()[target_table_name+'_sql']['Date']+"|"+vars()[target_table_name+'_sql']['RaceNo'].astype(str)
        date_raceno_key_in_db=date_raceno_key_in_db.tolist()
        #check=date_raceno_key_in_db.copy()
        #if already in db, then yes
        race_key['in_db']=race_key['date_raceno'].apply(lambda x: 'yes' if x in date_raceno_key_in_db else 'no')
        
        # in race_key, remove races already in db
        #so only download tables not in db
        race_key=race_key.loc[race_key['in_db']=='no',:].reset_index(drop=True)
    
    #i=0
    vars()[target_table_name+'_table']=pd.DataFrame([])
    problem_races_all=pd.DataFrame([])
    
    
    #for i in range(0,race_key.shape[0]):
    #i=0
    for i in range(0,min(no_of_races_extract,race_key.shape[0])):
        row_use=race_key[i:i+1]
        #globals()[extraction_function_name] is to call the extraction function
        
        if (target_table_name=='racecard') or(target_table_name=='sickhistory'):
            vars()[target_table_name+'_temp'],problem_races_temp=globals()[extraction_function_name](date_key=row_use['Date_key'].values[0],raceno_key=str(row_use['RaceNo'].values[0]),track_key=str(row_use['track_ST_HV_only'].values[0]))
            #check3= vars()[target_table_name+'_temp']
            #vars()[target_table_name+'_temp'],problem_races_temp=vars()[extraction_function_name](date_key=row_use['Date_key'].values[0],raceno_key=str(row_use['RaceNo'].values[0]))
            #vars()[target_table_name+'_temp'],problem_races_temp=extract_mset(date_key=row_use['Date_key'].values[0],raceno_key=str(row_use['RaceNo'].values[0]))
        else:
            vars()[target_table_name+'_temp'],problem_races_temp=globals()[extraction_function_name](date_key=row_use['Date_key'].values[0],raceno_key=str(row_use['RaceNo'].values[0]))
        
        vars()[target_table_name+'_table']=vars()[target_table_name+'_table'].append(vars()[target_table_name+'_temp'])
        #check1=vars()[target_table_name+'_table'].copy()
        if problem_races_temp.shape[0]!=0:
            problem_races_all=problem_races_all.append(problem_races_temp)
        
        eprint("Finished ",i+1, "out of ",min(no_of_races_extract,race_key.shape[0]),'------------------------------------------------------------------------------------------------------------------------')
        print("Finished ",i+1, "out of ",min(no_of_races_extract,race_key.shape[0]),'------------------------------------------------------------------------------------------------------------------------')
    
    #save problem races
    problem_races_all.to_csv("log/problem_races_"+target_table_name+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv",index=False)
    
    #using this method, if hkjc added more column, it can also be merged
    vars()[target_table_name+'_table']=vars()[target_table_name+'_sql'].append(vars()[target_table_name+'_table'])
    #check=vars()[target_table_name+'_table'].copy().head(1000)
    
#    a=vars()[target_table_name+'_sql'].copy()
#    vars()[target_table_name+'_sql'].columns.values
#    aa=vars()[target_table_name+'_table'].copy()
#    vars()[target_table_name+'_table'].columns.values
#    
#    aaa=vars()[target_table_name+'_table'].head(500)
    
    
    vars()[target_table_name+'_table']=vars()[target_table_name+'_table'].drop_duplicates()
    
    vars()[target_table_name+'_table']=vars()[target_table_name+'_table'].sort_values(by=['Date','RaceNo'],ascending=[False,True])

    #output to excel and mysql
    #csv_file_name="log/temp_data_"+target_table_name+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv"
    #vars()[target_table_name+'_table'].to_csv(csv_file_name,index=False)
    vars()[target_table_name+'_table'].to_sql(target_table_name,connection,if_exists='replace',chunksize=1000,index=False) 


#check=pd.read_csv(r"C:\Users\simon\Dropbox\notebooks\horse\log\temp_data_sickhistory_20190802_021259.csv")
#check.to_sql('sickhistory',connection,if_exists='replace',chunksize=1000,index=False) 

































#date_key='2020/07/12';raceno_key=str(9);track_key='ST';horse_no='3';HorseName_Brand='good stading_B105'
#date_key='2020/09/06';raceno_key=str(2);track_key='ST';horse_no='1';HorseName_Brand='adonis_A324'
#date_key='2020/11/22';raceno_key=str(1);track_key='ST';horse_no='1';HorseName_Brand='above_B345'

def check_horse_retirement(date_key,raceno_key,track_key,horse_no,HorseName_Brand):
    #date_key='2006/10/04';raceno_key=str(3);track_key='HV'
    date_key_old=date_key
    date_key=dt.strptime(date_key,'%Y/%m/%d').strftime('%Y%m%d')
    #url='https://racing.hkjc.com/racing/info/meeting/RaceCard/English/Local/'+date_key+'/'+track_key+'/'+raceno_key
    url='https://racing.hkjc.com/racing/information/English/Racing/RaceCard.aspx?RaceDate='+date_key+'&Racecourse='+track_key+'&RaceNo='+raceno_key
    #url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2008/12/03&Racecourse=HV&RaceNo=1'
    #print(url)
    horse_no_old=horse_no
    horse_brand=HorseName_Brand
    
    html_found=False
    retired='no'

    for ii in range(0,100):
        #https://stackoverflow.com/questions/53434555/python-requests-enable-cookies-javascript?rq=1
        html_found=False
        source_html=''
        try:
            s = requests.Session()
            r = s.get(url)
            source_html=r.text            
            if not ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                source_html=get_html_using_selenium(url)
            
            if ('starter f_tac f_fs13 draggable hiddenable' in source_html) & ('f_fs13' in source_html):
                html_found=True
                break  
            sleep(0.2)
        except:
            eprint("Oops!",sys.exc_info()[0],"occured when extracting ",'sickhistory ',date_key_old.replace("/","-"),'_',raceno_key,'\n')
        #sleep(0.1)
        
    if html_found:
        no_of_trial=ii+1
        eprint('retire_racecard html code',' tried ',no_of_trial,' times extraction with success',date_key_old.replace("/","-"),'_',raceno_key,'\n')
    else:
        problem_races=problem_races.append(pd.Series([date_key,raceno_key,track_key,horse_no,HorseName_Brand]),ignore_index=True)


    html_string=source_html
    #save source code
    file_name='store_html'+'/'+"retire_racecard_source_code_"+date_key_old.replace("/","-")+'_'+raceno_key+'_'+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".xml"
    #sometime wb mode not working, so use w
    try:
        file = open(file_name,"wb") #open file in binary mode
        file.write(html_string)
        file.close()
    except:
        file = open(file_name,"w") #open file in binary mode
        file.write(html_string)
        file.close()  
        
        
    if html_found:
        soup = BeautifulSoup(html_string, 'lxml') # Parse the HTML as a string
    
        #table = soup.find_all('table')[0] # Grab the first table
        #find table html code with class tableBorderBlue tdAlignC

        try:
            table = soup.find_all('table', {'class':'tableBorderBlue tdAlignC'})[0] # Grab the first table     
            i=0
            #row=table.find_all('tr')[2]
            #row
            #i=4
            #a=table.find_all('tr')
            #len(table.find_all('tr')[1].find_all('tr'))
            for row in table.find_all('tr')[1].find_all('tr'):
                i=i+1
                #print(i)
                #print(row)
                #row=table.find_all('tr')[1].find_all('tr')[1]
                #row
                columns = row.find_all('td')
    #                if i==3:
    #                    print("i is ",i, ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    #                    print(row)
    #                    print(len(columns))
#                if i==5:
#                    break
                #j=1
                if (i>=1)&(columns[0].get_text()==horse_no_old):
                    horse_link=columns[3].find('a').get('href')
                    horse_link='https://racing.hkjc.com'+horse_link

                    
                    for ii in range(0,50):
                        try:
                            s1 = requests.Session()
                            r1 = s1.get(horse_link)
                            if 'X-AA-Challenge' in r1.text:
                                challenge = parse_challenge(r1.text)
                                r1 = s1.get(horse_link, headers={
                                    'X-AA-Challenge': challenge['challenge'],
                                    'X-AA-Challenge-ID': challenge['challenge_id'],
                                    'X-AA-Challenge-Result': challenge['challenge_result']})
                            
                                yum = r1.cookies
                                r1 = s1.get(horse_link, cookies=yum)
                                source_html_horse_page=r1.text
                            else:
                                source_html_horse_page=r1.text
                        except:
                            eprint(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, horse page ',horse_brand)
                            print(sys.exc_info()[0],' ',sys.exc_info()[1], 'when doing horse, horse page ',horse_brand)
                            pass
                        
                        if 'horseProfile' in source_html_horse_page:
                            break
                        sleep(0.2)
                        
                        
                    
                    soup2 = BeautifulSoup(source_html_horse_page, 'lxml') 
                    table2 = soup2.find_all('table', {'class':'horseProfile'})[0]
                    
                
                    
                    #find horse title, example GOOD STANDING (B105) (Retired)
                    horse_status=table2.find_all('span', {'class':'title_text'})[0].get_text()
                    if '(Retired)' in horse_status:
                        retired='yes'
                    else:
                        retired='no'
                        
            eprint('horse ',HorseName_Brand, ' retirement status found')
            print('horse ',HorseName_Brand, 'retirement status found')
        except:
            eprint("retirement status may not be standard ",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
            print("retirement status may not be standard ",date_key_old.replace("/","-"),'_',raceno_key,'_',HorseName_Brand)
            eprint(sys.exc_info()[0],sys.exc_info()[1])
            print(sys.exc_info()[0],sys.exc_info()[1])


    else:
        eprint('retirement statusin html code not found ',date_key_old.replace("/","-"),'_',raceno_key)
        print('retirement status in html code not found ',date_key_old.replace("/","-"),'_',raceno_key)
    return retired

#a1=check_horse_retirement(date_key,raceno_key,track_key,horse_no,HorseName_Brand)


##
#target_table_name='trackwork'
#extraction_function_name='extract_trackwork_v2'
#target_table_data_start_in_hkjc='2006-09-10'
#no_of_horses_extract=1000
    

#target_table_name='sickhistory'
#extraction_function_name='extract_sickhistory_v2'
#target_table_data_start_in_hkjc='2006-09-10'
#no_of_horses_extract=500





##read data
#fn = r'C:\Users\simon\Dropbox\notebooks\index_analysis\hsi_y_x.hdf5'
#store = pd.HDFStore(fn)
#print(store)
#hsi_y_x= store.select('hsi_y_x_dataframe')
#list(hsi_y_x.columns.values)
#store.close()




  
#target_table_name='trackwork'
#@jit(target ="cuda")  
def data_extraction_for_per_horse_basis(target_table_name,extraction_function_name,target_table_data_start_in_hkjc,no_of_horses_extract):
    query='SELECT * from racecard'
    try:
        racecard= sql.read_sql(query,connection)
        #check=vars()[target_table_name+'_sql'].copy()
    except:
        racecard=pd.DataFrame([])


        
    if target_table_name=='trackwork':
        fn ='./h5_data/trackwork.hdf5'
        if os.path.exists(fn):
            #read from h5
            store = pd.HDFStore(fn)
            vars()[target_table_name+'_sql']= store.select('trackwork_dataframe')
            store.close()
        
        else:
            vars()[target_table_name+'_sql']=pd.DataFrame([]) 
             
    else:
        query='SELECT * from '+target_table_name
        try:
            vars()[target_table_name+'_sql'] = sql.read_sql(query,connection)
            #check=vars()[target_table_name+'_sql'].copy()
        except:
            vars()[target_table_name+'_sql']=pd.DataFrame([])        

    #check=list(vars()[target_table_name+'_sql'].HorseName_Brand.unique())
    #check=vars()[target_table_name+'_sql'].loc[vars()[target_table_name+'_sql']['HorseName_Brand']=='best gift_CD296',:]




    racecard_selected=racecard[['Date','RaceNo','track','Horse No.','Horse','Brand No.']].copy()

    #edit horseno
    racecard_selected=racecard_selected.rename(columns={'Horse No.':'HorseNo'})
    
    #there is a withdraw near horse name
    #main_data_check=main_data.loc[(main_data['Date']=='2019-09-21')&(main_data['RaceNo']=='6'),:]
    racecard_selected[['HorseName','withdrawn']]=racecard_selected.Horse.str.split('(',expand=True)
    racecard_selected.loc[pd.isnull(racecard_selected['withdrawn']),"withdrawn"]=''
    racecard_selected['withdrawn']=racecard_selected['withdrawn'].apply(lambda x:x.replace(')',''))
    racecard_selected['withdrawn']=racecard_selected['withdrawn'].apply(lambda x: x.strip())
    racecard_selected['withdrawn']=racecard_selected['withdrawn'].apply(lambda x: x.lower())

    del racecard_selected['Horse']


    #trim horse name right and left space coz some horse name have right space
    racecard_selected['HorseName']=racecard_selected['HorseName'].apply(lambda x: x.strip())
    #lower case horse name
    racecard_selected['HorseName']=racecard_selected['HorseName'].apply(lambda x: x.lower())
    check_horsename=list(racecard_selected['HorseName'].unique())
    
    
    #check brand name
    racecard_selected=racecard_selected.rename(columns={'Brand No.':'BrandName'})
    racecard_selected['BrandName']=racecard_selected['BrandName'].apply(lambda x: x.strip()) #no brand name with right space, but for safety reason, just also trim them 
    check_brand=list(racecard_selected['BrandName'].unique())
    racecard_selected['Horse_Brand']=racecard_selected['HorseName']+'_'+racecard_selected['BrandName']

    horse_unique=racecard_selected.groupby(['Horse_Brand']).head(1)
    horse_unique=horse_unique.reset_index(drop=True)
    list(horse_unique['HorseNo'].unique())
    horse_unique.shape[0]
    
    lateset_date_in_racecard=horse_unique['Date'].values[0]
    #split horse unique into two, one is latest date in racecard, and other
    horse_unique_lateset_date_in_racecard=horse_unique.loc[horse_unique['Date']==lateset_date_in_racecard,:]
    horse_unique_other=horse_unique.loc[~(horse_unique['Date']==lateset_date_in_racecard),:]
    
    
    #trackwork_all.to_sql('trackwork',connection,if_exists='replace',chunksize=1000,index=False) 
    
    
    #for today horse, check whether it is retired
    #because starting from 12/7/2020, if horse retired, no trackwork and sick history in horse page
    #so won't remove it in trackwork database. for example GOOD STANDING (B105), note that trackwork database already removed this horse
    #so this horse trackwork no longer exist
    #i=0
    retirement_status_all=[]
    for i in range(0,horse_unique_lateset_date_in_racecard.shape[0]):
        row_use=horse_unique_lateset_date_in_racecard[i:i+1]    
        retirement_status=check_horse_retirement(date_key=row_use['Date'].values[0],raceno_key=str(row_use['RaceNo'].values[0]),track_key=str(row_use['track'].values[0]),horse_no=str(row_use['HorseNo'].values[0]),HorseName_Brand=str(row_use['Horse_Brand'].values[0]))
        retirement_status_all.append(retirement_status)
    
    horse_unique_lateset_date_in_racecard['retired']=retirement_status_all
    horse_unique_lateset_date_in_racecard_inforce=horse_unique_lateset_date_in_racecard.loc[horse_unique_lateset_date_in_racecard['retired']=='no',:].copy()
    horse_unique_lateset_date_in_racecard_inforce=horse_unique_lateset_date_in_racecard_inforce.reset_index(drop=True)
    

    

    #remove all latest date horses (appear in race card) in sql table
    #this is to make sure run this program, must download today horse
    if vars()[target_table_name+'_sql'].shape[0]!=0:
        today_horse=list(set(horse_unique_lateset_date_in_racecard_inforce['Horse_Brand']))
        vars()[target_table_name+'_sql']['remove']=vars()[target_table_name+'_sql']['HorseName_Brand'].apply(lambda x: 'yes' if x in today_horse else 'no')
        vars()[target_table_name+'_sql']=vars()[target_table_name+'_sql'].loc[vars()[target_table_name+'_sql']['remove']=='no',:]
        del vars()[target_table_name+'_sql']['remove']
    
    #in horse_unique, remove already exist in database table
    if vars()[target_table_name+'_sql'].shape[0]!=0:
        #find what is currently in database
        horse_key_in_db=list(set(vars()[target_table_name+'_sql']['HorseName_Brand']))
        #check=date_raceno_key_in_db.copy()
        #if already in db, then yes
        horse_unique['in_db']=horse_unique['Horse_Brand'].apply(lambda x: 'yes' if x in horse_key_in_db else 'no')
        
        # in race_key, remove races already in db
        #so only download tables not in db
        horse_unique=horse_unique.loc[horse_unique['in_db']=='no',:].reset_index(drop=True)
        del horse_unique['in_db']


    #remove GOOD STANDING (B105) in horse_unique. because trackwork database already removed this horse
    #so when everytime run this program, it will download this horse's trackwork, causing error
    #so need to remove it
    horse_unique=horse_unique.loc[horse_unique['Horse_Brand']!='good standing_B105',:].reset_index(drop=True)


    #becasue hkjc after 2020 will remove historical racecard, so no point in going back previous racecard
    horse_unique=horse_unique.loc[horse_unique['Date']==lateset_date_in_racecard,:].copy()
    horse_unique=horse_unique.reset_index(drop=True)
    
    #i=0
    vars()[target_table_name+'_table']=pd.DataFrame([])
    problem_races_all=pd.DataFrame([])


    
    #for i in range(0,race_key.shape[0]):
    #i=0
    for i in range(0,min(no_of_horses_extract,horse_unique.shape[0])):
        row_use=horse_unique[i:i+1]
        #date_key='2019/10/12';raceno_key=str(1);track_key='ST';horse_no='1';HorseName_Brand='blazing partners_B071'
        vars()[target_table_name+'_temp'],problem_races_temp=globals()[extraction_function_name](date_key=row_use['Date'].values[0],raceno_key=str(row_use['RaceNo'].values[0]),track_key=str(row_use['track'].values[0]),horse_no=str(row_use['HorseNo'].values[0]),HorseName_Brand=str(row_use['Horse_Brand'].values[0]))

        vars()[target_table_name+'_table']=vars()[target_table_name+'_table'].append(vars()[target_table_name+'_temp'])
        #check1=vars()[target_table_name+'_table'].copy()
        if problem_races_temp.shape[0]!=0:
            problem_races_all=problem_races_all.append(problem_races_temp)
        
        eprint("Finished ",i+1, "out of ",min(no_of_horses_extract,horse_unique.shape[0]),'------------------------------------------------------------------------------------------------------------------------')
        print("Finished ",i+1, "out of ",min(no_of_horses_extract,horse_unique.shape[0]),'------------------------------------------------------------------------------------------------------------------------')
    
    
    #save problem races
    problem_races_all.to_csv("log/problem_races_"+target_table_name+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv",index=False)
    
    #using this method, if hkjc added more column, it can also be merged
    vars()[target_table_name+'_table']=vars()[target_table_name+'_sql'].append(vars()[target_table_name+'_table'])
    #check=vars()[target_table_name+'_table'].copy().head(1000)
    
    
    vars()[target_table_name+'_table']=vars()[target_table_name+'_table'].sort_values(by=['Date_racecard','RaceNo_racecard','HorseNo_racecard'],ascending=[False,True,True])
    
    
    
    if target_table_name=='trackwork':    
        #save as h5
        store = pd.HDFStore("./h5_data/trackwork.hdf5", "w", complib=str("zlib"), complevel=5)
        store.put("trackwork_dataframe", vars()[target_table_name+'_table'], data_columns=vars()[target_table_name+'_table'].columns)
        store.close()
    else:
        #output to excel and mysql
        #csv_file_name="log/temp_data_"+target_table_name+"_"+time.strftime("%Y%m%d")+'_'+time.strftime("%H%M%S")+".csv"
        #vars()[target_table_name+'_table'].to_csv(csv_file_name,index=False)
        vars()[target_table_name+'_table'].to_sql(target_table_name,connection,if_exists='replace',chunksize=1000,index=False)
    
    
    




#adhothot check trackwork
#trackwork_sql_check=trackwork_sql.head(100)
#trackwork_sql_check=trackwork_sql.loc[trackwork_sql['HorseName_Brand']=='harmony victory_B405',:]
#trackwork_sql_check=trackwork_sql.loc[trackwork_sql['Date']=='22/11/2020',:]











    
    
    

if sys.argv[1]=='post':
    #extract mset
    data_extraction(target_table_name='mset',
                    extraction_function_name='extract_mset',
                    target_table_data_start_in_hkjc='2008-04-02',
                    no_of_races_extract=999999999)

    data_extraction(target_table_name='race_result',
                    extraction_function_name='extract_race_result',
                    target_table_data_start_in_hkjc='2003-08-31',
                    no_of_races_extract=999999999)

    data_extraction(target_table_name='dividend',
                    extraction_function_name='extract_dividend',
                    target_table_data_start_in_hkjc='2003-08-31',
                    no_of_races_extract=999999999)

#extract racecard
data_extraction(target_table_name='racecard',
                extraction_function_name='extract_racecard',
                target_table_data_start_in_hkjc='2006-09-10',
                no_of_races_extract=999999999)




#extract trackwork
data_extraction_for_per_horse_basis(target_table_name='trackwork',
                extraction_function_name='extract_trackwork_v2',
                target_table_data_start_in_hkjc='2006-09-10',
                no_of_horses_extract=4000)





#extract sickhistory
data_extraction_for_per_horse_basis(target_table_name='sickhistory',
                extraction_function_name='extract_sickhistory_v2',
                target_table_data_start_in_hkjc='2006-09-10',
                no_of_horses_extract=4000)
















sys.stderr.close()
sys.stderr = sys.__stderr__




sleep(1)






#stan_out_log=os.path.join('log','stan_out_post_race_data_part_20191027_172001_production.log')



with open(stan_out_log) as f:
    data = f.readlines()

key_words=['error','Error','exception','Exception','not found']

#check key work in log file line by line
output=[x for x in data if any(s in x for s in key_words)]
output="\n".join(output)

error_exist= 'No errors' if len(output)==0 else 'has errors'


#python send email to check is there any exception/error in log file
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
mail_content = output
#The mail addresses and password
sender_address = 'random9522@gmail.com'
sender_pass = '95229522'
receiver_address = 'simon.ertwewe@gmail.com'
#Setup the MIME
message = MIMEMultipart()
message['From'] = sender_address
message['To'] = receiver_address
message['Subject'] = 'Error/Exception in pre/post: '+error_exist  #The subject line
#The body and the attachments for the mail
message.attach(MIMEText(mail_content))

#attachement
fp = open(os.path.join(target_dir,stan_out_log), 'rb')
part = MIMEBase('application',"octet-stream")           #send txt file as attachement
part.set_payload(fp.read())
fp.close()
encoders.encode_base64(part)
part.add_header('Content-Disposition', 'attachment', filename=stan_out_log)
message.attach(part)


#Create SMTP session for sending the mail
session = smtplib.SMTP('smtp.gmail.com', 587) #use gmail with port
session.starttls() #enable security
session.login(sender_address, sender_pass) #login with mail_id and password
text = message.as_string()
session.sendmail(sender_address, receiver_address, text)
session.quit()
print('Mail Sent')

















